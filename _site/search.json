[
  {
    "objectID": "posts/2019-08-05_clustering-bob-ross-paintings/index.html",
    "href": "posts/2019-08-05_clustering-bob-ross-paintings/index.html",
    "title": "Exploring Bob Ross paintings",
    "section": "",
    "text": "In this post, I try something new and share an analysis I did without stopping to explain the code along the way (with a few exceptions). I analyze a dataset on Bob Ross paintings from last week’s Tidytuesday, an initiative by the R for Data Science online learning community. Each Monday, a new dataset is posted on GitHub with a short description. You can see some analyses and visualizations people have done by searching for the #tidytuesday hashtag on Twitter.\nIn this dataset, each row describes a painting Bos Ross (or a guest) did on his show, with the episode, season, title, and a column for each feature (e.g. “tree”, “mountain”) that is 1 if that painting included that feature and 0 otherwise. Let’s go exploring!"
  },
  {
    "objectID": "posts/2019-08-05_clustering-bob-ross-paintings/index.html#set-up",
    "href": "posts/2019-08-05_clustering-bob-ross-paintings/index.html#set-up",
    "title": "Exploring Bob Ross paintings",
    "section": "Set up",
    "text": "Set up\n\nlibrary(tidyverse)\nggthemr::ggthemr('fresh')\n\nbob_ross <- read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-08-06/bob-ross.csv\")\n\n# copied from tidytuesday repo\nbob_ross <- bob_ross %>%\n  janitor::clean_names() %>% \n  separate(episode, into = c(\"season\", \"episode\"), sep = \"E\") %>% \n  mutate(season = str_extract(season, \"[:digit:]+\")) %>% \n  mutate_at(vars(season, episode), as.integer) %>%\n  # added to clean up title\n  mutate(title = str_remove_all(title, '\"')) \n\nbob_ross %>%\n  head(5) %>%\n  select(1:7) %>%\n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\nseason\nepisode\ntitle\napple_frame\naurora_borealis\nbarn\nbeach\n\n\n\n\n1\n1\nA WALK IN THE WOODS\n0\n0\n0\n0\n\n\n1\n2\nMT. MCKINLEY\n0\n0\n0\n0\n\n\n1\n3\nEBONY SUNSET\n0\n0\n0\n0\n\n\n1\n4\nWINTER MIST\n0\n0\n0\n0\n\n\n1\n5\nQUIET STREAM\n0\n0\n0\n0"
  },
  {
    "objectID": "posts/2019-08-05_clustering-bob-ross-paintings/index.html#exploratory-graphs",
    "href": "posts/2019-08-05_clustering-bob-ross-paintings/index.html#exploratory-graphs",
    "title": "Exploring Bob Ross paintings",
    "section": "Exploratory graphs",
    "text": "Exploratory graphs\nLet’s start with some basic data exploration. What are the ten most frequent features in his paintings?\n\nbob_ross %>%\n  summarize_at(vars(apple_frame:wood_framed), ~ sum(.)) %>%\n  gather(feature, nb_paintings) %>%\n  mutate(feature = fct_reorder(feature, nb_paintings)) %>%\n  top_n(10, nb_paintings) %>%\n  ggplot(aes(x = feature, y = nb_paintings)) + \n  geom_col() + \n  coord_flip() + \n  labs(y = \"Number of paintings\", \n       title = \"What were most common features in Bob Ross paintings?\",\n       x = \"\")\n\n\n\n\nIf you’ve seen Bob Ross paintings, it’s probably not too surprising that “tree” is the top feature. I am a bit surprised to find that trees, deciduous, and conifer are all separate features (the latter two are types of trees).\nLet’s adapt this to show what percent of paintings contained the feature, rather than the total number.\n\nbob_ross %>%\n  summarize_at(vars(apple_frame:wood_framed), ~ sum(.)) %>%\n  gather(feature, nb_paintings) %>%\n  mutate(total_paintings = nrow(bob_ross),\n         pct_w_feature = nb_paintings / total_paintings,\n         feature = fct_reorder(feature, pct_w_feature)) %>%\n  top_n(10, pct_w_feature) %>%\n  ggplot(aes(x = feature, y = pct_w_feature)) + \n  geom_col() + \n  coord_flip() + \n  labs(y = \"Percent of paintings with feature\", \n       title = \"What were most common features in Bob Ross paintings?\",\n       x = \"\") + \n  scale_y_continuous(label = scales::percent) + \n  expand_limits(y = 1)\n\n\n\n\nThis gives some more context of just how common having a tree was - more than 80% of the paintings had one!\nDid the content of the paintings change over time? We can answer this question in a lot of different ways - let’s start by looking at features that appeared in more than 150 paintings and their trends over the seasons.\n\nnb_features_by_season <- bob_ross %>%\n  group_by(season) %>%\n  summarize_at(vars(apple_frame:wood_framed), ~ sum(.)) %>%\n  gather(feature, nb_paintings, -season)\n\n\nnb_features_by_season %>%\n  add_count(feature, wt = nb_paintings, name = \"nb_total\") %>%\n  filter(nb_total > 150) %>%\n  ggplot(aes(x = season, y = nb_paintings, color = feature)) + \n  geom_line() + \n  facet_wrap(~ feature) + \n  labs(y = \"Number of paintings with feature\",\n       title = \"Bob Ross always loved trees, but fell out of love with clouds\",\n       subtitle = \"Dashed line is number of episodes in the season\") + \n  geom_hline(yintercept = 13, lty = 2) + \n  theme(legend.position = \"none\") + \n  expand_limits(y = 0)\n\n\n\n\nTrees (of all types) stay consistently popular across seasons, while clouds became less featured.\nWhat features have the great absolute difference between the first and second half of the series?\n\nby_feature_half <- nb_features_by_season %>%\n  mutate(season_half = if_else(season <= 15, \"first\", \"second\")) %>%\n  group_by(season_half, feature) %>%\n  summarize(nb_paintings = sum(nb_paintings)) %>%\n  ungroup() %>%\n  mutate(nb_paintings = ifelse(season_half == \"first\", \n                               nb_paintings / (15 * 67), \n                               nb_paintings / (16 * 67))) %>%\n  spread(season_half, nb_paintings) %>%\n  mutate(difference = second - first) %>%\n  top_n(10, abs(difference)) %>%\n  gather(season_half, nb_paintings, first, second)\n\n`summarise()` has grouped output by 'season_half'. You can override using the\n`.groups` argument.\n\nby_feature_half %>%\n  mutate(feature = fct_reorder(feature, difference, .desc = TRUE),\n         season_half = ifelse(season_half == \"first\", \"1-15\", \"16-31\")) %>%\n  ggplot(aes(nb_paintings, feature)) +\n  geom_line() +\n  geom_point(aes(color = season_half)) +\n  scale_color_manual(values = c(\"#E84646\", \"#233B43\")) + \n  scale_x_continuous(labels = scales::percent) + \n  labs(x = \"Percent of paintings with feature\",\n       y = \"\",\n       title = \"What features became more or less frequent?\",\n       color = \"Seasons\")\n\n\n\n\nAs we had seen in our graph of the top 6 features, clouds (and it’s like, cumuls and cirrus) went down, while framed got a little more popular.\nWhat’s the distribution of the number of features per painting (code for a calculating the summary of a row thanks to this tweet by Jenny Bryan)?\n\nbob_ross %>%\n  mutate(nb_features = select_at(., vars(apple_frame:wood_framed)) %>% pmap_dbl(sum)) %>%\n  ggplot(aes(x = nb_features)) + \n  geom_histogram(bins = 16) + \n  labs(x = \"Number of features in a painting\",\n       y = \"Number of paintings\",\n       title = \"Most paintings had between 5 and 11 features\")\n\n\n\n\nMost paintings have between 5 and 11 features, although one has more than 15 and a couple have none.\n\nPrincipal Component Analysis\nIn short, principal component analysis is a dimensionality-reduction technique that takes all of your variables and creates a new set of characteristics that still contains the most information. For more information, I highly recommend checking out this StackOverflow answer for great explanations from simple to mathematical and Julia Silge’s post where she explains principal component analysis using StackOverflow data.\nWe’ll start by selecting only features that appear in at least 5 paintings and calculate three principal components, adding scale = TRUE to scale the features. This is a critical step; while the features are all on the same scale (0 or 1, not say 0 or 100 for one and 0 or 50 for another), they have different variances as some have a lot of 1s and some a lot of 0s. We need to center our data so that they all have the same mean and variance.\n\nbob_ross_for_pca <- bob_ross %>% \n  select(-season, -episode, -title)  %>%\n  select_if(~ sum(.) >= 5)\n\n\ntags_pca <- irlba::prcomp_irlba(bob_ross_for_pca, n = 3, scale. = TRUE)\n\ntidied_pca <- bind_cols(feature = colnames(bob_ross_for_pca),\n                        as_tibble(tags_pca$rotation)) %>%\n  gather(PC, value, PC1:PC3)\n\nLet’s take a look at which features have the most relative importance (highest absolute value) for each component. We’re going to use the new tidytext functions reorder_within() and scale_x_reordered() to make the features ordered within each plot (see Julia Silge’s post for more).\n\nlibrary(tidytext)\n\ntidied_pca %>%\n  group_by(PC) %>%\n  top_n(8, abs(value)) %>%\n  ungroup() %>%\n  ggplot(aes(x = reorder_within(feature, value, PC), y = value)) +\n  geom_col() + \n  coord_flip() + \n  facet_wrap(~ PC, scales = \"free\", ncol = 1) + \n  scale_x_reordered() + \n  labs(title = \"What distinguishes Bob Ross paintings?\",\n       subtitle = \"Trees vs. beach, structures vs. mountains, and water vs. winter structures\", y = \"Relative importance in principal component\",\n       x = \"\")\n\n\n\n\nWe find that the first principal component divides painting between those at the ocean (with beach and palm trees) and those in the tree covered mountains. The next component divides mountains from structures like cabin and barns, while the final one distinguishes a cabin in the snow from a grassy river."
  },
  {
    "objectID": "posts/2019-08-05_clustering-bob-ross-paintings/index.html#conclusion",
    "href": "posts/2019-08-05_clustering-bob-ross-paintings/index.html#conclusion",
    "title": "Exploring Bob Ross paintings",
    "section": "Conclusion",
    "text": "Conclusion\nI hope you enjoyed this new style of post and were able to learn some new tricks along the way. Let me know what you think on twitter!"
  },
  {
    "objectID": "posts/2018-11-16_the-lesser-known-stars-of-the-tidyverse/index.html",
    "href": "posts/2018-11-16_the-lesser-known-stars-of-the-tidyverse/index.html",
    "title": "The Lesser Known Stars of the Tidyverse",
    "section": "",
    "text": "In early 2018, I gave a few conference talks on “The Lesser Known Stars of the Tidyverse.” I focused on some packages and functions that aren’t as well known as the core parts of ggplot2 and dplyr but are very helpful in exploratory analysis. I walked through an example analysis of Kaggle’s 2017 State of Data Science and Machine Learning Survey to show how I would use these functions in an exploratory analysis.\nThis post shares that analysis with some extra written commentary. If you’d like to watch the talks as well, both the RStudio::conf and New York R conference ones were recorded."
  },
  {
    "objectID": "posts/2018-11-16_the-lesser-known-stars-of-the-tidyverse/index.html#reading-in-the-data",
    "href": "posts/2018-11-16_the-lesser-known-stars-of-the-tidyverse/index.html#reading-in-the-data",
    "title": "The Lesser Known Stars of the Tidyverse",
    "section": "Reading in the data",
    "text": "Reading in the data\nFirst, we’ll try reading in our dataset with base R’s read.csv.\n\nmultiple_choice_responses_base <- read.csv(\"multipleChoiceResponses.csv\")\n\nLet’s say we wanted to know the numbers of NAs in each column. We can use is.na to change each entry in a column to TRUE or FALSE, depending on whether it’s NA, and then sum the column (because TRUE evaluates as 1 and FALSE as 0) to get the total number of NAs.\n\n# for one column\nsum(is.na(multiple_choice_responses_base$Country))\n\n[1] 0\n\n\nTo do this for a group of columns, say the first five, we can use summarise_at function. summarise_at takes three arguments: the dataset, which we pipe in, the set of columns we want to apply a function to, and a function to apply to each of the columns. It returns a dataframe with the same column names and one row with the result of our function. In our case, that will be the number of NAs in each column.\nYou can select the columns by position, as we’ll do here with 1:5, or by name. Note that if you do it by name, you need to put them within vars() (e.g. vars(GenderSelect:StudentStatus) or vars(GenderSelect, Age)).\n\n# for five columns\nmultiple_choice_responses_base %>%\n  summarise_at(1:5, ~sum(is.na(.))) \n\n  GenderSelect Country Age EmploymentStatus StudentStatus\n1            0       0 331                0             0\n\n\nWow that’s lucky! Four of them that don’t have any NAs. But … is this too good to be true? Let’s look at the entries of one column.\n\nmultiple_choice_responses_base %>%\n  count(StudentStatus) \n\n  StudentStatus     n\n1               15436\n2            No   299\n3           Yes   981\n\n\nYep. We see here we have a lot of \"\" entries instead of NAs. We can correct this with na_if from dplyr, which takes as an argument what we want to turn into NAs. We can also use %<>%, which is a reassignment pipe. While this is nice to save some typing, it can make it confusing when reading a script, so use with caution.\n\nmultiple_choice_responses_base %<>%\n  na_if(\"\")\n\n## is the same as: \n\nmultiple_choice_responses_base <- multiple_choice_responses_base %>%\n  na_if(\"\")\n\nNow let’s count the NAs again.\n\nmultiple_choice_responses_base %>%\n  summarise_at(1:5, ~sum(is.na(.))) \n\n  GenderSelect Country Age EmploymentStatus StudentStatus\n1           95     121 331                0         15436\n\n\nAnd it’s fixed!\nHow could we have avoided this all in the first place? By using readr::read_csv instead of read.csv.\nIf you’re not familiar with ::, it’s for explicitly setting what package you’re getting the function on the right from. This is helpful in three ways:\n\nThere can be name conflicts, where two packages have functions with the same name. Using :: ensures you’re getting the function you want.\nif you only want to use one function from a package, you can use :: to skip the library call. As long as you’ve installed the package, you don’t need to have loaded it to get the function.\nFor teaching purposes, it’s nice to remind people where the function is coming from.\n\n\nmultiple_choice_responses <- readr::read_csv(\"multipleChoiceResponses.csv\")\n\nWarning: One or more parsing issues, see `problems()` for details\n\n\nIt’s definitely faster, but it seems we have some errors. Let’s inspect them.\n\nproblems(multiple_choice_responses)\n\n# A tibble: 65 × 5\n     row   col expected           actual           file \n   <int> <int> <chr>              <chr>            <chr>\n 1   297    84 1/0/T/F/TRUE/FALSE Rarely           \"\"   \n 2   673    84 1/0/T/F/TRUE/FALSE Most of the time \"\"   \n 3  1210   207 a number           -                \"\"   \n 4  1317    99 1/0/T/F/TRUE/FALSE Rarely           \"\"   \n 5  1595    99 1/0/T/F/TRUE/FALSE Often            \"\"   \n 6  2444    99 1/0/T/F/TRUE/FALSE Most of the time \"\"   \n 7  2467    99 1/0/T/F/TRUE/FALSE Most of the time \"\"   \n 8  2623    99 1/0/T/F/TRUE/FALSE Often            \"\"   \n 9  2631    99 1/0/T/F/TRUE/FALSE Sometimes        \"\"   \n10  2725    99 1/0/T/F/TRUE/FALSE Often            \"\"   \n# … with 55 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nWe see each row and column where a problem occurs. What’s happening is that read_csv uses the first 1000 rows of a column to guess its type. But in some cases, it’s guessing the column is an integer, because the first 1000 rows are whole numbers, when actually it should be double, as some entries have decimal points. We can fix this by changing the number of rows read_csv uses to guess the column type (with the guess_max argument) to the number of rows in the data set.\n\nmultiple_choice_responses <- readr::read_csv(\"multipleChoiceResponses.csv\", \n                                             guess_max = nrow(multiple_choice_responses))\n\nError-free!"
  },
  {
    "objectID": "posts/2018-11-16_the-lesser-known-stars-of-the-tidyverse/index.html#initial-examination",
    "href": "posts/2018-11-16_the-lesser-known-stars-of-the-tidyverse/index.html#initial-examination",
    "title": "The Lesser Known Stars of the Tidyverse",
    "section": "Initial examination",
    "text": "Initial examination\nLet’s see what we can glean from the column names themselves. I’ll only look at the first 20 since there are so many.\n\ncolnames(multiple_choice_responses) %>%\n  head(20)\n\n [1] \"GenderSelect\"                      \"Country\"                          \n [3] \"Age\"                               \"EmploymentStatus\"                 \n [5] \"StudentStatus\"                     \"LearningDataScience\"              \n [7] \"CodeWriter\"                        \"CareerSwitcher\"                   \n [9] \"CurrentJobTitleSelect\"             \"TitleFit\"                         \n[11] \"CurrentEmployerType\"               \"MLToolNextYearSelect\"             \n[13] \"MLMethodNextYearSelect\"            \"LanguageRecommendationSelect\"     \n[15] \"PublicDatasetsSelect\"              \"LearningPlatformSelect\"           \n[17] \"LearningPlatformUsefulnessArxiv\"   \"LearningPlatformUsefulnessBlogs\"  \n[19] \"LearningPlatformUsefulnessCollege\" \"LearningPlatformUsefulnessCompany\"\n\n\nWe can see that there were categories of questions, like “LearningPlatform,” with each platform having its own column.\nNow let’s take a look at our numeric columns with skimr. Skimr is a package from rOpenSci that allows you to quickly view summaries of your data. We can use select_if to select only columns where a certain condition, in this case whether it’s a numeric column, is true.\n\nmultiple_choice_responses %>%\n  select_if(is.numeric) %>%\n  skimr::skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n16716\n\n\nNumber of columns\n13\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n13\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nAge\n331\n0.98\n32.37\n10.47\n0\n25\n30\n37\n100\n▁▇▂▁▁\n\n\nLearningCategorySelftTaught\n3607\n0.78\n33.37\n25.79\n0\n15\n30\n50\n100\n▇▅▃▂▁\n\n\nLearningCategoryOnlineCourses\n3590\n0.79\n27.38\n26.86\n0\n5\n20\n40\n100\n▇▃▂▁▁\n\n\nLearningCategoryWork\n3605\n0.78\n15.22\n19.00\n0\n0\n10\n25\n100\n▇▂▁▁▁\n\n\nLearningCategoryUniversity\n3594\n0.78\n16.99\n23.68\n0\n0\n5\n30\n100\n▇▂▁▁▁\n\n\nLearningCategoryKaggle\n3590\n0.79\n5.53\n11.07\n0\n0\n0\n10\n100\n▇▁▁▁▁\n\n\nLearningCategoryOther\n3622\n0.78\n1.80\n9.36\n0\n0\n0\n0\n100\n▇▁▁▁▁\n\n\nTimeGatheringData\n9186\n0.45\n36.14\n21.65\n0\n20\n35\n50\n100\n▇▇▆▂▁\n\n\nTimeModelBuilding\n9188\n0.45\n21.27\n16.17\n0\n10\n20\n30\n100\n▇▃▁▁▁\n\n\nTimeProduction\n9199\n0.45\n10.81\n12.26\n0\n0\n10\n15\n100\n▇▁▁▁▁\n\n\nTimeVisualizing\n9187\n0.45\n13.87\n11.72\n0\n5\n10\n20\n100\n▇▁▁▁▁\n\n\nTimeFindingInsights\n9193\n0.45\n13.09\n12.97\n0\n5\n10\n20\n303\n▇▁▁▁▁\n\n\nTimeOtherSelect\n9203\n0.45\n2.40\n12.16\n0\n0\n0\n0\n100\n▇▁▁▁▁\n\n\n\n\n\nI love the histograms. We can quickly see from them that people self teach a lot and spend a good amount of time building models and gathering data, compared to visualizing data or working in production.\nLet’s see how many distinct answers we have for each question. We can use n_distinct(), a shorter and faster version of length(unique()). We’ll use summarise_all, which is the same as summarise_at except that you don’t select a group of columns and so it applies to every one in the dataset.\n\nmultiple_choice_responses %>%\n  summarise_all(n_distinct) %>%\n  select(1:10)\n\n# A tibble: 1 × 10\n  Gender…¹ Country   Age Emplo…² Stude…³ Learn…⁴ CodeW…⁵ Caree…⁶ Curre…⁷ Title…⁸\n     <int>   <int> <int>   <int>   <int>   <int>   <int>   <int>   <int>   <int>\n1        5      53    85       7       3       4       3       3      17       4\n# … with abbreviated variable names ¹​GenderSelect, ²​EmploymentStatus,\n#   ³​StudentStatus, ⁴​LearningDataScience, ⁵​CodeWriter, ⁶​CareerSwitcher,\n#   ⁷​CurrentJobTitleSelect, ⁸​TitleFit\n\n\nThis data would be more helpful if it was tidy and had two columns, question and num_distinct_answers. We can use tidyr::gather to change our data from “wide” to “long” format and then arrange it so we can see the columns with the most distinct answers first. If you’ve used (or are still using!) reshape2, check out tidyr; reshape2 is retired and is only updated with changes necessary for it to remain on CRAN. While not exactly equivalent, tidyr::spread replaces reshape2::dcast, tidyr::separate reshape2::colsplit, and tidyr::gather reshape2::melt.\n\nmultiple_choice_responses %>%\n  summarise_all(n_distinct) %>%\n  tidyr::gather(question, num_distinct_answers) %>%\n  arrange(desc(num_distinct_answers))\n\n# A tibble: 228 × 2\n   question               num_distinct_answers\n   <chr>                                 <int>\n 1 WorkMethodsSelect                      6191\n 2 LearningPlatformSelect                 5363\n 3 WorkToolsSelect                        5249\n 4 WorkChallengesSelect                   4288\n 5 WorkDatasetsChallenge                  2221\n 6 PastJobTitlesSelect                    1856\n 7 MLTechniquesSelect                     1802\n 8 WorkDatasets                           1723\n 9 WorkAlgorithmsSelect                   1421\n10 MLSkillsSelect                         1038\n# … with 218 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nLet’s take a look at the question with the most distinct answers, WorkMethodsSelect.\n\nmultiple_choice_responses %>%\n  count(WorkMethodsSelect, sort = TRUE)\n\n# A tibble: 6,191 × 2\n   WorkMethodsSelect                           n\n   <chr>                                   <int>\n 1 <NA>                                     8943\n 2 Data Visualization                        144\n 3 Other                                     144\n 4 Logistic Regression                        66\n 5 Time Series Analysis                       49\n 6 Neural Networks                            45\n 7 A/B Testing                                42\n 8 Data Visualization,Time Series Analysis    37\n 9 Text Analytics                             36\n10 Decision Trees                             29\n# … with 6,181 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nWe can see this is a multiple select question, where if a person selected multiple answers they’re listed as one entry, separated by commas. Let’s tidy it up.\nFirst, let’s get rid of the NAs. We can use !is.na(WorkMethodsSelect), short for is.na(WorkMethodsSelect) == FALSE, to filter out NAs. We then use str_split, from stringr, to divide the entries up. str_split(WorkMethodsSelect, \",\") says “Take this string and split it into a list by dividing it where there are ,s.”\n\nnested_workmethods <- multiple_choice_responses %>%\n  select(WorkMethodsSelect) %>%\n  filter(!is.na(WorkMethodsSelect)) %>%\n  mutate(work_method = str_split(WorkMethodsSelect, \",\")) \n\nnested_workmethods %>%\n  select(work_method)\n\n# A tibble: 7,773 × 1\n   work_method\n   <list>     \n 1 <chr [5]>  \n 2 <chr [12]> \n 3 <chr [17]> \n 4 <chr [14]> \n 5 <chr [12]> \n 6 <chr [1]>  \n 7 <chr [14]> \n 8 <chr [12]> \n 9 <chr [7]>  \n10 <chr [5]>  \n# … with 7,763 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nNow we have a list column, with each entry in the list being one work method. We can unnest this so we can get back a tidy dataframe.\n\nunnested_workmethods <- nested_workmethods %>%\n  tidyr::unnest(work_method) %>%\n  select(work_method)\n\nunnested_workmethods\n\n# A tibble: 59,497 × 1\n   work_method                     \n   <chr>                           \n 1 Association Rules               \n 2 Collaborative Filtering         \n 3 Neural Networks                 \n 4 PCA and Dimensionality Reduction\n 5 Random Forests                  \n 6 A/B Testing                     \n 7 Bayesian Techniques             \n 8 Data Visualization              \n 9 Decision Trees                  \n10 Ensemble Methods                \n# … with 59,487 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nGreat! As a last step, let’s count this data so we can find which are the most common work methods people use.\n\nunnested_workmethods %>%\n  count(work_method, sort = TRUE)\n\n# A tibble: 31 × 2\n   work_method                          n\n   <chr>                            <int>\n 1 Data Visualization                5022\n 2 Logistic Regression               4291\n 3 Cross-Validation                  3868\n 4 Decision Trees                    3695\n 5 Random Forests                    3454\n 6 Time Series Analysis              3153\n 7 Neural Networks                   2811\n 8 PCA and Dimensionality Reduction  2789\n 9 kNN and Other Clustering          2624\n10 Text Analytics                    2405\n# … with 21 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nWe see the classic methods of data visualization, logistic regression, and cross-validation lead the pack.\n\nGraphing Frequency of Different Work Challenges\nNow let’s move on to understanding what challenges people face at work. This was one of those categories where there were multiple questions asked, all having names starting with WorkChallengeFrequency and ending with the challenge (e.g “DirtyData”).\nWe can find the relevant columns by using the dplyr select helper contains. We then use gather to tidy the data for analysis, filter for only the non-NAs, and remove the WorkChallengeFrequency from each question using stringr::str_remove.\n\nWorkChallenges <- multiple_choice_responses %>%\n  select(contains(\"WorkChallengeFrequency\")) %>%\n  gather(question, response) %>%\n  filter(!is.na(response)) %>%\n  mutate(question = stringr::str_remove(question, \"WorkChallengeFrequency\")) \n\nWorkChallenges\n\n# A tibble: 31,486 × 2\n   question response        \n   <chr>    <chr>           \n 1 Politics Rarely          \n 2 Politics Often           \n 3 Politics Often           \n 4 Politics Often           \n 5 Politics Rarely          \n 6 Politics Most of the time\n 7 Politics Often           \n 8 Politics Often           \n 9 Politics Rarely          \n10 Politics Most of the time\n# … with 31,476 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nLet’s make a facet bar plot, one for each question with the frequency of responses.To make the x-axis tick labels readable, we’ll change them to be vertical instead of horizontal.\n\nggplot(WorkChallenges, aes(x = response)) + \n  geom_bar() + \n  facet_wrap(~question) + \n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\n\n\n\nThis graph has two main problems. First, there are too many histograms for it to be really useful. But second, the order of the x-axis is wrong. We want it to go from least often to most, but instead rarely is in the middle. We can manually reorder the level of this variable using forcats::fct_relevel.\n\nWorkChallenges %>%\n  mutate(response = fct_relevel(response, \"Rarely\", \"Sometimes\", \n                                \"Often\", \"Most of the time\")) %>%\n  ggplot(aes(x = response)) + \n  geom_bar() + \n  facet_wrap(~question) + \n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\n\n\n\nNow we’ve got the x-axis in the order we want it. Let’s try dichotomizing the variable by grouping “most of the time” and “often” together as the person considering something a challenge. We can use if_else and %in%. %in% is equivalent to response == \"Most of the time\" | response == \"Often\" and can save you a lot of typing if you have a bunch of variables to match.\nGrouping by the question, we can use summarise to reduce the dataset to one row per question, adding the variable perc_problem for the percentage of responses that thought something was a challenge often or most of the time. This way, we can make one graph with data for all the questions and easily compare them.\n\nperc_problem_work_challenge <- WorkChallenges %>%\n  mutate(response = if_else(response %in% c(\"Most of the time\", \"Often\"), 1, 0)) %>%\n  group_by(question) %>%\n  summarise(perc_problem = mean(response)) \n\n\nggplot(perc_problem_work_challenge, aes(x = question, y = perc_problem)) + \n  geom_point() +\n  coord_flip()\n\n\n\n\nThis is better, but it’s hard to read because the points are scattered all over the place. Although you can spot the highest one, you then have to track it back to the correct variable. And it’s also hard to tell the order of the ones in the middle.\nWe can use forcats:fct_reorder to change the x-axis to be ordered by another variable, in this case the y-axis. While we’re at it, we can use scale_y_continuous andscales::percent to update our axis to display in percent and labs to change our axis labels.\n\nggplot(perc_problem_work_challenge, \n       aes(x = perc_problem, \n           y = fct_reorder(question, perc_problem))) + \n  geom_point() +\n  scale_x_continuous(labels = scales::percent) + \n  labs(y = \"Work Challenge\", \n       x = \"Percentage of people encountering challenge frequently\")\n\n\n\n\nMuch better! You can now easily tell which work challenges are encountered most frequently.\n\n\nConclusion\nI’m a big advocate of using and teaching the tidyverse for data analysis and visualization in R (it runs in the family). In addition to doing these talks, I’ve released a DataCamp course on Categorical Data in the Tidyverse. I walk through some of the functions in this course and more from forcats. It’s part of the new Tidyverse Fundamentals skill track, which is suitable for people are new to R or those looking to switch to the tidyverse. Check it out and let us know what you think.\nSome other good resources for learning the tidyverse are Hadley Wickham and Garrett Grolemund’s free R for Data Science book and RStudio’s cheat sheets. If you have questions, I recommend using the tidyverse section of RStudio community and/or the #rstats hashtag on Twitter. If you do, make sure you include a reproducible example (see best practices here) with the reprex package!"
  },
  {
    "objectID": "posts/2021-04-06_publishing-a-technical-book-part-1/index.html",
    "href": "posts/2021-04-06_publishing-a-technical-book-part-1/index.html",
    "title": "Publishing a Technical Book (Part 1): Why Do It?",
    "section": "",
    "text": "Following the sage wisdom of brother Dave to write a blog post when you’ve given the same advice three times, after I had a few people reach out to me with questions about writing a technical book, I’ve put together my thoughts in this four-part series a year after publishing Build a Career in Data Science with Jacqueline Nolis.\nIn April 2018, Jacqueline reached out to me about whether I would be interested in writing a book. We had met at the DataDay Texas conference where we were both speakers that January and since then Manning, a book publisher, had reached out to her about writing a data science book. She was interested in having a co-author and thought we could write a good book together. We started writing the proposal in May 2018, signed the contract in August 2018, finished writing in December 2019, and got our hands on the physical copies in March 2020.\nA disclaimer: while I put the book Jacqueline and I wrote, Build a Career in Data Science, more on the technical side than not, we had no code in our book except a few excerpts in our appendix. That means we didn’t have a technical editor, need to do any statistics or programming research, or worry about formatting code.\n\nWhy write a book?\nI once heard someone say that you write a book because you can’t not write it. I would say that was pretty true for me. I felt strongly that there was a need for a book that offered career guidance to data scientists. While there are a ton of free and paid resources for technical topics ranging from getting started programming in R to deep learning with Tensorflow to ggplot2 cookbooks, when people asked for advice about negotiating a job offer or dealing with stakeholders, I at most could gather up a few scattered blog posts. And then there were the “unknown unknowns” - things people weren’t even thinking about but were actually very important for their career, like sponsorship.\nVicki Boykis recently wrote a post about “ghost knowledge”, “knowledge that exists within expert communities but is never written down and basically doesn’t exist for you unless you have access to those communities.” Some of the career advice we include in our book, like how to write a good resume or ace a behavioral interview, has been written about extensively in general career books, but not everyone has read those and there were still some data science specific points we could add. But other topics, such as how working as a data scientist differs between companies or how to handle when a project fails, are something you often learn only after years of working in the field.\nAspiring data scientists often focus on the technical knowledge they need, but especially as you advance in your career, it’s often skills like communication, managing up, and picking the right things to work on that matter. Even when you’re looking for your first job, you’re usually not held back because you don’t have enough technical skills; it’s because you’re applying for the wrong jobs, or aren’t presenting your story well in your resume and cover letter, or aren’t leveraging your network.\nBefore starting the book, I’d already written some blog posts focused around career advice, which to see if there’d be an audience for the topic as well as practicing my writing. But writing a book instead of just more blog posts gave me the time and space to really think comprehensively about data science careers. A book also expanded our audience reach, since it’s listed on Amazon and Manning and people who never would have read my blog might find it by searching “data science career” on those sites.\nOverall, I wanted to create the guidebook that I wish I had when I started my career as well as the guidebook I did have because I was lucky enough to have a brother already in data science, who among other things encouraged me to take statistics in college, where the introductory class happened to be taught by one Hadley Wickham. I’ve learned a ton over the past years of getting into and working in data science, and I am extremely against the gatekeeping that is too common in tech. I’d much rather live by this quote from Michelle Obama:\n\n\n❤️❤️❤️@robinson_es's latest post! itreminds me of M. Obama's: \"…when you’ve worked hard, and done well, and walked through that doorway of opportunity, you do not slam it shut behind you. You reach back and you give other folks the same chances that helped you succeed.\" https://t.co/HivEvImw5s\n\n— Jesse Mostipak (@kierisi) February 15, 2018\n\n\nI’m always so happy when people reach out to say how the book has helped them, whether by making data science accessible, providing a blueprint for entering the field, and even get promoted to principal data scientist. I do recognize that the cost of the book can be prohibitive though, which is one of the reasons Jacqueline and I made a podcast where we cover a chapter per episode, sharing the higlights and adding our own personal stories.\n\n\nDon’t write a book for the money\n“Do you like ice cream? You’d likely make the same amount of money working in an ice cream shop for minimum wage as you would writing a book. And ice cream is delicious.” - Jacqueline Nolis\nSpeaking of money, the first thing I always say when people ask about writing a book (and even if they don’t) is that you shouldn’t write a book for the money. If you write a book you’ll probably make on the order of $5k-$15k over the first 4 years for 500+ hours of work.\nThe standard royalty rate is 10% for physical books, with some publishers offering 25% for e-books while others keep the rate at 10%. It seems pretty rare to successfully negotiate higher royalties, except for maybe slightly higher after you’ve sold X copies, where X is how many copies a decently successful book might sell in 4 years.\nAdvances are usually fairly small - I believe $5,000 is standard, half when you deliver the first third of the book and half when you deliver the final draft. I think it’s highly unlikely the publisher would give an advance they would not expect to earn back within a year or two. Generally tech book authors don’t quit their day jobs, so they don’t need the advance to pay for expenses while writing. The royalties you earn are counted against your advance. So if you get a $5,000 advance and have a 10% royalty rate, the first $50,000 your book sells earns you no royalties, and every $1,000 after gets you $100. If you have a co-author, you’ll split the advance and royalties (e.g. you each get 5%, not both getting 10%) - at least with Manning, they let the authors determine the split.\nIn terms of how much you can expect to sell, it obviously really depends on your topic. Martin Kleppman, author of Designing Data-Driven Applications, wrote a great post sharing how many copies his book has sold and how much he’s received in royalties. His book is one of the most successful tech books out there and has made him almost half a million dollars over 6 years, but he also spent the equivalent of 2.5 years writing it, including 1 year working full-time on it with no other income. As he even says in his post, he’d recommend valuing your royalties at close to zero. Most tech book authors don’t even make a 10th of what he’s made.\nWhile it’s great to get some passive income from royalties, if your motivation is financial you have to weigh that against the opportunity cost of how else you could have earned money in the time you spent writing the book. It’s my bet that most people who can write a tech book could make (a lot) more money consulting.\nThat said, there can be financial benefits beyond the royalties. If you want to start consulting, give paid workshops, or get a higher paid job, having literally written the book on a topic is a great way to show you’re an expert. Some publishers like Manning also offer affiliate programs, where you get 8% of any purchases (not just your own book) made by someone who landed on Manning from your affiliate link, in addition to any royalties.So far, Jacqueline and I have each made about $900 (pre-tax).\nRemember that writing a book isn’t your only option for creating public work. Besides writing a blog or giving talks, you could make an interactive online course with LinkedIn or O’Reilly or even by yourself with learnr or binder.\nIf you do want to write a book though, continue to part 2 for how to write a book proposal and find a publisher."
  },
  {
    "objectID": "posts/2018-08-07_guidelines-for-a-b-testing/index.html",
    "href": "posts/2018-08-07_guidelines-for-a-b-testing/index.html",
    "title": "Guidelines for A/B Testing",
    "section": "",
    "text": "When I was working at Etsy, I benefited from a very robust A/B testing system. Etsy had been doing A/B testing for more than 6 years. By the time I left, Etsy’s in-house experimentation system, called Catapult, had more than 5 data engineers working on it full-time. Every morning, I was greeted with a homepage that listed all the experiments that Etsy had run in the prior four years. When you clicked on one, you got a summary of what the experiment was testing (usually written by the product manager). Numbers for all the key metrics, such as conversion rate and add to cart rate, were already calculated. You could easily add any event that happened on the site and get those rates calculated too. You got to see the % change from the control to treatment with its accompanying p-value and how many days until we had 80% power to detect a 1% change. We even had beautiful little confidence intervals that changed color based on whether they overlapped with zero!\nAnd yet sometimes I would spend a majority of my time working on experiments, even though I tried to never be working on more than 3 or 4 at once. How could that take so long when so much was already done for me? The concept of A/B Testing seems pretty simple. A classic example is you change the color of a button and measuring if the click-rate changes. Assuming your assignment of visitors and data collection is working, all you need to do is run a proportion test, right? And if you already have the proportion test calculated, why is a data scientist even needed? Maybe you need one if you want to do some fancy techniques like multi-armed bandits, but how can classic, frequentist A/B Testing be a challenge?\nUnfortunately, “generating numbers is easy; generating numbers you should trust is hard!” There’s many ways A/B Testing can go wrong, but most of them won’t be obvious.\nThis post outlines some recommended best practices for A/B Testing. I’ve found that a lot of analysts and data scientists struggle with A/B testing, especially those not classically trained in statistics or who are trying to start their company’s A/B testing system. While A/B testing correctly isn’t easy, these 12 guidelines will help you guard against some common mistakes and set you up for success."
  },
  {
    "objectID": "posts/2018-08-07_guidelines-for-a-b-testing/index.html#guidelines-for-ab-testing",
    "href": "posts/2018-08-07_guidelines-for-a-b-testing/index.html#guidelines-for-ab-testing",
    "title": "Guidelines for A/B Testing",
    "section": "12 Guidelines for A/B Testing",
    "text": "12 Guidelines for A/B Testing\n\nHave one key metric for your experiment. You can (and should!) monitor multiple metrics to make sure you don’t accidentally tank them, but you should have one as a goal. Revenue is probably the wrong metric to pick. It is likely a very skewed distribution which makes traditional statistics tests behave poorly. See my discussion in my A/B testing talk (around the 23-minute mark). I generally recommend proportion metrics. First, you often you care more about the number of people doing something than how much they do it. Second, you don’t have to deal with outliers and changing standard deviations.\nWhy only one metric? Once you start testing many metrics, you end up with an increased false positive rate. While correction methods can counteract this, they make each test more conservative. That means you become less likely to detect a difference in any given test. One metric also makes decision-making clearer. What would you do if you have three “equally important” metrics, and two go up a small amount while the other goes down a substantial amount?\nUse that key metric do a power calculation. A common mistake in A/B testing is to run a test with such small traffic or base rate you’d need there to be a huge increase to be able to detect it within a week. To avoid this, run a power calculation first to determine how long it would take to detect an X% increase. You’ll need the current rate (if a proportion metric) or mean and standard deviation of your key metric, how many visitors you get daily, what type of change you’re aiming to get (1% increase? 5%?), the percentage of people you’ll be allocating to your new version (e.g. are you doing 50/50 or 75/25 split), desired level of power (usually 80%), and the significance threshold (usually 95% or 90%). If you’re doing a proportion metric, experimentcalculator.com is good for this.\nTwo things will generally happen: 1) you’ll find that it will take a few days or weeks or 2) you’ll find that it will take 3 years, 5 months, and 23 days. If the latter happens, you may either have to go for a different metric with a higher baseline rate or decide you only care about bigger changes. For example, you can decide that it’s okay that you can’t detect a 5% increase clicks because only a 10% or greater increase is meaningful. If you want to learn more about power, check out Julia Silge’s excellent introductory post. She even created a shiny app so you can calculate how your power level changes with your effect size and population.\nRun your experiment for the length you’ve planned on. You should monitor it in the first few days to make sure nothing exploded, but plan on running it for the length you planned on in your power calculation. Don’t stop as soon as something is significant or you will get a lot of false positives. See the review section in Dave Robinson’s Bayesian A/B Testing blog post. Don’t be another p-hacking statistic:\n\n\n\nWhen p-hacking is taken into account, 73% of A/B tests on Optimizely have no effect https://t.co/YNsx6IGBfk pic.twitter.com/xeLnOal3ba\n\n— Josh Kalla (@j_kalla) July 18, 2018\n\n\n\nPay more attention to confidence intervals than p-values. They have a 1-1 relationship such that if the p-value is less than .05, the 95% confidence interval does not overlap with 0. But if the confidence interval is wide and very close to zero, you’ve got a lot less evidence of a change than if it’s tiny and far away.\nDon’t run tons of variants. Say you want to redesign your homepage and your designers come up with six possibilities. How do you pick one? Well, that’s what A/B Testing is for, right? Wrong. You will lower your ability to detect a statistical effect, as each group will have fewer people in it. You’ll also raise the likelihood of a false positive if you simply test the control against each treatment group. As a rule of thumb, stick to only a treatment and control most of the time and don’t go more than four total groups (control and three variations).\nDon’t try to look for differences for every possible segment. If your test doesn’t work overall, it can be tempting to hold out hope that it actually did, just not for everyone. Or even if your A/B tests did succeed, you may want to know if it was driven by a big change in one segment. Did we help US visitors? New visitors? Visitors on Saturday? Down that road lies the madness of false positives from multiple testing, also known as detecting differences in health based on astrological signs. If you really think there will be a difference, either pre-specify your hypothesis or run separate tests (e.g. one for new visitors and one for returning).\nCheck that there’s not bucketing skew. Bucketing skew, also known as sample ratio mismatch, is where the split of people between your variants does not match what you planned. For example, maybe you wanted to split people between the control and treatment 50/50 but after a few days, you find 40% are in the treatment and 60% in the control. That’s a problem! If you have lots of users, even observing 49.9% in the control and 50.1% in the treatment can indicate a problem with your set-up. To check if you have an issue, run a proportion test with the number of visitors in each group and check if your p-value is less than .05. If you do have bucketing skew, you have a bug. Unfortunately, it can be difficult to find it, but a good place to start is checking is if the skew differs based on web browser, country, or another visitor factor. Also check if your treatment is significantly slower; it may be that users with slow connections are dropping out before they get bucketed into the treatment. Finding the bug and rerunning the test is very important because generally users aren’t going missing at random. If you’re systematically dropping people who use internet explorer in the treatment, who also never buy your product, your conversion rate will look artificially better because the population in the control vs. treatment is different.\nDon’t overcomplicate your methods. Maybe you have engineers who’ve read about multi-armed bandit testing, stats nerds who want to use Bayesian methods, or product managers who want the key metric to be a complicated sequence of behaviors. If you’re just starting out A/B testing methods, focus on getting the basic, frequentist methods right. Even after a few years, it’s usually better to invest in experiment design and education rather than fancy statistical methods.\nBe careful of launching things because they “don’t hurt”. There may actually be a negative change that’s too small to detect but could have a meaningful effect in the long-term. When deciding whether to launch on “neutral,” the first step is to look at your non-key metrics. If other metrics you care about have been impacted negatively, you’ll probably want to rollback. If not, this is where your product intuition and other data can come in. Is this a change users have been asking for? Does it set the foundation for future changes you want to make? In general, default to rolling it back. This is also where your power analysis comes in - whatever increase you had 80% power to detect, would you be okay if you launch and you actually had decrease of that same size? The smaller changes you were set to detect, the less risky launching on neutral is. You could also look into non-inferiority testing, which is designed to test that your treatment is not worse than the control by a pre-specified amount. While I haven’t used it before, this looks like a good resource.\nHave a data scientist/analyst involved in the whole process. As Sir R. A. Fisher once said, “to consult the statistician after an experiment is finished is often merely to ask [them] to conduct a post mortem examination. [They] can perhaps say what the experiment died of.” If a team tries to bring in a data scientist after they launched an experiment, they may find the data doesn’t exist to measure their key metric, the test is severely underpowered, or there’s a design flaw that means they can’t draw any conclusions.\nOnly include people in your analysis who could have been affected by the change. If you have users in your experiment whose experience could not have been impacted by your change, you’re adding noise and reducing your ability to detect an effect. For example, if you’re changing the layout of the search page, only add users to the experiment if they visit the search page. In a more complicated example (from this great paper on triggering), let’s say you want to experiment with changing the threshold for a free shipping offer (displayed only when they meet the criteria) from $35 to $25. You should only put users in the experiment who have cart sizes between $25 to $35 because those are the only people who would see something different in the treatment vs. control group. Relatedly, start tracking your metrics after the user sees the relevant page. Imagine you’re running an experiment on the search page, and someone visits your sites, buys something from the homepage, and then visits the search page, entering the experiment. You don’t want to count their earlier conversion, as it could not have been a result of your change.\nFocus on smaller, incremental tests that change one thing at a time. It’s very tempting to launch big changes or a bundle of smaller changes in the hope that they result in big wins. But the problem is that you will often invest tons of effort up front only to find out your change doesn’t work. And when it doesn’t, it’s hard to figure out why - was it just one part that failed? Or was it an interaction of the changes? A better practice is to split them up into smaller tests.\nDan McKinley, former principal engineer at Etsy, gives a great example of this problem in his presentation on continuous experimentation. His team spent weeks working on enabling infinite scroll for the search page. But when they ran the A/B test, they found it performed worse! Their first reaction was that it must be a bug, but while they did find some, the results remained unchanged. So they went back and tested the assumptions behind why they believed infinite scroll would be better. First, are more items actually better? When they changed just the number of items on the search page, they found there were more clicks, but the same number of purchases. Second, were faster results better? Nope, artificially slowing down the search page didn’t hurt anything. If they’d checked those first, they would not have invested in infinite scroll. They learned from this and changed to making a series of smaller design-develop-measure (with A/B tests) cycles culminating up to a big change.\n\n\n\n\nimage\n\n\n\nNext Time\nWhile I learned some of these guidelines in my statistics classes or experience at Etsy and DataCamp, others I picked up from the great set of A/B Testing resources available online. In a future post, I’ll share a list of some of my favorite papers, blog posts, and talks, with short summaries of what I took away and suggested audience level."
  },
  {
    "objectID": "posts/2017-09-28_managing-business-challenges-in-data-science/index.html",
    "href": "posts/2017-09-28_managing-business-challenges-in-data-science/index.html",
    "title": "Managing Business Challenges in Data Science",
    "section": "",
    "text": "A few weeks ago, I wrote about my experience giving my first data science talk. If you’re interested, the full talk is available online, as well as the slides. In this post, I wanted to share some suggestions for managing business challenges that I didn’t have time to cover in my talk."
  },
  {
    "objectID": "posts/2017-09-28_managing-business-challenges-in-data-science/index.html#why-business-challenges",
    "href": "posts/2017-09-28_managing-business-challenges-in-data-science/index.html#why-business-challenges",
    "title": "Managing Business Challenges in Data Science",
    "section": "Why Business Challenges?",
    "text": "Why Business Challenges?\nWhy devote a whole post and half a talk to business challenges instead of, say, cutting edge deep learning papers or the shiny new language for handling Big DataTM?\nWhile basic technical skills are the table stakes for a data science career, the importance of non-technical skills in a data science career is often overlooked. I think Yonatan Zunger, a former senior Google engineer, covers this very well in the second section of this post. While he’s talking about engineering, his points can be equally applied to data science:\n\nPeople who haven’t done engineering, or people who have done just the basics, sometimes think that what engineering looks like is sitting at your computer and hyper-optimizing an inner loop, or cleaning up a class API… Engineering is not the art of building devices; it’s the art of fixing problems… Essentially, engineering is all about cooperation, collaboration, and empathy for both your colleagues and your customers.\n\nAt Etsy, each of the analysts works with a partner team, including marketing, seller services, retention and loyalty, and finance. What we help them with can vary a lot. I mainly help design and analyze experiments for search, while other analysts create dashboards in Looker, new tables to make data access easier, or models for estimating the lifetime value of a new customer. While we may be especially deeply embedded with partner teams, all data scientists and analysts (or their managers) will have to work with non-analysts1 sometimes. While I still have a long way to go, I wanted to share some advice and techniques I’ve found helpful."
  },
  {
    "objectID": "posts/2018-02-14_the-importance-of-sponsorship/index.html",
    "href": "posts/2018-02-14_the-importance-of-sponsorship/index.html",
    "title": "The Importance of Sponsorship",
    "section": "",
    "text": "In my last post, I discussed the importance of building your network and some strategies for effectively reaching out. I closed with emphasizing how helpful your peers or people one step ahead of you can be. But there’s a specific area where people with more resources, status, or experience can help you: sponsorship."
  },
  {
    "objectID": "posts/2018-02-14_the-importance-of-sponsorship/index.html#what-is-sponsorship",
    "href": "posts/2018-02-14_the-importance-of-sponsorship/index.html#what-is-sponsorship",
    "title": "The Importance of Sponsorship",
    "section": "What is sponsorship?",
    "text": "What is sponsorship?\nWhen people discuss what they’re seeking from a more senior person in their field, they usually talk about “mentorship.” Generally, mentoring involves offering advice, helping someone adjust to their new job, or giving feedback on their work. Sponsorship, on the other hand, is all about giving people opportunities, whether by funding them, advocating for their promotion, introducing them to important people, or making sure they get assigned to the types of challenging projects that can help them grow.\nA lot of articles about sponsorship and mentorship are about finding one at your company, and this is especially important if you work at a larger organization that you want to stay in for many years. Nowadays, however, people (especially in tech) often change companies every few years. The tech community has enough small subpockets that you can start building a positive reputation and sponsors in the larger field who will stay with you through multiple companies."
  },
  {
    "objectID": "posts/2018-02-14_the-importance-of-sponsorship/index.html#how-do-you-find-a-sponsor",
    "href": "posts/2018-02-14_the-importance-of-sponsorship/index.html#how-do-you-find-a-sponsor",
    "title": "The Importance of Sponsorship",
    "section": "How do you find a sponsor?",
    "text": "How do you find a sponsor?\nLooking for a sponsor can be very awkward if you try to force it or really need one right now - desperation is rarely endearing. You want to build sponsorship relationships before you need them.\nI previously discussed how offering value and building a mutually fulfilling relationship is the best way to network. This applies doubly to sponsorship. If someone sponsors you, they need to be confident you will do a good job, because you’re representing them. For example, if someone recommends you as a speaker for a conference and you never respond to the organizer or you give a clearly unprepared talk, that reflects poorly on the sponsor.\nSo how can you show someone you’re up for the task? One of the best ways is to have public examples of your previous successful work. It doesn’t have to be evidence of you doing the exact same thing; otherwise, how could you ever get your first opportunity? Mikhil Popov wrote a great post on how to deal with this kind of Catch-22, specifically needing experience as a data scientist to get a job as a data scientist. He said that your experience doesn’t need to be at a job with the title “data scientist”; what experience really stands out is working on an end-to-end data project (gathering, cleaning, modeling, and visualizing). You can show you’ve done this by writing a blog post about how you wrangled real data and the cool analysis you did. Or if you’re interested in speaking at conferences, you can start with a lightning (short) talk at a local meetup and apply for conferences specifically for first-time speakers, such as Deconstruct."
  },
  {
    "objectID": "posts/2018-02-14_the-importance-of-sponsorship/index.html#how-can-you-be-a-sponsor",
    "href": "posts/2018-02-14_the-importance-of-sponsorship/index.html#how-can-you-be-a-sponsor",
    "title": "The Importance of Sponsorship",
    "section": "How can you be a sponsor?",
    "text": "How can you be a sponsor?\nThe best way to thank those who have helped you is to “pay it forward.” When you start to get more influence and opportunities, it becomes your responsibility to help pull others up behind you. For example, if you’re a seasoned speaker, think about recommending others for opportunities, especially when you can’t go yourself, like Julia Evans recently did:\n\n\nI get email about a lot of speaking opportunities. if you're looking to give more talks (especially if you're from a marginalized group), reply to this with a link to a video of a talk you're proud of!\n\n— 🔎Julia Evans🔍 (@b0rk) February 6, 2018\n\n\nFor more ideas, Caitlin Hudon has a great twitter thread:\n\n\nBelow are examples of \"good guys in tech\" and things they have done that have helped my career. I hope others can follow their example.\n\n— Caitlin Hudon👩🏼 💻 (@beeonaposy) August 11, 2017\n\n\nSome examples she gives include colleagues praising her work in front of her boss, sharing their salary information with her, and promoting the work of women, including by doing something as seemingly small as tweeting about it. When Caitlin started RLadies Austin, she had men reaching out to offer their time, training materials, meeting spaces, and promotion of the meetup.\nSponsorship is important for everyone, but some groups are disproportionality less likely to receive it. For example, research has found that women are overmentored and undersponsored and that men’s mentors are more likely to be senior. Mentorship is great, but often people need opportunity and visibility, not more advice.\nIf you’re a man and looking for more ways to be an ally to women and non-binary people, my colleagues at Etsy have written a great guide. It’s worthwhile to read the whole thing, but here’s an excerpt about some concrete things you can do1:"
  },
  {
    "objectID": "posts/2018-02-14_the-importance-of-sponsorship/index.html#my-experience",
    "href": "posts/2018-02-14_the-importance-of-sponsorship/index.html#my-experience",
    "title": "The Importance of Sponsorship",
    "section": "My experience",
    "text": "My experience\nMy own career shows how sponsorship can be incredibly important. Some ways that I’ve been sponsored include:\n\nGetting my foot in the door for my first data science job because Hilary Parker introduced me to her former colleague at Etsy, who referred me.\nAttending my first data science conference, RStudio::conf 2017, thanks to a diversity scholarship from RStudio that allowed me to go to conference and training days for free and included a small travel budget.\nGiving my first data science talk because Jared Lander asked me when, not if, I wanted to speak at his meetup. He believed I could do it before I did and has been a very enthusiastic champion. The talk was recorded and was seen by the Director of the Data Science Center at West Point, who invited me to give a talk there last September (it was awesome).\nGiving my first data science conference talk at Data Day Texas two weeks ago because Hilary Parker and other invited speakers recommended me.\n\nActions that may not rise to the level of sponsorship can also make a big impact. Specifically, I want to thank Hadley Wickham, for encouraging me to submit my talk to RStudio::conf 2018:\n\n\nYou should submit it to rstudio::conf!\n\n— Hadley Wickham (@hadleywickham) August 24, 2017\n\n\nAnd Jim Hester citing me as one of the contributors to the glue documentation.\n\n\nWant to give a belated shoutout to @jimhester_ and the tidyverse team for providing a really nice experience to a OS newbie:A month ago, I was trying to use a new function in glue and getting an error it didn’t exist. Colleague was pinging me about same error. 1/\n\n— Emily Robinson (@robinson_es) November 15, 2017\n\n\nFinally, I am incredibly lucky to have my amazing brother Dave Robinson. He has been my most encouraging data science sponsor and is a tireless advocate for women in data science in general. His tweets on international women’s day got all of those mentioned a lot more followers:\n\n\nFive amazing #rstats data scientists to follow if you're not already:@juliasilge @JennyBryan @hspter @ma_salmon @robinson_es #IWD2017 pic.twitter.com/OWqi0PzyWM\n\n— David Robinson (@drob) March 8, 2017"
  },
  {
    "objectID": "posts/2018-02-14_the-importance-of-sponsorship/index.html#next-time",
    "href": "posts/2018-02-14_the-importance-of-sponsorship/index.html#next-time",
    "title": "The Importance of Sponsorship",
    "section": "Next Time",
    "text": "Next Time\nI’m learning to be cautious predicting what my next posts will be, but I do have a couple ideas I’m drafting, including an introduction to A/B Testing for data scientists and analysts and how to prioritize and respond to the many ad-hoc requests you can get as a data scientist partnering with business teams.\n\n\nCan you please blog about this\n\n— David Robinson (@drob) February 15, 2018\n\n\nIf you found this post useful, you might be interested in the book on data science careers I wrote with Jacqueline Nolis, “Build Your Career in Data Science,”, avalable for 40% off with code buildbook40%."
  },
  {
    "objectID": "posts/2017-01-19_rstudio-conference-recap/index.html",
    "href": "posts/2017-01-19_rstudio-conference-recap/index.html",
    "title": "RStudio Conference Recap",
    "section": "",
    "text": "Last week I spent four amazing days in Orlando at the first ever rstudio::conf. I learned a ton, met some really cool people, connected with several I hadn’t seen in a while, and came out feeling ready to take on the world.\nI’ve divided my summary of the conference into two parts. This first one shares my personal experience and some more general learnings, while the other one has quick, bullet-pointed lists on writing functions, packages, tools, and functions I learned, and general tips and tricks. I also learned a lot about writing packages on the second day of Hadley’s Master R Developer workshop, which will be included in a future post on writing my first package.\nYou can find most of the slides from the conference here, organized by Karl Broman (who was greatly missed), and videos of all the talks here."
  },
  {
    "objectID": "posts/2017-01-19_rstudio-conference-recap/index.html#diversity-and-inclusion-in-r",
    "href": "posts/2017-01-19_rstudio-conference-recap/index.html#diversity-and-inclusion-in-r",
    "title": "RStudio Conference Recap",
    "section": "Diversity and Inclusion in R",
    "text": "Diversity and Inclusion in R\nFirst, I want to thank RStudio for providing the diversity scholarship that enabled me to attend the conference at all. When I applied for the scholarship, I was just finishing up Metis and was still months away from joining my current company, Etsy. Without a company to pay for my ticket and not knowing when I would start earning a salary again, I could not have covered the $1000+ for the conference ticket and travel, and certainly not the training day ticket.\nThe availability of rstudio::conf diversity scholarships for women, LGBT+, and underrepresented minorities reflects the broader inclusion efforts I’ve seen in the R community. We have the newly-rebranded R Forwards taskforce, which changed their name from the R Foundation Task Force on Women so that they could “consider the needs of other marginalised groups as part of our mission.” There is also the R-Ladies organization, which has meet-up groups all around the world. UseR!, one of the main R conferences, is providing childcare options this year to make it easier for primary caregivers to attend. Overall, members of the R community seem aware of diversity issues and supportive of efforts to make the community more inclusive. That being said, there’s always more work to be done, as emphasized in this report on the demographics of the useR! attendees. I hope to do some of my part for these efforts by giving at least one R tutorial for the R-Ladies New York group this spring.\nI met many kick-ass R Ladies at the conference, including Jenny Bryan, Charlotte Wickham, Hilary Parker, and these awesome graduate students from Vanderbuilt:\n\n\nComing at you strong from @RLadiesNash - excited to kickoff #rstudioconf pic.twitter.com/1DuGzx1w3h\n\n— Lucy (@LucyStats) January 13, 2017\n\n\nOne person I was especially excited to meet was Julia Silge, who works with my brother Dave at StackOverflow. I’d been following her on Twitter and reading her blog for a while, but we had never met in person. In fact, this conference was only the second time she met Dave in person, even though he hired her as the second Data Scientist at Stack Overflow and developed the tidy text package with her!\n\n\n@hspter @robinson_es @jaredlander A delight to meet you all in person, and @drob now for a GRAND TOTAL TWO TIMES.\n\n— Julia Silge (@juliasilge) January 15, 2017\n\n\nJulia only started using R about a year and a half ago, but she already has a great and active blog, a popular package, and a new career as a Data Scientist. While her strong previous scientific training (including an Astronomy PhD) certainly helped her, she’s a great example of how you can transition into data science later in your career without necessarily having a statistics or computer science degree."
  },
  {
    "objectID": "posts/2017-01-19_rstudio-conference-recap/index.html#training-days",
    "href": "posts/2017-01-19_rstudio-conference-recap/index.html#training-days",
    "title": "RStudio Conference Recap",
    "section": "Training Days",
    "text": "Training Days\nThere were three training day workshops available: Master R Developer with Hadley Wickham, Intermediate Shiny with Joe Cheng, and Master the Tidyverse with Garrett Grolemund. I was already familiar with most of the material in the Tidyverse workshop, and while I definitely want to learn more Shiny, I decided to take the Master R Developer workshop. I’ve been meaning to learn more about making packages, the focus of the second day, and I always enjoy getting the chance to learn from Hadley again:\n\n\nGreat to be learning again in-person, six years later, from my first stats professor @hadleywickham pic.twitter.com/AttTrTaOFm\n\n— Emily Robinson (@robinson_es) September 13, 2016\n\n\nThere were about 70 people in the Master R Developer workshop. Hadley ran a fairly interactive workshop, with lots of exercises to work on by yourself or with your neighbor. This was a great way to try out what you’d just learn, figure out what you didn’t understand, and get help from the couple of TAs if needed.\nThe first day had some material I’d already heard in Hadley’s meetups, but it’s always helpful to get a refresher. Other material, such as information on Object-Oriented Programming and S3 classes, was new. The second day on packages was entirely new to me and a great way to help overcome inertia and finally start writing my own packages.\nMuch of the information Hadley covered is available in two of his free online books, Advanced R and R Packages. But I personally prefer learning programming from a lecture and interactive exercises to reading a book, and going to the training days also forced me to work on the material and allowed you to ask questions (and learn from others’ questions). Hadley also made some off-the-cuff remarks that, because of his long experience with R, were very helpful. Finally, the workshop was just plain enjoyable, and a great way to spend two days."
  },
  {
    "objectID": "posts/2017-01-19_rstudio-conference-recap/index.html#main-conference",
    "href": "posts/2017-01-19_rstudio-conference-recap/index.html#main-conference",
    "title": "RStudio Conference Recap",
    "section": "Main Conference",
    "text": "Main Conference\nOutside of the two keynotes and closing panel, the conference had two parallel tracks of talks. This meant I couldn’t get to them all, although I plan to watch a few I missed once the videos are uploaded.\nIt was a great, diverse line-up of speakers; some of my favorite talks were Hilary’s talk on Opinionated Analysis Development, Bob Rudis’ on Writing Readable Code with Pipes, Jenny Bryan’s on List-Columns, Julia Silge’s on Text Mining, the Tidy Way, and Jonathan McPherson’s on R Notebook Workflow. If you’re interested, I would suggest watching the videos rather than just reading the slides so you can get the full impact; each talk is only about 20 minutes. I picked up a lot of technical tips and tricks from the content of the talks (see my other post for these), but I could always have watched these later online. What I couldn’t have gotten is a full immersion into the R community.\nTo me, getting to meet and talk with new people and non-New York friends was one of the best parts of the conference. At lunch one day I met James, a Columbia professor, and we realized we were on the same flight back to New York. He asked how I was getting to the airport, and I told him I was sharing a cab with my brother, whose work was paying for the ride (Thanks Stack Overflow!), and asked if he wanted to join us.\nHe and Dave quickly hit it off, with my brother noticing that James seemed to know a lot about one of Dave’s packages, gganimate. About halfway through the ride, James said, “Yeah, a couple months ago I used it to animate NBA plays.” At which point, Dave asked, “Your last name is Curley, isn’t it?” It turns out Dave had been following James on twitter for a while and had meant to find him at the conference, but hadn’t made the connection. They spent the next few hours as we waited for our delayed flight in animated discussion while I enjoyed playing Super Mario Run."
  },
  {
    "objectID": "posts/2017-01-19_rstudio-conference-recap/index.html#harry-potter-world",
    "href": "posts/2017-01-19_rstudio-conference-recap/index.html#harry-potter-world",
    "title": "RStudio Conference Recap",
    "section": "Harry Potter World",
    "text": "Harry Potter World\nI’d be remiss if I didn’t mention the amazing night at Harry Potter world, where we had all of Hogsmeade to ourselves, as RStudio had rented out the park for the evening. Hilary Parker took some beautiful pictures on her DSLR that captured the mood:\n\nIt was a bit bizarre to go on a roller coaster that had more staff members than riders, but I definitely didn’t miss the lines. I was even picked to get a free wand from Ollivander’s (Vine with Unicorn hair), which you can use to interact with shop windows in the park. But again, whether exploring Hogwarts, enjoying a butter beer, or grabbing the front row of a ride, the experience was made all the richer for the people I got to enjoy it with:"
  },
  {
    "objectID": "posts/2017-01-19_rstudio-conference-recap/index.html#wrap-up",
    "href": "posts/2017-01-19_rstudio-conference-recap/index.html#wrap-up",
    "title": "RStudio Conference Recap",
    "section": "Wrap-Up",
    "text": "Wrap-Up\nFor those considering going to rstudio::conf next year, I hope this gave you a better idea of what the experience was like. I want to emphasize again how friendly everyone at this conference was, even prominent and notable members of the community. If you’re new to conferences, don’t be shy about introducing yourself after talks or at the lunch table.\nIn the final panel of Hadley Wickham, J.J. Allaire, and Joe Cheng, one of them said: “the strength of R is in the human factors. It’s possible for those who don’t have a conventional software background to be successful with R.” The R language maintainers, RStudio, and the many open-source package developers deserve much of the credit for this. But I think the final piece of really making R accessible is the rest of the great and friendly community. Whether it’s Stack Overflow power-users who enable you to find the answer when you Google questions, or the active bloggers who walk you through their code and analysis, or the Twitter users who share the newest packages and their insights, there are always people ready to help."
  },
  {
    "objectID": "posts/2016-07-05_from-social-scientist-to-data-scientist/index.html",
    "href": "posts/2016-07-05_from-social-scientist-to-data-scientist/index.html",
    "title": "From Social Scientist to Data Scientist",
    "section": "",
    "text": "I learned a lot in my two years at INSEAD getting a master’s in Organizational Behavior. Many of the skills and knowledge I gained there will be beneficial as a data scientist, from defining a problem to carrying out a thoughtful analysis to communicating to a range of audiences. But one skill I wasn’t developing was working in an industry-level coding environment.\nThis hit home one day this spring. In one of my classes, we had been writing code in STATA. The professor wanted us to be able to see the code the other students had written, so, naturally, he suggested we do it through a shared dropbox folder. I don’t think he had ever heard of Github, let alone tried to use it.\nThis was one of the downsides of being in stand-alone business school, especially one in a small foreign town. We didn’t have a Statistics or Computer Science program, and there was no community of programmers outside the school either. A few PhD students did use Python extensively in their research, and two of them even put together a 2-day beginner Python workshop for professor and PhD students. On the first day, the student teacher showed how you could use pygeocoder to get the location of a list of universities. A professor sitting in on the class exclaimed, “I’ve spent five hours doing that before!” While learning programming at INSEAD is becoming more common and encouraged, almost all of it was still self-taught, with no best practices, formal curriculum, or expert professors available."
  },
  {
    "objectID": "posts/2016-07-05_from-social-scientist-to-data-scientist/index.html#starting-off-in-r",
    "href": "posts/2016-07-05_from-social-scientist-to-data-scientist/index.html#starting-off-in-r",
    "title": "From Social Scientist to Data Scientist",
    "section": "Starting off in R",
    "text": "Starting off in R\nI was fortunate to attend Rice University and take classes from and designed by Hadley Wickham, the most prominent developer of packages for R (he has even been referred to as the man who revolutionized R). I didn’t fully appreciate the amazing opportunity I was receiving at that time, despite my brother David telling me how jealous he was that I personally knew the great Hadley Wickham. Now David has gotten to know Hadley as well through his creation of the broom package, which tidies the output of R functions like linear regression and t-tests into dataframes. Broom has now even become a core part of the what Hadley would like us to call “the tidyverse” (instead of the “Hadleyverse,” as is the current practice). But at least I can always still say I knew Hadley first.\nTaking course designed by Hadley means that I’ve been programming in R for almost five years. I also took the introduction to computer science course, which used Python, but I never really kept up with it. While I still have a lot to learn in R, especially functional programming, I feel comfortable working with and growing in the “tidyverse.”\nThe same could not be said for Python. I took some online courses, but mostly I just continued using R. I decided what I needed was an immersive experience where I could work with Python while learning machine learning, web scraping, D3, cloud computing, and more. Coding bootcamps have proliferated over the last few years, with whole websites dedicated to helping people chose one. Metis’s data science bootcamp is a 12-week program that helps students transition into a career as a data scientist, and it seemed like the perfect environment for me."
  },
  {
    "objectID": "posts/2016-07-05_from-social-scientist-to-data-scientist/index.html#metis-so-far",
    "href": "posts/2016-07-05_from-social-scientist-to-data-scientist/index.html#metis-so-far",
    "title": "From Social Scientist to Data Scientist",
    "section": "Metis So Far",
    "text": "Metis So Far\nWell, it’s one week and one day in, and so far it’s been a great experience. The group of 25 students is amazingly diverse. Experience ranges from one year out of school to 15 or even 20 years in industry. Some people have mathematics or computer science degrees, a few have PhDs, while others learned to code mostly on their own. Each day we start off with 45 minutes of pair programming. We’re randomly assigned to a different person each day, so one day I can be guiding someone through accessing elements of a pandas dataframe and the other day learning the tricks of Jupyter notebook.\nI’m excited to continue my journey at Metis over the next few months. I am certain I’ll never have to be worried about being bored!"
  },
  {
    "objectID": "posts/2019-08-08_pokemon-matrices-and-combinations-oh-my/index.html",
    "href": "posts/2019-08-08_pokemon-matrices-and-combinations-oh-my/index.html",
    "title": "What Types Should You Have on Your Pokémon Team? Efficient Simulation with Matrices in R",
    "section": "",
    "text": "I recently started playing Pokémon again - “Pokémon Let’s Go Eevee” on the Nintendo Switch to be specific. In the classic Pokémon games, you have a team of 6 Pokémon that you use to battle against other trainers. In battles, type match-ups are very important, as some types of moves are “super effective” against other types. For example, fire moves are super effective against grass Pokémon, which means they do double the damage they normally would. If you can set your team up so that you’re always optimally matched, you’re going to have a much easier time.\nBut there are 18 types and you only get 6 Pokémon on your team. This leads to the question - what are the combinations of 6 types that make you super effective against the most types of Pokémon?1 It turns out this is a question a lot of people have asked.\nI knew there was a chart out there that matches up every attacking type against every defending and tells you whether they’re super effective, normal, not very effective, or doesn’t have any effect. So I decided to use my R skills to answer this question (many thanks to my brother David Robinson for his guidance at various points). Along the way, we’ll do a quick exploratory analysis, learn about combinatorials, and leave the tidyverse to use matrices and some base functions."
  },
  {
    "objectID": "posts/2019-08-08_pokemon-matrices-and-combinations-oh-my/index.html#data-exploration",
    "href": "posts/2019-08-08_pokemon-matrices-and-combinations-oh-my/index.html#data-exploration",
    "title": "What Types Should You Have on Your Pokémon Team? Efficient Simulation with Matrices in R",
    "section": "Data Exploration",
    "text": "Data Exploration\nI found a csv of the Pokémon type chart on GitHub. Using read_csv() on the url didn’t work, and rather than try to debug it, I decided to “cheat”” and use the magic package datapasta package. On Github, I clicked to edit the file, copied everything in it, and then used tribble_paste(), which output my clipboard into the code that would create a tibble I called type_comparisons.\n[Added 8/26]: As Jim Hester kindly pointed out, read_csv() will work if I use it on the raw link, generated by clicking the “Raw” button.\n\nlibrary(tidyverse)\n# this didn't work\n# type_comparisons <- read_csv(\"https://github.com/robinsones/pokemon-chart/blob/master/chart.csv\")\n\n\nlibrary(datapasta)\n# use tribble_paste()\ntype_comparisons <- tibble::tribble(\n     ~Attacking, ~Normal, ~Fire, ~Water, ~Electric, ~Grass, ~Ice, ~Fighting, ~Poison, ~Ground, ~Flying, ~Psychic, ~Bug, ~Rock, ~Ghost, ~Dragon, ~Dark, ~Steel, ~Fairy,\n       \"Normal\",       1,     1,      1,         1,      1,    1,         1,       1,       1,       1,        1,    1,   0.5,      0,       1,     1,    0.5,      1,\n         \"Fire\",       1,   0.5,    0.5,         1,      2,    2,         1,       1,       1,       1,        1,    2,   0.5,      1,     0.5,     1,      2,      1,\n        \"Water\",       1,     2,    0.5,         1,    0.5,    1,         1,       1,       2,       1,        1,    1,     2,      1,     0.5,     1,      1,      1,\n     \"Electric\",       1,     1,      2,       0.5,    0.5,    1,         1,       1,       0,       2,        1,    1,     1,      1,     0.5,     1,      1,      1,\n        \"Grass\",       1,   0.5,      2,         1,    0.5,    1,         1,     0.5,       2,     0.5,        1,  0.5,     2,      1,     0.5,     1,    0.5,      1,\n          \"Ice\",       1,   0.5,    0.5,         1,      2,  0.5,         1,       1,       2,       2,        1,    1,     1,      1,       2,     1,    0.5,      1,\n     \"Fighting\",       2,     1,      1,         1,      1,    2,         1,     0.5,       1,     0.5,      0.5,  0.5,     2,      0,       1,     2,      2,    0.5,\n       \"Poison\",       1,     1,      1,         1,      2,    1,         1,     0.5,     0.5,       1,        1,    1,   0.5,    0.5,       1,     1,      0,      2,\n       \"Ground\",       1,     2,      1,         2,    0.5,    1,         1,       2,       1,       0,        1,  0.5,     2,      1,       1,     1,      2,      1,\n       \"Flying\",       1,     1,      1,       0.5,      2,    1,         2,       1,       1,       1,        1,    2,   0.5,      1,       1,     1,    0.5,      1,\n      \"Psychic\",       1,     1,      1,         1,      1,    1,         2,       2,       1,       1,      0.5,    1,     1,      1,       1,     0,    0.5,      1,\n          \"Bug\",       1,   0.5,      1,         1,      2,    1,       0.5,     0.5,       1,     0.5,        2,    1,     1,    0.5,       1,     2,    0.5,    0.5,\n         \"Rock\",       1,     2,      1,         1,      1,    2,       0.5,       1,     0.5,       2,        1,    2,     1,      1,       1,     1,    0.5,      1,\n        \"Ghost\",       0,     1,      1,         1,      1,    1,         1,       1,       1,       1,        2,    1,     1,      2,       1,   0.5,      1,      1,\n       \"Dragon\",       1,     1,      1,         1,      1,    1,         1,       1,       1,       1,        1,    1,     1,      1,       2,     1,    0.5,      0,\n         \"Dark\",       1,     1,      1,         1,      1,    1,       0.5,       1,       1,       1,        2,    1,     1,      2,       1,   0.5,      1,    0.5,\n        \"Steel\",       1,   0.5,    0.5,       0.5,      1,    2,         1,       1,       1,       1,        1,    1,     2,      1,       1,     1,    0.5,      2,\n        \"Fairy\",       1,   0.5,      1,         1,      1,    1,         2,     0.5,       1,       1,        1,    1,     1,      1,       2,     2,    0.5,      1\n     )\n\nTo make it easier to explore the data, I’m going to start by tidying it.\n\ntidied_comparison <- type_comparisons %>%\n  gather(Defending, outcome, -Attacking)\n\ntidied_comparison %>%\n  slice(103:109) %>%\n  knitr::kable()\n\n\n\n\nAttacking\nDefending\noutcome\n\n\n\n\nRock\nIce\n2\n\n\nGhost\nIce\n1\n\n\nDragon\nIce\n1\n\n\nDark\nIce\n1\n\n\nSteel\nIce\n2\n\n\nFairy\nIce\n1\n\n\nNormal\nFighting\n1\n\n\n\n\n\nWe now have a dataset of 324 rows, with each Attacking-Defending combination and what the outcome is. Here, outcome is 2 if it’s super effective (what we’re interested in), 1 if normal, .5 if not very effective, and 0 if no effect.\nWhat types are super effective against the most other types?\n\ntidied_comparison %>%\n  group_by(Attacking) %>%\n  summarize(nb_super_effective = sum(ifelse(outcome == 2, 1, 0))) %>%\n  arrange(desc(nb_super_effective)) %>%\n  knitr::kable()\n\n\n\n\nAttacking\nnb_super_effective\n\n\n\n\nFighting\n5\n\n\nGround\n5\n\n\nFire\n4\n\n\nIce\n4\n\n\nRock\n4\n\n\nBug\n3\n\n\nFairy\n3\n\n\nFlying\n3\n\n\nGrass\n3\n\n\nSteel\n3\n\n\nWater\n3\n\n\nDark\n2\n\n\nElectric\n2\n\n\nGhost\n2\n\n\nPoison\n2\n\n\nPsychic\n2\n\n\nDragon\n1\n\n\nNormal\n0\n\n\n\n\n\nFighting and Ground are both super effective against 5 different types, while Normal isn’t super effective against any.\nAre there any types where only one Attacking type is super-effective?\n\ntidied_comparison %>%\n  filter(outcome == 2) %>%\n  add_count(Defending) %>%\n  arrange(n) %>%\n  head(4) %>%\n  knitr::kable()\n\n\n\n\nAttacking\nDefending\noutcome\nn\n\n\n\n\nFighting\nNormal\n2\n1\n\n\nGround\nElectric\n2\n1\n\n\nElectric\nWater\n2\n2\n\n\nGrass\nWater\n2\n2\n\n\n\n\n\nYes - if we want to be super effective against Normal and Electric types, we need Fighting and Ground types respectively."
  },
  {
    "objectID": "posts/2019-08-08_pokemon-matrices-and-combinations-oh-my/index.html#building-pokémon-teams",
    "href": "posts/2019-08-08_pokemon-matrices-and-combinations-oh-my/index.html#building-pokémon-teams",
    "title": "What Types Should You Have on Your Pokémon Team? Efficient Simulation with Matrices in R",
    "section": "Building Pokémon Teams",
    "text": "Building Pokémon Teams\nThe first step is to build out all the hypothetical teams of 6. If you remember your introduction to statistics days, this is a combinatorial problem: we have 18 options (although we don’t expect “Normal” to show up since it’s not super effective against anything), need to choose 6, and the order doesn’t matter (e.g. 1 to 6 is the same as 6 to 1). We can do this in R with the function combn:\n\nall_combinations <- combn(18, 6)\ndim(all_combinations)\n\n[1]     6 18564\n\n\nall_combinations is a 6 by 18,564 matrix: each column is a different combination of types. For example, let’s look at the first two columns:\n\nall_combinations[, 1:2]\n\n     [,1] [,2]\n[1,]    1    1\n[2,]    2    2\n[3,]    3    3\n[4,]    4    4\n[5,]    5    5\n[6,]    6    7\n\n\nThe first column is one team with the types 1 through 6, while the second is a team with 1 through 5 and 7.\nNow we need to take this and understand how many types each team is super effective against."
  },
  {
    "objectID": "posts/2019-08-08_pokemon-matrices-and-combinations-oh-my/index.html#matrix-magic",
    "href": "posts/2019-08-08_pokemon-matrices-and-combinations-oh-my/index.html#matrix-magic",
    "title": "What Types Should You Have on Your Pokémon Team? Efficient Simulation with Matrices in R",
    "section": "Matrix Magic",
    "text": "Matrix Magic\nI originally was thinking of calling this post “Going back to the Base[ics],” since I’m moving out of the tidyverse and into the world of matrices, but there’s really nothing basic about this. Let’s walk through it step by step.\nFirst, we’re going to take our table and make it a matrix. We can’t just do as.matrix() directly, as it will make the Attacking column the first column, while we want that to be the rownames, so we’ll do it in two steps.\n\nm <- as.matrix(type_comparisons[, -1])\nrownames(m) <- type_comparisons$Attacking\n\nNext, because we only care about whether the entry is 2 or not, we’ll change every entry that’s a 2 to be 1 and every entry that’s not to be 0 (the 1L * makes it 1 or 0 instead of TRUE or FALSE).\n\nsuper_effective_m <- (m == 2) * 1L\n\n\nsuper_effective_m\n\n         Normal Fire Water Electric Grass Ice Fighting Poison Ground Flying\nNormal        0    0     0        0     0   0        0      0      0      0\nFire          0    0     0        0     1   1        0      0      0      0\nWater         0    1     0        0     0   0        0      0      1      0\nElectric      0    0     1        0     0   0        0      0      0      1\nGrass         0    0     1        0     0   0        0      0      1      0\nIce           0    0     0        0     1   0        0      0      1      1\nFighting      1    0     0        0     0   1        0      0      0      0\nPoison        0    0     0        0     1   0        0      0      0      0\nGround        0    1     0        1     0   0        0      1      0      0\nFlying        0    0     0        0     1   0        1      0      0      0\nPsychic       0    0     0        0     0   0        1      1      0      0\nBug           0    0     0        0     1   0        0      0      0      0\nRock          0    1     0        0     0   1        0      0      0      1\nGhost         0    0     0        0     0   0        0      0      0      0\nDragon        0    0     0        0     0   0        0      0      0      0\nDark          0    0     0        0     0   0        0      0      0      0\nSteel         0    0     0        0     0   1        0      0      0      0\nFairy         0    0     0        0     0   0        1      0      0      0\n         Psychic Bug Rock Ghost Dragon Dark Steel Fairy\nNormal         0   0    0     0      0    0     0     0\nFire           0   1    0     0      0    0     1     0\nWater          0   0    1     0      0    0     0     0\nElectric       0   0    0     0      0    0     0     0\nGrass          0   0    1     0      0    0     0     0\nIce            0   0    0     0      1    0     0     0\nFighting       0   0    1     0      0    1     1     0\nPoison         0   0    0     0      0    0     0     1\nGround         0   0    1     0      0    0     1     0\nFlying         0   1    0     0      0    0     0     0\nPsychic        0   0    0     0      0    0     0     0\nBug            1   0    0     0      0    1     0     0\nRock           0   1    0     0      0    0     0     0\nGhost          1   0    0     1      0    0     0     0\nDragon         0   0    0     0      1    0     0     0\nDark           1   0    0     1      0    0     0     0\nSteel          0   0    1     0      0    0     0     1\nFairy          0   0    0     0      1    1     0     0\n\n\nThe all_combinations matrix we created before is essentially a set of indices for the super_effective_m matrix. For example, column 1 of all_combinations are the numbers 1 through 6, which means we want to get rows 1 through 6 of super_effective_m. Remember, each row of super_effective_m is an attacking type on our team, and each column is a defending type. We then want to get the sum of each column and know how many columns have a sum of more than 0, meaning at least one of our attacking types was super effective against it. We’ll make a function, super_effective_nb:\n\nsuper_effective_nb <- function(indices) {\n  sum(colSums(super_effective_m[indices, ]) > 0)\n}\n\nNow we can use apply() to get a vector, for all 18k+ teams, of how many types they’re super effective against. If you’re not familiar with apply(), the first argument is what we’re applying our function to, the second is whether it should apply to the rows or columns (we choose 2 for column, since each column is the team), and the third is the function.\n\nsuper_effective_results <- apply(all_combinations, 2, super_effective_nb)\n\nWhat are the combinations that are super effective against the maximum number of types possible?\n\nwhich(super_effective_results == max(super_effective_results))\n\n [1] 14323 14325 15610 15612 16454 16459 16852 16854 16989 16994\n\n\nWe see there are 10 possible combinations of six types. Let’s take a look at them by getting those columns from all_combinations.\n\nbest_combos <- all_combinations[, super_effective_results == max(super_effective_results)]\nbest_combos\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n[1,]    4    4    5    5    5    5    6    6    6     6\n[2,]    6    6    6    6    8    8    7    7    7     7\n[3,]    7    7    7    7    9    9    8    8    9     9\n[4,]    9    9    9    9   13   13    9    9   10    10\n[5,]   10   10   10   10   14   16   10   10   14    16\n[6,]   14   16   14   16   18   18   14   16   17    17\n\n\nWe now have a matrix, best_combo, where each column is a team. For example, we see a team of types 4, 6, 7, 9, 10, and 14 cover the maximum number of defending types. But what is type 4? To answer that, we take the row names from super_effective_m and index it by best_combos.\n\nrownames(super_effective_m)[best_combos]\n\n [1] \"Electric\" \"Ice\"      \"Fighting\" \"Ground\"   \"Flying\"   \"Ghost\"   \n [7] \"Electric\" \"Ice\"      \"Fighting\" \"Ground\"   \"Flying\"   \"Dark\"    \n[13] \"Grass\"    \"Ice\"      \"Fighting\" \"Ground\"   \"Flying\"   \"Ghost\"   \n[19] \"Grass\"    \"Ice\"      \"Fighting\" \"Ground\"   \"Flying\"   \"Dark\"    \n[25] \"Grass\"    \"Poison\"   \"Ground\"   \"Rock\"     \"Ghost\"    \"Fairy\"   \n[31] \"Grass\"    \"Poison\"   \"Ground\"   \"Rock\"     \"Dark\"     \"Fairy\"   \n[37] \"Ice\"      \"Fighting\" \"Poison\"   \"Ground\"   \"Flying\"   \"Ghost\"   \n[43] \"Ice\"      \"Fighting\" \"Poison\"   \"Ground\"   \"Flying\"   \"Dark\"    \n[49] \"Ice\"      \"Fighting\" \"Ground\"   \"Flying\"   \"Ghost\"    \"Steel\"   \n[55] \"Ice\"      \"Fighting\" \"Ground\"   \"Flying\"   \"Dark\"     \"Steel\"   \n\n\nThis gets us a character vector though. It’s in order, so we know that the first six is team 1, the second six team 2, etc., but it’s not displayed very well. We can use matrix to turn this into a matrix instead, specifying that we want 6 rows.\n\nstrongest_teams <- matrix(rownames(super_effective_m)[best_combos], nrow = 6)\n\n\nstrongest_teams\n\n     [,1]       [,2]       [,3]       [,4]       [,5]     [,6]     [,7]      \n[1,] \"Electric\" \"Electric\" \"Grass\"    \"Grass\"    \"Grass\"  \"Grass\"  \"Ice\"     \n[2,] \"Ice\"      \"Ice\"      \"Ice\"      \"Ice\"      \"Poison\" \"Poison\" \"Fighting\"\n[3,] \"Fighting\" \"Fighting\" \"Fighting\" \"Fighting\" \"Ground\" \"Ground\" \"Poison\"  \n[4,] \"Ground\"   \"Ground\"   \"Ground\"   \"Ground\"   \"Rock\"   \"Rock\"   \"Ground\"  \n[5,] \"Flying\"   \"Flying\"   \"Flying\"   \"Flying\"   \"Ghost\"  \"Dark\"   \"Flying\"  \n[6,] \"Ghost\"    \"Dark\"     \"Ghost\"    \"Dark\"     \"Fairy\"  \"Fairy\"  \"Ghost\"   \n     [,8]       [,9]       [,10]     \n[1,] \"Ice\"      \"Ice\"      \"Ice\"     \n[2,] \"Fighting\" \"Fighting\" \"Fighting\"\n[3,] \"Poison\"   \"Ground\"   \"Ground\"  \n[4,] \"Ground\"   \"Flying\"   \"Flying\"  \n[5,] \"Flying\"   \"Ghost\"    \"Dark\"    \n[6,] \"Dark\"     \"Steel\"    \"Steel\"   \n\n\nFor our final step, we’re actually going to make this a tibble, so I can look at which types appear the most often across the different team possibilities.\n\nstrongest_teams %>%\n  as_tibble() %>%\n  gather(team, type) %>%\n  count(type, sort = TRUE) %>%\n  knitr::kable()\n\n\n\n\ntype\nn\n\n\n\n\nGround\n10\n\n\nFighting\n8\n\n\nFlying\n8\n\n\nIce\n8\n\n\nDark\n5\n\n\nGhost\n5\n\n\nGrass\n4\n\n\nPoison\n4\n\n\nElectric\n2\n\n\nFairy\n2\n\n\nRock\n2\n\n\nSteel\n2\n\n\n\n\n\nWe see all 10 of the teams need a ground type, where 8 have a Fighting, Flying, or Ice type. On the other hand, Electric, Fairy, Rock, and Steel are only each used by two teams."
  },
  {
    "objectID": "posts/2019-08-08_pokemon-matrices-and-combinations-oh-my/index.html#conclusion",
    "href": "posts/2019-08-08_pokemon-matrices-and-combinations-oh-my/index.html#conclusion",
    "title": "What Types Should You Have on Your Pokémon Team? Efficient Simulation with Matrices in R",
    "section": "Conclusion",
    "text": "Conclusion\nWhile this is a bit of a silly use case, the code we walked through and lessons learned could be applied to a lot of different projects. When I advise people to make a portfolio of data science projects if they’re looking for a job, I sometimes get asked, “But how do I find something to work on?” I recommend looking in your own life and interests where you could use data science. If you’re a runner and use an activity tracker, graph how your run distances and times are related to the weather. If you’re active on a subreddit, you could use the reddit API to get the last 500 posts and do a text analysis. The possibilities are limitless!\nI also played around with getting this code even faster and trying to do everything in a tidy way instead. Including those methods made the post run a little too long, so I may follow up with a part 2 of this post. To make this analysis more useful, I could also take into account how common types are - for example, only a few Pokémon have a Dragon type, so it’s less important to have types that are super effective against Dragon.\n[1] Pokémon players will know that you can have more than 6 types on your team, both because some Pokémon have two types and because Pokémon can learn moves of other types (e.g. a Normal type Pokémon may be able to learn a Dark move). But for the purposes of this analysis I simplified it."
  },
  {
    "objectID": "posts/2017-08-14_giving-your-first-data-science-talk/index.html",
    "href": "posts/2017-08-14_giving-your-first-data-science-talk/index.html",
    "title": "Giving Your First Data Science Talk",
    "section": "",
    "text": "A few weeks ago, I gave my first ever data science talk. Jared Lander, organizer of the New York Statistical Open Programming meetup, had been asking me to speak, and after about six months at Etsy I thought I could share what I’d learned about A/B Testing and its challenges. If you’d like to watch it, you can view the recording here. The talk starts around 9:00 and ends at 41:30, with the rest being Q&A (I tried to repeat the questions, so you should be able to follow that as well).\nThis is part one of a blog post series about the talk. This first post is targeted to those who are considering giving a meetup or conference talk. The next one will be covering what I went over in my talk, and the final one will be distilling more of what I’ve learned about A/B Testing from my time at Etsy and by reading papers."
  },
  {
    "objectID": "posts/2017-08-14_giving-your-first-data-science-talk/index.html#why-do-a-talk",
    "href": "posts/2017-08-14_giving-your-first-data-science-talk/index.html#why-do-a-talk",
    "title": "Giving Your First Data Science Talk",
    "section": "Why do a Talk?",
    "text": "Why do a Talk?\nDisclaimer\nI want to preface this section by acknowledging that it is more difficult for some people to give talks than others. I live in New York, a major tech city, but for those who live in places where the tech community is smaller, there may not be a meetup for you to speak at or a large audience that you can draw. That being said, there are more of us than you might think; Rladies has chapters in over 40 cities and you may be surprised when you look to see what your community can offer.\nAnother major constraint is time. For those who have caregiving responsibilities or simply want to keep their work mainly limited to the office, doing a talk may not be feasible or appealing. While I think there are many benefits for yourself and others in doing public work, it should not become a requirement for a fulfilling data science career.\n\n\nBecause relying on public code massively discriminates in favour of those who have time to code outside of work?\n\n— Hadley Wickham (@hadleywickham) July 7, 2017\n\n\nGiving Back\nOn the other hand, if you’re considering giving a talk but think you don’t have anything to contribute, don’t underestimate yourself! You probably have more to offer than you think. The main reason an open source language can thrive is because of the community that contributes to it, usually without being paid. Certainly one way to give back is by contributing code, but you can also give talks or write blog posts. While I haven’t made or contributed code to a public package (yet!), I believe that this blog and recorded, free talks can help.\nPersonal Benefits\n\n\nCreate things useful to other people. https://t.co/MgptvBTeOz\n\n— Chris Albon (@chrisalbon) July 26, 2017\n\n\nGiving a talk is a great way to build your network and raise your data science profile. While you can (and certainly should!) meet new people as an attendee, when you’re the speaker, people will probably come up to you after the talk to continue the discussion. And if your talk is recorded and publicly shared, you’ll have a public piece of work available."
  },
  {
    "objectID": "posts/2017-08-14_giving-your-first-data-science-talk/index.html#giving-a-talk",
    "href": "posts/2017-08-14_giving-your-first-data-science-talk/index.html#giving-a-talk",
    "title": "Giving Your First Data Science Talk",
    "section": "Giving a Talk",
    "text": "Giving a Talk\nTarget the you of six months (one year, five years) ago\nOne of my favorite parts about working at Etsy is the womenby (women and non-binary) in tech group. We have a private slack channel, and the day after my talk, a senior engineer was asking for opinions on an upcoming talk. She wanted to frame the talk as a story about how they built a distributed streaming platform on docker and the mistakes they made, but she worried it would make her and her team look inexperienced and possibly stupid for not anticipating the difficulties.\nMy advice was to think of yourself before taking on this project and ask if you would have found it useful then. I would bet most of the time the answer is yes, and that the audience is more similar to you six months ago than you after this experience.\nDon’t have text or idea heavy slides\nHaving seen multiple paragraphs in size 10 font literally read off of a slide, I thought I was doing pretty well in the initial version of my talk. I had no slides with more than a few bullet points and in a few of them I had varied up the design. But on one these slides, even though there were only about 20 words in six nice little boxes, I was trying to cover six points in one go!\nWhile I had avoided visual overload, I hadn’t thought about the cognitive overload. To solve the issue with my slide, I switched to only cover three points and spread them out over about twelve slides. Although I ended up covering fewer points, no one’s going to remember everything about your talk. Focus on keeping your audience engaged so that they can absorb the points most relevant to them.\nI highly advise you to check out this deck on slide design for more tips and examples.\nPractice, practice, practice (and ask for feedback!)\nThis sounds pretty basic, but it can be easy to procrastinate on preparing the talk and feel just reading over the slides is fine. Fortunately as an Etsy employee I had to submit my slides to our communication and legal teams ahead of time, so I was forced to make them early.\nStarting two weeks before my presentation, I practiced with five different people (a few of them multiple times). Try to get people with different backgrounds: I practiced with fellow and former Etsy analysts, but also with my non-technical (but presentation expert) sister-in-law. Their advice was very helpful in improving the slides and the talk, but I also found that simply giving the talk out loud helped me see where transitions or slides didn’t work.\nRally your support team\nWhile I feel fairly comfortable with public speaking, I’d never done it to an audience this size or on data science. I was also shy about promoting my talk.\nThis is where your supporters can come in to help. My manager advertised my talk at work, my brother tweeted about it, and the other NYC rladies organizers promoted it in our slack channel. Many of them also came to the talk (my brother almost straight off a flight from London!), so I had some friendly faces in the audience.\nAnd finally, if your nerves start overcoming you before your talk, just remember this classic and sage advice from one of my search engineering partners:\n\n\nBest advice for reducing nerves before giving a data science talk: \"just imagine that your audience uses p-values of .25 for significance\"\n\n— Emily Robinson (@robinson_es) July 27, 2017\n\n\nIf you found this post useful, you might be interested in the book on data science careers I wrote with Jacqueline Nolis, “Build Your Career in Data Science,”, available for 40% off with the code buildbook40%."
  },
  {
    "objectID": "posts/2016-07-26_creating-my-first-r-package-part-1/index.html",
    "href": "posts/2016-07-26_creating-my-first-r-package-part-1/index.html",
    "title": "Creating My First R Package Part 1",
    "section": "",
    "text": "As I mentioned in my previous post, I was fortunate to enter graduate school with a few years of programming experience in R. I learned R exclusively through my Statistics classes; while I took the graduate-level psychology statistics course at Rice and was a research assistant in multiple departments, all used SPSS.\nAs this discrepancy suggests, the social sciences are often lagging behind in teaching and using open-source software. Fortunately, there is some effort to change this. The PhD director at INSEAD recently sent out an email expressing his delight that students were becoming increasingly interested in learning how to do computer programming. He announced that new students in future years would be asked to spend their summer before joining INSEAD learning basic programming, although it would not become an admission requirement.\nWhile a few INSEAD students and research assistants use Python, R, MATLAB, or other languages, all the courses I took used STATA. Here’s a brief summary of my experience with STATA:\nBut STATA was the language of INSEAD’s analytical classes, and so STATA was the language I would use. Until I finally reached a breaking point while trying to replicate the MEMORE macro in STATA for a spontaneous class assignment by a teacher who had not yet tried to do so himself."
  },
  {
    "objectID": "posts/2016-07-26_creating-my-first-r-package-part-1/index.html#the-memore-function",
    "href": "posts/2016-07-26_creating-my-first-r-package-part-1/index.html#the-memore-function",
    "title": "Creating My First R Package Part 1",
    "section": "The MEMORE function",
    "text": "The MEMORE function\nMEMORE stands for “MEdiation and MOderation in REpeated-measures designs” and is a macro for SPSS and SAS created by Montoya and Hayes. As they describe:\n\nIt estimates the total, direct, and indirect effects of X on Y through one or more mediators M in the two-condition or two-occasion within-subjects/repeated measures design. In a path analytic form using OLS regression as illustrated in Montoya and Hayes (2015), it implements the method described by Judd, Kenny, and McClelland (2001, Psychological Methods) and extended by Montoya and Hayes (2015) to multiple mediators. Along with an estimate of the indirect effect(s), MEMORE generates confidence intervals for inference about the indirect effect(s) using bootstrapping approach.\n\nI managed to get the STATA code to work eventually, but in the meantime I decided to tackle it with R. I also wanted the chance to keep to continue practicing using R Markdown instead of R Scripts. With R Markdown, I could not only write a function equivalent to the MEMORE macro but also include an explanation of the function in the same document. Using a combination of the documentation of the function and the accompanying paper, I replicated the basic function and included the options for setting number of bootstrap repetitions, confidence interval (CI) size, and which bootstrap CI would be calculated.\nThis did earn me some praise from my difficult-to-please German professor:\n\nI also created a GitHub repository to store my R Markdown document. But since March I had pretty much left it there."
  },
  {
    "objectID": "posts/2016-07-26_creating-my-first-r-package-part-1/index.html#motivation",
    "href": "posts/2016-07-26_creating-my-first-r-package-part-1/index.html#motivation",
    "title": "Creating My First R Package Part 1",
    "section": "Motivation",
    "text": "Motivation\nI had bookmarked Hilary Parker’s blog post on writing R packages months ago, but never got around to doing anything with it. I appreciated how straightforward and simple she made the process seem (and of course that she used an example with cats).\nSince her post, Hadley had released a book on writing packages in R. I started reading the first chapter, but I quickly realized I would only really retain and understand writing packages if I did it myself. So I decided to finally turn the MEMORE R Markdown document into a real, easily useable package.\nTurning this function into a package offers a few benefits:\n\nBetter Understanding of Packages: When I want to understand what a function does or what parameters it accepts, I usually look at the documentation. But sometimes I want look at the implementation of the function. For a package with many files and folders (e.g. ggplot2), this can quickly become overwhelming. Going through the steps of writing my own package should help me understand the importance of each component and the overall organization.\nContributing to Open-Source: Unlike STATA, SPSS, or SAS, R is a free, open-source software. Without the many people who volunteer their time to create new packages, fix bugs, and answer questions for no financial compensation, R could not hope to compete with those softwares. The people I know in the R community have been very welcoming, and I hope to meet more by attending conferences such as rstudio::conf and useR!. Eventually I want to write packages that go beyond just adding one statistical tool to contributing the data analysis process itself. But first I need to learn the basic structure of creating a package. I’m not sure if anyone will ever use my package, but I’m sure the exercise will pay off in the long run."
  },
  {
    "objectID": "posts/2016-07-26_creating-my-first-r-package-part-1/index.html#to-be-continued",
    "href": "posts/2016-07-26_creating-my-first-r-package-part-1/index.html#to-be-continued",
    "title": "Creating My First R Package Part 1",
    "section": "To Be Continued",
    "text": "To Be Continued\nTrying to write my first R program while in the middle of a data science boot camp based in Python is not easy. Any time I could set aside for it could also be spent working on my boot camp projects or trying to consolidate what we learned that day or week. I’m still optimistic that I’ll get it done before the boot camp ends in mid-September, but even if not, I’ll write follow-up blog post where I document the process of creating an R package."
  },
  {
    "objectID": "posts/2018-07-03_red-flags-in-data-science-interviews/index.html",
    "href": "posts/2018-07-03_red-flags-in-data-science-interviews/index.html",
    "title": "Red Flags in Data Science Interviews",
    "section": "",
    "text": "This post was co-written with Jacqueline Nolis, Principal at Nolis, LLC. Check out the rest of her blog posts, including ones on prioritizing data science work, hiring data scientists, and what to do when your data science project isn’t working.\nWhen interviewing for any position, you should be evaluating the company just as much as they are evaluating you. While you can research the company beforehand on glassdoor and similar sites, interviews are the best place to get a deeper understanding of the company and ask important questions. Companies will never straight up tell you they are bad to work for, so you have to look for the signs yourself.\nHere is our list of 12 signs the company you are interviewing with for a data scientist job should be avoided (and the questions to ask during the interview). The first six mainly apply to companies that already have multiple data scientists or analysts. If you’re thinking of joining a company as their first data scientist, you’ll face a whole different set of challenges, including most likely doing a lot of data engineering work (see flag 1) and spreading a data science mindset. Someone has to do it, but we generally advise against it for your first data science role unless you come from an engineering background and want to do that work. If there’s just a data science leader and they’re building out a team, ask how they plan to handle the issues raised below, but keep in mind it’s always easier to promise an ideal system than implement one."
  },
  {
    "objectID": "posts/2018-07-03_red-flags-in-data-science-interviews/index.html#red-flags-on-how-the-data-science-team-runs",
    "href": "posts/2018-07-03_red-flags-in-data-science-interviews/index.html#red-flags-on-how-the-data-science-team-runs",
    "title": "Red Flags in Data Science Interviews",
    "section": "Red flags on how the data science team runs",
    "text": "Red flags on how the data science team runs\n\nNo data engineering or infrastructure. Data science requires data to be easily available for analysis. If the company doesn’t have a well-maintained data infrastructure, you won’t have what you need to do your job. A data engineer is a person who prepares data for analysis, and if your company doesn’t have them you’ll have to do the work yourself. If you feel qualified to take on the role of a data engineer that may be okay, but otherwise you’ll be struggling to deliver anything of value. Question to ask during the interview: what is your data infrastructure like and who maintains it? What format is the data typically in (Excel, a SQL database, csv)?\nNo peer review between data scientists. A strong data science team will have ways to ensure mistakes don’t slip through the cracks. These can include code reviews, practice presentations, and consistent check-ins with the team. If the team doesn’t consistently do these, mistakes won’t be found until the work is already delivered, which usually ends with someone getting reprimanded. Question to ask: what steps does the team take for QA and peer review?\nNo standard set of languages on the team: Many data science teams take the approach of letting anyone on the team use any language they want. The idea is that if everyone uses their favorite languages work will be completed faster. There is a huge problem with this: when everyone uses separate languages, no one will be able to pass off work to anyone else. Every data science task will have a single person responsible for it, and if they quit, get sick, or just need help no one will be able to do so, creating a very stressful environment. It’s fine to use R, Python, or even dare we say SAS, but just have a consistent set of languages amongst the team. Question to ask: what languages does your team use, and how do you decide whether to adopt a new one?\nNo understanding of the data hierarchy of needs: Similar to not having a data infrastructure, some companies get really excited about concepts like AI without having the foundation in place. Machine learning and AI require a company to have a high level of data science maturity, including understanding how to build models, their limitations, and how to deploy them. You might get blamed when their unrealistic expectations meet reality. Question to ask: how does the company balance spending time on complex approaches like AI with foundational work like cleaning data, checking data quality, and adding logging?\nNo version control: Mature data science teams use git to keep track of changes to analyses and code. Other teams instead use methods like shared network folders, which don’t let you see when things changed, why they are changed, or previous versions. Occasionally teams don’t share code at all and work just lives on the data scientists individual laptops. Avoid these last groups like the plague. Not having methods of sharing code means the team can’t work together. Question to ask: how to you share code amongst the team? Is all code shared or just some of it?\nNo clear delineation between people who run reports vs do analyses: The skillsets required to create and maintain reports, to build data science models, and to put machine learning models into production are all different. If the company doesn’t have a clear way of determining who does what work, you could start your job and end up doing work totally different than what you expected. You don’t want to walk in on your first day expecting to build a time-series forecast and find out your job is to refresh the monthly sales Excel spreadsheet. Question to ask: how are reporting, analysis, and production-model building tasks split?"
  },
  {
    "objectID": "posts/2018-07-03_red-flags-in-data-science-interviews/index.html#red-flags-on-how-they-value-people",
    "href": "posts/2018-07-03_red-flags-in-data-science-interviews/index.html#red-flags-on-how-they-value-people",
    "title": "Red Flags in Data Science Interviews",
    "section": "Red flags on how they value people",
    "text": "Red flags on how they value people\n\nA totally non-structured interview process: A structured interview process means that each candidate gets the same set of questions and can be more equally compared. Not only does it decrease bias, it also requires the team think through what’s important in the people they bring on. If the interview process is unstructured, with the interviewer seemingly asking questions off the top of their head, then it’s a strong sign they haven’t figured out what they want in a candidate and how to get it. If they don’t know what they want you are going to have trouble giving them what they want on the job. Suggestion: see if they bring a set of questions to the interview, or ask the meta-question of how they chose what to ask you.\nNo time for your questions: Since interviews are also for you to find out about the company, you need to have time to do so. If there isn’t time for you to ask questions, the interviewer isn’t interested in making you feel comfortable and allowing you to assess your fit. Suggestion: if you get to the interview and you didn’t have time to ask questions, make a note of it and ask the interviewer when would be a good time to ask them instead.\nNo coding required in the interview: While programming isn’t the most important skill for a data scientist, it is something you will have to do on the job. The coding part of the interview could be on-site or a take-home test, but it should definitely exist. If the interview process doesn’t include programming it could be for a few reasons: (1) The data science team is new so no one can run the interview. In this case, be aware you won’t have support on the job. (2) The team doesn’t have the time to create a programming interview. This is a sign they don’t value hiring. (3) They don’t program and use BI tools like Tableau and Excel for their work. (4) They trust your resume so much that they don’t need to test you. While flattering, this a sign that they are desperate to hire. Suggestion: if the interview doesn’t include a programming component, ask them how do they tell which candidates have the technical skills for the job.\nNo plan for your first few months: Companies put job postings out for good reasons. If they aren’t able to clearly articulate exactly what you’ll be doing in the first few months, that reason is probably “we are totally overwhelmed with work and we are going to throw people at the problem until we can handle it.” That’s an extremely dangerous way to grow a team. What’s worse, this usually happens at the companies that don’t have onboarding processes for new hires. So these situations are extremely stressful for the whole team and that usually falls on you too. Suggestion: ask if they have a clear project and on-boarding process for your start. If they don’t have an extremely precise answer for this, run.\nNo support for continuing education: Data science is a large and rapidly advancing field and if you don’t keep learning you’ll fall behind. Teams should have some way of helping people keep up, whether it’s by providing funding for online education or attending conferences, monthly meetings where you discuss industry blog posts, encouragement to attend meetups or get involved in open source, or a speaker series. This also shows they’re willing to invest in their people generally. Suggestion: ask how they support the continued education of the team. Is there funding for conferences or workshops?\nInconsistent answers between interviewers about the role: Usually interviews have you talking to many people within the company, including your future manager, teammates, and business stakeholders. If they each tell you different things around the level of responsibility, type of work, what the role delivers, and hours you’ll have to work, they themselves probably don’t agree. If they can’t agree, especially on things related to what work you’ll end up doing, your job will end up being full of conflict. Suggestion: keep track of what people say in different interviews. If you find an inconsistency, ask why.\n\nWhile these twelve flags may feel like a lot, companies tend to show none of them or most of them, with only few in the middle. By keeping an eye out of them you can avoid the problem of getting to a job you dislike. If you want more data science thoughts and tips, we wrote a book on data science careers, “Build a Career in Data Science,” available for 40% off with the code buildbook40%."
  },
  {
    "objectID": "posts/2016-08-08_better-plotting-in-python-with-seaborn/index.html",
    "href": "posts/2016-08-08_better-plotting-in-python-with-seaborn/index.html",
    "title": "Better Plotting in Python with Seaborn",
    "section": "",
    "text": "Coming into Metis, I knew one of the hardest parts would be switching from R to Python. Beyond simply having much more experience in R, I had come to rely on Hadley Wickham’s fantastic set of R packages for data science. One of these is ggplot2, a data visualization package. While there is a version of ggplot2 for python, I decided to learn the main plotting system in Python, matplotlib. Then I actually created and saw my first matplotlib graph:\n\n\n\ncenter\n\n\nI was horrified. I hated the color, the tick marks on all four sides of the plot, the white background. I promptly sent my brother the following texts:\n\nFortunately, he wrote back quickly suggesting I try seaborn, and my boot camp experience was saved. Six weeks later, I’ve become known in my Metis cohort as a seaborn evangelist. On presentation days at Metis, not a plot goes by without me marking down if it has base matplotlib aesthetics. I then follow up with the presenters afterwards, asking them why they don’t use seaborn. Usually this is followed by, “Is it to make me sad?”"
  },
  {
    "objectID": "posts/2016-08-08_better-plotting-in-python-with-seaborn/index.html#advantages-of-seaborn-better-aesthetics-and-built-in-plots",
    "href": "posts/2016-08-08_better-plotting-in-python-with-seaborn/index.html#advantages-of-seaborn-better-aesthetics-and-built-in-plots",
    "title": "Better Plotting in Python with Seaborn",
    "section": "Advantages of Seaborn: Better Aesthetics and Built-In Plots",
    "text": "Advantages of Seaborn: Better Aesthetics and Built-In Plots\nSeaborn is a data visualization library in Python based on matplotlib. The seaborn website has some very helpful documentation, including a tutorial. And like the rest of your programming questions, anything you can’t find on that website can generally be found on the Stack Overflow page that is your first google result.\nTo get started with seaborn, you’re going to need to install it in the terminal with either pip install seaborn or conda install seaborn. Then simply include import seaborn as sns at the top of your python file.\n\nNicer Default Aesthetics\nOne of the biggest advantages of seaborn is that its default aesthetics are much more visually appealing than matplotlib. If I import seaborn at the top of my python file and re-run the same exact commands that generated this post’s earlier plot, I now get this:\n\nThat’s right: you can run the exact same code you’ve already written and get prettier plots, no extra code or new syntax required. Recently I was horrified when a more senior data scientist, and much better Python programmer, presented with default matplotlib aesthetics. When I asked him why he didn’t use seaborn, he said “It’s on my list of things to learn, I just haven’t gotten around to it.” But this isn’t a valid excuse! All you need to do to start benefitting from seaborn is import it. Seaborn has much more to it besides these default aesthetics, but this feature already offers an exponential improvement.\n\n\nEasily Customizable Aesthetics\nIf you want to change either the background or the colors of all your graphs, you can do so easily with two commands: sns.set_style and sns.set_palette.\n\nsns.set_style takes one of five arguments: white, dark, whitegrid, darkgrid, and ticks. These are the five options for the background of your plot; the default one is darkgrid. Play around and see what you like best!\nsns.set_palette will change the color palette. Use sns.palplot to print out a set of colors before you change your default colors to them. For example, try sns.palplot(sns.light_palette(\"green\")). If you decide you like those colors, run sns.set_palette(sns.light_palette(\"green\")) to change your graphs. Check out an extensive set of possible color palettes here. This page also gives a great tip on how you can divide color palettes into three different categories, and which one is appropriate for which type of data:\nQualitative color palettes, where you want to distinguish between distinct data that doesn’t have an ordering. These color palettes are just a variety of different colors.\n\n\n\nSequential color palettes, where your data range goes from relatively uninteresting or low values to relatively interesting or high values. These color palettes go from light to dark or dark to light in one color or similar colors.\n\n\n\nDiverging color palettes, where the interesting points are on either end and you want to under-emphasize the middle points. These color palettes are dark at the end and light in the middle, with a different color for each side.\n\n\n\n\nStatistically-Minded Plots\nThe other big advantage of seaborn is that seaborn has some built-in plots that matplotlib does not. Most of these can eventually be replicated by hacking away at matplotlib, but they’re not built in and require much more code. Facet plots and regression plots are just two examples of those that take much longer to create with matplotlib; the regression plot does a regression line, confidence interval, and a scatter plot, all with one short function: sns.regplot(x=\"total_bill\", y=\"tip\", data=tips)!\nMaking plots in seaborn also generally matches your intuition for what the syntax would be. For example, to make a barchart with confidence intervals, you can run the following code (having loaded the tips dataset with tips = sns.load_dataset(\"tips\")):\nbarplot = sns.barplot(x = \"day\", y = \"total_bill\", data = tips, order = [\"Thur\", \"Fri\", \"Sat\", \"Sun\"])\nbarplot.set(xlabel = \"Day\", ylabel = \"Average Total Bill\", title = \"Total Bill by Day\")\n\nMeanwhile, in matplotlib you actually have to create a new dataset with your means (and standard deviations if you want confidence intervals). Matplotlib also won’t accept categorical variables as the variable for the x-axis, so you have to first make the bar chart with numbers as the x-axis, then change the tick-marks on the x-axis back to your original categories. You also have to write three lines instead of one for changing the x-label, y-label, and title. Here’s the code for making the graph in matplotlib, which doesn’t even include re-ordering the x-axis or the confidence intervals:\ntotal_bill_by_day = tips.groupby(\"day\").mean()\nax = plt.bar([1, 2, 3, 4], total_bill_by_day[\"total_bill\"], align = \"center\")\nplt.xticks([1, 2, 3, 4], total_bill_by_day.index)\nplt.xlabel(\"Day\")\nplt.ylabel(\"Average Total Bill\")\nplt.title(\"Total Bill by Day\")\n\nThis is far from an unusual case. While seaborn certainly does not have its own plots for everything, it has a lot of the ones you’d typically use for exploratory purposes."
  },
  {
    "objectID": "posts/2016-08-08_better-plotting-in-python-with-seaborn/index.html#final-verdict",
    "href": "posts/2016-08-08_better-plotting-in-python-with-seaborn/index.html#final-verdict",
    "title": "Better Plotting in Python with Seaborn",
    "section": "Final Verdict",
    "text": "Final Verdict\nI come from a family tradition of caring too much about plotting frameworks. Earlier this year, my brother David Robinson, a data scientist at Stack Overflow, became part of a flare-up in a long-running debate on ggplot2 versus base R graphics.\nIn February, JHU professor Jeff Leek wrote a blog post entitled “Why I don’t use ggplot2”. The post wasn’t even out two days before David followed up with a post of his own on why he uses ggplot2. Soon others joined the fray. While it settled down after a month, the battle still simmers under the surface:\n\nI bring this up not only to illustrate some family resemblance, but also to set up a contrast to the seaborn/matplotlib decision. While Base R graphics and ggplot2 require completely different syntax, seaborn is based on matplotlib, and so starting to use seaborn is as easy as importing it.\nI think every python programmer can benefit from using seaborn for visualizations. The advantage of matplotlib is that you can do essentially anything you want with it by building a plot piece-by-piece. You certainly can make beautiful, professional plots in matplotlib. Seaborn doesn’t take away any of that, but rather adds some nice default aesthetics and built-in plots that complement and sometimes replace the complicated matplotlib code you may already be writing.\nAs someone who started off using seaborn right away and has been using it for less than two months, I’m far from an expert on seaborn or matplotlib. But I hoped this post would be helpful for new Python users or reluctant seaborn adapters for the great advantages I see in Seaborn."
  },
  {
    "objectID": "posts/2021-04-06_publishing-a-technical-book-part-4/index.html",
    "href": "posts/2021-04-06_publishing-a-technical-book-part-4/index.html",
    "title": "Publishing a Technical Book (Part 4): After the Book is Published",
    "section": "",
    "text": "[This is my final post on publishing a technical book. If you’re wondering if you should write one, how to find a publisher, or what the writing process is like, start from the beginning.]\nIt’s really cool to have a physical copy of something you wrote! I still sometimes just flip through the pages, marveling at how professional it looks and how much content we created. The final book ended up being 350 pages, so although it’s no mammoth technical reference, you can certainly spot it on your bookshelf.\nJacqueline and I had planned to do book launch parties in Seattle and New York (where we lived) in parternship with local data science meetup groups, but unfortunately our book came out right when COVID hit. We pivoted instead to doing some virtual events, where we would read and discuss a few excerpts from the book (usually for about 30 minutes total) and then do a Q&A session. This format worked pretty well for us; it involves minimal preparation on our side (just picking the excerpts) and Jacqueline and I have a good and fun rapport.\nWe’ve also done a lot of podcasts and conference talks in the past year to promote our book. Some of the podcasts came through Manning, but the rest was people reaching out directly to us. Sometimes they would want us to speak specifically about the book, and other times it was a general invitation where we would offer to do a career-focused talk. While the events have been fun, they’ve all had to be virtual because of Covid and I have found that to be less energizing than getting to meet people in person. Except for a few cases where we spoke on very popular podcasts like SuperDataScience, I also don’t think it really translated into expanding our audience or selling books. I’m not sure if that would be different for physical events, where perhaps people would be encouraged to buy the books to get them signed. Book signings at RStudio::conf book signing were very popular, although I think workshop attendees got to choose a book for free:\n\n\nthis is not comic con, this is the #rstats open source book signing at #rstudioconf. So much gratitude and generosity. Democratizing data science, one bookdown repo at a time @rstudio. pic.twitter.com/BccOXZ1bHr\n\n— Max Held (@maxheld) January 28, 2020\n\n\nFor each meetup and podcast, Manning has been very generous in providing free e-book codes. When we resume in-person events again, I’m hopeful we’ll be able to get a physical copy or two for each (I know Pearson, for example, does this).\nOne of my favorite things Jacqueline and I have done is make our own podcast. We’ve done 14 episodes so far (plus a bonus episode), each covering one chapter of the book. The podcast is a nice way to make the book more accessible, since it’s free and some people prefer listening to reading (although we do have an audiobook with that rich barritone narration). We also get to share our personal stories and things we wish we had included in the book. We’ve gotten great feedback that people, and I like how we’ve balanced making it informative and useful while also keeping people entertained.\nWe’re able to track our sales on a daily level with Manning and weekly level with Amazon US. For everywhere else, we have to wait until Manning issues us our royalty statements, which usually comes out 3.5 months after the end of the quarter (so we got our Q3 sales in mid-January and expect our Q4 numbers soon). With Manning we make exactly our royalty rate * the sale price, but that’s not the case for sales through Amazon or other bookstores, as those companies buy the book at a (unknown to me) discount. A company bought the Korean rights for Build a Career in Data Science back in July and apparently they usually come out after 12 months, so I’m pretty excited for that! Another bonus is that we get 50% of the proceeds from the sale, including the advance and royalty.\nI think there’s still a lot of people out there who could really benefit from our book but don’t have it - Jacqueline and I joke with each other when we do Q&A sessions that almost every question asked is addressed in our book. I’m not sure where the disconnect is in reaching people - if it’s them knowing about the book, thinking it will have new relevant information, or believing it’s worth spending money on. We were also both were hoping to have more groups like bootcamps or university classes using our book, since they’re one of their target audiences. There was one short course (I think for a university’s January term) that used our book for the syllabus, but unfortunately I’ve lost the link.\nIn general though, I’ve been really happy with the positive reception for the book. It always makes my day when someone messages or tweets about how the book has helped them. That feeling is what makes the labor of the book worthwhile. If this sounds lame to you and what you want from your effort is an author cutout to put in the mansion that your book royalties paid for, may I suggest “Harry Potter and Object-Oriented Programming”?"
  },
  {
    "objectID": "posts/2021-04-06_publishing-a-technical-book-part-3/index.html",
    "href": "posts/2021-04-06_publishing-a-technical-book-part-3/index.html",
    "title": "Publishing a Technical Book (Part 3): The Writing Process",
    "section": "",
    "text": "[This is my third post on publishing a technical book. Check out the previous posts on why you should (and shouldn’t) write a book and how to find a publisher.]\n\nFinding the time\nOverall I’d estimate writing the book, including the proposal process, took us each around 400 hours over a year and a half. This doesn’t count the work promoting the book after it was out, which I’ll talk about in part 4. Jacqueline and I split the 16 chapters equally between us. Generally we would take 3-4 weeks to write the first draft of our chapter, 2 weeks to review the other person’s chapter, and 2 weeks to incorporate their edits and suggestions into a final draft, which meant we were finishing about a chapter a month. We’d send our chapters to Manning as they were finished to be reviewed by our editor, and then Manning would send out 5 chapter chunks to external reviewers, which usually took about a month. In a very meta blog post, Jacqueline actually analyzed our GitHub commit history, so you can see the entire timeline of our book writing there.\nSome people find it very important to write some every day, possibly at a specific time. We definitely did not do that. That might be more important if you’re trying to write in a short period of time - for example, Susan Fowler wrote her first technical book in three months by writing at least a thousand words every day.\nWriting a book is a big time commitment. We both had full-time jobs completely separate from the book and in Jacqueline’s case she was also raising a toddler. If you are in a relationship, I think it’s really important to have a supportive partner. If you have children, maybe they agree to be “solo parent” for two nights each week so you can go to a coffee shop to write. Even if you don’t have children, you’ll need to spend a lot of time on the book, which means less time for spending with them either alone or with friends.\nIf your struggle is not so much with having time available as motivating yourself to use it to write, I highly recommend adopting the “shitty first draft” method advocated by Anne Lammot in Bird by Bird (an excellent book on writing). “Shitty first drafts” means that you should just focus on getting words on the page and not worry about having it be perfect. In my case, I often started by writing my thoughts in bullet points and then converting it to paragraphs. A few times when I was really struggling I even gave those bullet points to my husband (who is not a data scientist or even in tech) and asked him to write a first draft, since I found editing easier than writing.\nHaving the outline we wrote for our book proposal also really helped, since it broke the chapters down into chunks. Instead of starting from “I need to write some career advice,” I would start from “I need to write about tailoring your resume.” One of our core needs as humans is having a sense of progress, and by thinking in subsections I was able to get that progress vs. if I was thinking about getting an entire 350-page book written.\n[Serial procrastinators, skip this next section!]\nWhile you will have a timeline established with your publisher when you begin writing your book, it’s pretty common for those to get extended, even though there might be language in the contract like “if the draft is delayed by more than 60 days we reserve the right to bring on another author.” Knowing that, I didn’t find those especially motivating. As long as you’re being responsive and getting some progress done, your publisher is unlikely to drop you. They are often particularly understanding if there’s a life event slowing you down. The exception here might be if the technology you’re writing about will soon become outdated (e.g. if you were writing a book 2 years ago about Python 2) or a competing book will be released soon.\n[Procrastinators can come back now]\n\n\nWorking with a co-author\nIn the “about the author” section, I wrote about Jacqueline:\n\n“Whenever someone asks me whether I would recommend writing a book, I always say, ‘Only if you do it with a co-author.’ But that’s not actually the full picture. It should be, ‘Only if you do it with a co-author who is as fun, warm, generous, smart, experienced, and caring as Jacqueline.’”\n\nHaving a co-author was extremely helpful for me. Jacqueline had experience that I didn’t have, including being a hiring manager, a principal data scientist, and a director, along with working at many different types of companies as a consultant. It also helped that I can be very motivated by guilt, so when I saw Jacqueline writing or sending me a chapter, it prompted me to get mine done so I wouldn’t slow us down. Finally, her feedback on my chapters was immensely valuable, especially because we have somewhat different writing styles. Mine is more to the point, trying to convey as much information as possible, while Jacqueline’s includes more stories and color. Together, I think we found a really good balance in the book.\nAll that said, I could imagine working with a bad co-author would be a nightmare. Fortunately, writing the book proposal and reading anything they’ve written before should help you learn whether you like their writing style and if they’re good to work with.\n\n\nWriting tools\nWe just used Microsoft Word with GitHub for version control and collaboration, but that’s because we had almost no code and we already owned the software. Manning also offers templates TextMaker (part of SoftMaker offer, which is less expensive than Word), Google Docs, and AsciiDoc. If you’re writing an R book you can use Bookdown, and books written in bookdown have been published by O’Reilly and CRC Press, but according to the package author it will be tough going if your publisher doesn’t support LaTeX.\n[Added 4/6] Catherine Nelson shared that for the book she wrote for O’Reilly, she used O’Reilly’s flavor of Markdown using her favorite code editor and managed it using Git.\n\n\nWorking with the publisher\nIt’s really important to remember that the publisher wants your book to succeed. You should be open to their criticism and feedback, even if it might send you into an “I’m not good enough” spiral. Remember that they already decided you were good enough when they signed you.\nOur editor didn’t usually have a lot of feedback - I’m not sure if that’s standard or we’re just amazing writers. She did help us synthesize feedback from the draft reviewers, which was good as some reviews contradicted each other and some she said Manning just straight up disagreed with.\nOne area that their experience showed was when Manning encouraged us to write an appendix with interview questions, which we hadn’t initially considered. We were very glad they pushed us to do this as it was fairly quick and I have heard a few people say they bought the book mainly for the appendix. Other than that, I don’t think the content and tone changed that much from feedback from Manning.\nCheck out part 4 if you’re interested in what it was like after the book was published!"
  },
  {
    "objectID": "posts/2021-04-06_publishing-a-technical-book-part-2/index.html",
    "href": "posts/2021-04-06_publishing-a-technical-book-part-2/index.html",
    "title": "Publishing a Technical Book (Part 2): Finding a Publisher",
    "section": "",
    "text": "[This is my second post on publishing a technical book. If you’re not sure if you should write a book, check out part 1.]\nYou’ve probably heard of some book publishers before like Simon and Schuster or Random House. Those companies don’t usually get involved in smaller technical books. Instead there are several specialized publishers for books in the tech field; the biggest are O’Reilly, Manning, Pearson, and Chapman and Hall, along with some specialized ones like A Book Apart (which publishes short books).\nPublishers have roles beyond just printing books. They will help you refine your idea, edit, and connect you with an audience. Getting a good publisher can turn your “I thought this was a cool thing to write about” into an beautifully formatted, honest-to-goodness book, available at a bookstore near you (well, maybe just the big ones).\n\nFinding a publisher\n\nSubmitting a proposal\nThe first step to getting a publisher interested in your book idea is to submit a proposal. Anyone can download a copy of O’Reilly and Manning’s proposal forms and submit it to the email provided. I couldn’t find Pearson’s US link, but here’s the UK one, which also includes some information on why they turn down proposals. I think it’s pretty rare to have an agent if you’re looking to go with a technical book publisher. But if you’re someone who did, let me know! As mentioned above, in our case Manning reached out to Jacqueline, but we still went through the regular process proposal process.\nProposals sell a publisher on why they should publish a book and why you are the right one to write it. They include things like:\n\nWho the target audience is\nAny existing books on the topic and why yours would be better or cover important things they don’t\nYour qualifications to write this book\nHow long you expect the book to be\nAn estimated timeline for completion\nAn outline with the book chapters and subsections.\n\nA good proposal shows how this book would clearly add value by covering an in-demand topic where good resources don’t exist already. So trying to publish a general book on “deep learning in Python” would be a tough sell - just look on Amazon to find dozens of books already covering the subject, some written by the authors of the main deep learning packages.You also need to convince them that you’re the best person to write about it - showing how you’ve taught a course on the subject, done it for years at your job, or even written blog posts and given talks about it all provide evidence that you would write a good book.\n\n\nSelf-publishing\nSelf-publishing has a couple of advantages. First, you keep a lot more money from each sale. For example, if you make an e-book and sell through a platform like gumroad, you keep around 90% of the proceeds. Second, there’s no gatekeeper - you don’t need a publisher to accept your proposal. Finally, you have full control over the book. While we never had an issue with Manning asking us to include or take out something we didn’t want to, I’m sure it does sometimes happen. You can also set the price of your book, perhaps even letting people decide their own price if you want to make it more accessible.\nOn the other hand, going with a publisher does provide a lot of benefits. Credibility is the biggest one - people are more likely to take your work seriously. Having a publisher shows that a company was willing to take a bet on you, as they invest lots of their own resources before your book ever hits the shelves. It also ensures some level of quality thanks to the review the book undergoes in the publishing process (see paragraph below). If you’ve published books before and/or are already a well-known expert, this might not be as important, but I would question why you’re reading this series :)\nThe second biggest benefit is all the people who will help make your book better. You’ll have an editor who works with you on each chapter offering feedback, and at the end you’ll get an extensive review from a copyeditor. Additionally, the publisher will find people to review your book as it progresses - in our case, we got a review for the first, second, and final third of the book. They’ll figure out and pay for things like cover art design and audiobook narration , which can add up to a lot. Will Larson, who self-published Staff Engineer, wrote about the self-publishing process and how the cover art and marketing cost $1,760 plus an additional $3,000 for getting an audiobook narrator. The publisher will also take care of the logistics of making print versions of the book. While you can self-publish physical books with companies like Amazon, I have heard some bad reviews about the quality. Finally, a publisher will help you reach a larger audience - this is especially helpful if you don’t have an audience for your work already (e.g. through a mailing list, Twitter, or professional reputation).\nOverall, I’m very happy we went with a publisher. The final product was incredibly professional and polished and at least I benefited from the accountability of having signed a contract to write a book (vs. my many failed “contracts” with myself … see my blog posts from 2016 still waiting on a follow-up).\nCheck out part 3 for insight into the writing process and how it feels after the book is published!"
  },
  {
    "objectID": "posts/2018-01-10_building-your-data-science-network-finding-community/index.html",
    "href": "posts/2018-01-10_building-your-data-science-network-finding-community/index.html",
    "title": "Building Your Data Science Network: Finding Community",
    "section": "",
    "text": "So you’ve heard you’re supposed to network. That’s the key in getting a job or establishing a reputation in your broader field, right? And it’s true that the importance of having a good network is supported by a lot of social sciences research. But if the thought of networking makes you cringe, you’re not alone. Many people equate networking to sending out millions of unsolicited Linkedin requests with no message, handing out 20 business cards at a meetup once a week, or sending emails to prominent data scientists with the subject line “Can I pick your brain?” That’s not very appealing to most people, and it turns out not very effective either.\nBuilding a network is not a numbers game, and it’s not about looking to get the largest personal benefit from the least time invested. Fulfilling networking is, at its heart, about building great relationships and becoming part of a community. People (even us strong introverts!) are social, and we generally have more fun and are more productive when we can lean on each other for help. As I’ve shared before, part of the reason I choose to program in R is because I’ve found an incredibly supportive community:\n\n\nThis is just one example of how supportive and helpful I've found the #rstats community. The community is one of the main reasons I choose R (vs Python) and recommend it to DS beginners. So, thank you to everyone who makes it that way. Fin.\n\n— Emily Robinson (@robinson_es) November 15, 2017\n\n\nSo how do you become part of this community? I’ve written this two part post to try to share some strategies that have worked for me. This first part is focused on growing your community generally. If you’d like to learn about reaching out to specific people, check out the second part.\n\nAttend Meetups\nThere are a lot of great data science meetups. If you’re in New York, I highly recommend the New York Open Statistical Programming meetup; if you’re a woman, check out groups like RLadies, Women who Code, and PyLadies, all of which have chapters all over the world. Stick around after the meetup; there might be an after-event (the NY Open Statistical Programming meetup invites everyone to a bar), but even if there isn’t, some people will likely stay around and you can meet them when it’s less crowded and you have a natural discussion topic (the talk you both just heard). You can also talk to the organizers; they’re almost definitely friendly folks and may be able to introduce you to others who share your interests.\nIf you’re nervous about going somewhere where you don’t know anyone, you’re not alone! I definitely have been. Steph Locke, who runs her own data science consultancy, has shared some great tips for tackling different parts of social anxiety. You can also bring a friend, but try not to just talk with them the entire time. And if you go alone, look for pairs of people or groups following the pacman rule:\n\n\nLove the Pacman Rule: When standing as a group of people, always leave room for 1 person to join your group. https://t.co/eBi0gT19yl Ht @gnat pic.twitter.com/ajvLt2H5bo\n\n— LaurenceMillar (@LaurenceMillar) November 18, 2017\n\n\nIf you’re looking for a conversation starter, remember that at a meetup or conference, you’ll generally have the first-order topic of “we are here because of R (or machine learning, python, etc).” But there are some meatier second-order questions you can dive into, like: Why this meetup? What got you interested in R? What are your favorite learning resources? (Thanks to my colleague Jake Voytko for these suggestions). People love to talk about themselves, and everyone is someone you can learn from. You don’t need to try to meet as many people as possible or make sure you talk to the most important or well-known person in the room; if you have one enjoyable conversation at the meetup with someone new, I would call that a success.\n\n\nTry out Twitter\nGetting started with Twitter can be intimidating. How do I untangle these nested threads of replies, when should I use a hashtag, and how do I even get any followers? When I first started, I just followed people and didn’t tweet anything myself. I felt a bit ashamed about that - shouldn’t I be adding value and interacting? But I now think that’s actually a great way to start out. You can just use Twitter as learning platform to get a picture of what people are talking about and find some interesting blog posts. My main advice to my past self would be to put a profile picture up - don’t be an egg!\nThere’s a great, friendly R community on there. If you’re looking for people to follow, some good people include Hadley Wickham (hadleywickham), Jesse Maegan (kierisi), David Robinson (drob), Jenny Bryan (jennybryan), Mara Averick (dataandme), Julia Silge (juliasilge) and Maëlle Salmon (ma_salmon).\nCheck out Rachel Thomas’ post for more good advice about getting started with Twitter. This can be a great way to connect with people if there aren’t many (or any!) meetups in your area. For example, Daniela Vazquez, now a data scientist at IDATHA, didn’t have a local RLadies group in her country Uruguay, but through Twitter found out about the start of a Buenos Aires chapter and became a co-organizer.\n\n\nToday is my first #RLadies anniversary 🎉 It was a life changing move for me, and I couldn’t be any happier about that 😃 Friendly community + mentoring + 💜 Are you on the fence about joining? Read about my experience!EN https://t.co/NLslMn9Zo6ES https://t.co/ZVmtRPJ2o8 pic.twitter.com/jT8vntolT9\n\n— Daniela Vázquez (@d4tagirl) January 6, 2018\n\n\nWhen you are ready to start tweeting, there’s a lot of things you can talk about. You can announce when you’re going to a conference, ask technical questions, and share your work. It can also be a way to break the ice when you finally meet someone in person. For example, when I ran into Jenny Bryan last year at RStudio::conf, I felt more comfortable introducing myself because I’d been following her on Twitter and felt I “knew” her a bit. You can even say a “virtual” hello (as written about by Rick West, along with some excellent insights on building a personal brand) to someone beforehand if they’ve said they’ll be speaking or attending a conference you’ll be at.1\nOn the technical question side, you can try out the rstats hashtag. Just the other week I tweeted out asking if anyone could advise on setting a global color palette for ggplot2:\n\n\n#rstats twitter: is there a way to globally set the color scale for ggplot2? theme_set() seems to work only for background.\n\n— Emily Robinson (@robinson_es) December 20, 2017\n\n\nI got a lot of good responses, including one introducing me to the awesome ggthemr package. I then paid it forward by tweeting about the discovery:\n\n\nTIL how to set a global color palette for ggplot2 with ggthemr! Thank you @czeildi for the great post https://t.co/svW7A9szTA pic.twitter.com/bZjfm3IZFg\n\n— Emily Robinson (@robinson_es) December 20, 2017\n\n\nYou’ll often even find Hadley Wickham answering questions on Twitter; I think it’s more polite to use the rstats hashtag than to @ him directly, but he usually answers questions specifically directed to him as well.\nFinally, if you’re posting about your first blog post, you’ve got one prominent tweeter with you (with 17,000+ followers!) - @drob. If you point him to it, he will read and retweet your first few posts:\n\n\n\nExcellent #rstats post from @raymserrato of words used to describe elections: from rvest scraping to #tidyverse cleaning to #tidytext analysis! https://t.co/jxecRhbQuw #datablog pic.twitter.com/UNW7lPW7AX\n\n— David Robinson (@drob) January 5, 2018\n\n\n\n\nCombining the two\nOne of my favorite things to do is live tweet talks. It’s gotten to the point where it’s become expected:\n\n\nYay! And let the live tweeting begin! 🙌 (no pressure 😁)\n\n— Daniela Vázquez (@d4tagirl) December 12, 2017\n\n\nI like it because it serves as notes for myself, it’s a way to share key points from the talk with people who couldn’t attend, and it gets the speaker some more visibility. If you’d like to try it, here are some tips from a live-tweeter master, @drob:\n\n\n.@drob giving tips on live-tweeting a conference #rstatsnyc ✔️✔️✔️ pic.twitter.com/VUx7aJy2D4\n\n— Emily Zabor (@zabormetrics) April 21, 2017\n\n\nI’ll end this first part with a similar promise to Dave’s: if you live tweet a talk or share a great data science resource on Twitter, let me know with a tweet and I’ll retweet. Let’s continue building a friendly, supportive community together.\nIf you found this post useful, you might be interested in the book on data science careers I wrote with Jacqueline Nolis, “Build Your Career in Data Science,”, available for 40% off with the code buildbook40%.\n\n\n\n\n\nFootnotes\n\n\nA public (i.e. not a private message), brief hello is friendly. A virtual “I see you’re going to be at the NY R conference, what hotel are you staying in, where are you eating, will you be wearing the same black dress as last year, because you looked really hot” is not. Harassment and stalking are serious and common problems. Keep messages professional; don’t be this person:\n\n\nSometimes I get these creepy messages from fellow nerds (who I don’t know) who essentially say you fill this girlfriend shaped hole I have since you know computers and are “pretty”, you all realize that’s creepy af right? So why do it.\n\n— jessie frazelle (@jessfraz) January 5, 2018\n\n\n↩︎"
  },
  {
    "objectID": "posts/2021-06-22_career-resources/index.html",
    "href": "posts/2021-06-22_career-resources/index.html",
    "title": "Answers to your Career Questions",
    "section": "",
    "text": "When my co-author Jacqueline Nolis and I wrote our book, Build a Career in Data Science, we wanted to provide a comprehensive resource of the non-technical skills and knowledge needed to get into and succeed in data science. I’ve been really pleased with the positive feedback we’ve gotten from the people it and our free companion podcast has helped. But while I think most people will find something new and helpful in it, one book can never be the only word on a topic as broad as career advice.\nSo despite a certain co-author’s bafflement, I wanted to share a list of the career resources that I’ve found most helpful. Many sparked an “aha” moment - either finally giving me the word for something I’d felt but couldn’t define or enlightening me to something I’d never thought of. I hope they will do the same for you. Some of them are specific to technical roles, but others should be helpful to anyone. I’ve structured this post by career dilemma, as I’ve often found a long list of links can overwhelm.\n\nHow do I get promoted as an Individual Contributor?\nIf you’re frustrated because you feel ready for a promotion but it hasn’t happened\n\nAre you doing too much technical leadership instead of quantifiable technical code/products: Tanya O’Reilly’s “Glue Work”\nDo you have sponsorship: Lara Hogan’s “What Sponsorship Looks Like”\nIs there opportunity at your company: Charity Majors’ “Useful Things to Know About Engineering Levels”\nAre you going for a Staff+ level (e.g. lead, principal, staff) role: Will Larson’s book “Staff Engineer: Leadership on the Technical Track” (available on Amazon and Gumroad with most of the material free on his website)\n\nIf you don’t know where to start\n\nKeep track of and share your accomplishments: Julia Evans’ “Brag Documents” and Will Larson’s “Promo Packets”\nTalk with your manager: Rebecca Knight’s “How to Ask for a Promotion”\n\n\n\nShould I become a manager?\n\nIf you’re not sure if you’d like management: Julie Zhuo’s “So You Think You Want to Manage” and Dan Na’s “Choosing the Management Track”\nIf you’re not sure what the long-term path looks like and the potential downsides: Charity Major’s “Engineering Management: The Pendulum or the Ladder”\nIf you want a comprehensive look at what it’s like to be a manager: Julie Zhuo’s book “Making of a Manager” and Camille Fournier’s book “The Manager’s Path”\n\n\n\nHow can I build my network?\n\nIf the idea of “networking” makes you feel icky: Dina Smith’s “How to Cultivate a Broad Network Without Coming off as Slimy”\nIf you want to “find a mentor”: Janet T. Phan’s “What’s the Right Way to Find a Mentor?”\nIf you deal with social anxiety: Steph Locke’s “Overcoming Social Anxiety to Attend User Groups”\n\n\n\nDo I need a “personal brand”?\n\nIf you don’t like the idea of personal branding or aren’t sure how to get started sharing your work: Rachel Thomas’ “Making Peace with Personal Branding”\nIf you don’t know what your personal brand would be: Jesse Mostipak’s “It’s Called Branding”\n\n\n\nHow can I work better with my manager?\n\nIf you want to communicate with them better: Julia Evan’s “Things Your Manager Might Not Know”\nIf you’re not sure how to “manage up” or if that’s even something you should do: Lara Hogan’s “Is Managing Up Bad?”, its sequel “How to Manage Up”, and Katie Wilde’s “The myths and traps of ‘managing up’”\n\n\n\nI’m frustrated with what my company is doing (or not doing)\n\nIf you want to influence a process/outcome where you don’t have formal authority: Lara Hogan’s “Influence Without Authority”\nIf a leader did something you hate/think is unethical: Brandon Rohrer’s “Responding to Misbehavior”\nIf you don’t know why leadership isn’t doing the obvious thing: Lara Hogan’s “Why can’t they just …?”\n\nOne last thing: try to avoid “analysis paralysis” where you try to read as much as possible rather than taking action. Two reasons for that. First, your organization may do things totally differently! If you’re looking to be promoted to staff data scientist for example, ask the current staff data scientists and even the managers who promoted them what the process is. Second, our thoughts and ways of thinking can follow our action - see Hermania Ibarra’s book Act Like a Leader, Think Like a Leader. Sometimes the only way to learn is to try and maybe fail.\nAnd for career decisions, even with lots of reflection and discussions with informed colleagues/friends, you can’t really know how you’ll like a new company or changing from an individual contributor to a manager unless you try it. Remember those decisions aren’t permanent, and you’ll learn a lot from things that aren’t a good fit too."
  },
  {
    "objectID": "posts/2019-07-25_introducing-the-funneljoin-package/index.html",
    "href": "posts/2019-07-25_introducing-the-funneljoin-package/index.html",
    "title": "Introducing the funneljoin package",
    "section": "",
    "text": "Bradyn Trollip Have you ever had a “first this then that” question? For example, maybe you’re an e-commerce business and you want all the times people clicked on an item and then added it to their cart within 2 days, or the last page they visited before registering. Or you work with pharmaceutical data and need to know what drugs people took before drug x and which drugs they took afterward and when. Or you tag fish and need to know where they went and if they eventually migrated upstream.\nEnter the funneljoin package. The goal of funneljoin is to make it easy to analyze behavior funnels with the after_join(), funnel_start(), and funnel_step() functions. If you work with data where you have events with their time and associated user, you probably have a problem funneljoin can help with. I created this package with David Robinson and Anthony Baker in July 2018 and have continued to maintain and build on it since.\nIn this post, I’ll use funneljoin::after_join() to analyze data about all Stack Overflow questions and answers with the tag R up to September 24th, 2017. The data was downloaded from Kaggle here. The next post in this series will look at the funnel_start() and funnel_step() functions, which we’ll use when all of the events or behavior are in one table."
  },
  {
    "objectID": "posts/2019-07-25_introducing-the-funneljoin-package/index.html#set-up",
    "href": "posts/2019-07-25_introducing-the-funneljoin-package/index.html#set-up",
    "title": "Introducing the funneljoin package",
    "section": "Set-up",
    "text": "Set-up\n\n\n\n\nlibrary(tidyverse)\n\nanswers <- read_csv(\"Answers.csv\")\nquestions <- read_csv(\"Questions.csv\")\n\nfunneljoin is on CRAN, so you can install it as you would any CRAN package.\n\n# install.packages(\"funneljoin\")\nlibrary(funneljoin)\n\n\n\n\nLet’s take a quick look at the questions and answers data set.\n\nquestions\n\n# A tibble: 189,930 × 6\n       Id OwnerUserId CreationDate        Score Title                      Body \n    <dbl>       <dbl> <dttm>              <dbl> <chr>                      <chr>\n 1  77434       14008 2008-09-16 21:40:29   171 How to access the last va… \"<p>…\n 2  79709          NA 2008-09-17 03:39:16     3 Worse sin: side effects o… \"<p>…\n 3  95007       15842 2008-09-18 17:59:19    56 Explain the quantile() fu… \"<p>…\n 4 103312          NA 2008-09-19 16:09:26     4 How to test for the EOF f… \"<p>…\n 5 255697     1941213 2008-11-01 15:48:30     4 Is there an R package for… \"<p>…\n 6 359438        2173 2008-12-11 14:02:06     4 Optimization packages for… \"<p>…\n 7 439526       37751 2009-01-13 15:58:48    23 Thinking in Vectors with R \"<p>…\n 8 445059       37751 2009-01-14 23:09:02    12 Vectorize my thinking: Ve… \"<p>…\n 9 467110       11301 2009-01-21 21:33:13     5 Is R a compiled language?  \"<p>…\n10 476726         277 2009-01-24 21:56:23    10 Filtering data in R        \"<p>…\n# … with 189,920 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\n\nanswers \n\n# A tibble: 250,788 × 7\n      Id OwnerUserId CreationDate        ParentId Score IsAcceptedAnswer Body   \n   <dbl>       <dbl> <dttm>                 <dbl> <dbl> <lgl>            <chr>  \n 1 79741        3259 2008-09-17 03:43:22    79709    -1 FALSE            \"<p>It…\n 2 79768        6043 2008-09-17 03:48:29    79709     9 FALSE            \"<p>us…\n 3 79779        8002 2008-09-17 03:49:36    79709     0 FALSE            \"<p>Th…\n 4 79788          NA 2008-09-17 03:51:30    79709     4 FALSE            \"<p>It…\n 5 79827       14257 2008-09-17 03:58:26    79709     1 FALSE            \"<p>I'…\n 6 79893       14928 2008-09-17 04:11:08    79709     6 FALSE            \"<p>Re…\n 7 83162       15842 2008-09-17 13:27:17    77434    70 FALSE            \"<p>If…\n 8 83222        1428 2008-09-17 13:32:45    77434   236 FALSE            \"<p>I …\n 9 86804          NA 2008-09-17 19:39:37    79709     1 FALSE            \"<p>FY…\n10 95598        1179 2008-09-18 18:49:09    95007     5 FALSE            \"<p>Th…\n# … with 250,778 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nBefore I dive into the analysis, I’m going to use the janitor’s package clean_names() function to convert the column names to snake case. I’ll also get rid of the rows where user id is missing.\n\nquestions <- questions %>%\n  janitor::clean_names() %>%\n  filter(!is.na(owner_user_id))\n\nanswers <- answers %>%\n  janitor::clean_names() %>%\n  filter(!is.na(owner_user_id))"
  },
  {
    "objectID": "posts/2019-07-25_introducing-the-funneljoin-package/index.html#after_join",
    "href": "posts/2019-07-25_introducing-the-funneljoin-package/index.html#after_join",
    "title": "Introducing the funneljoin package",
    "section": "after_join()",
    "text": "after_join()\nLet’s start with a relatively simple question - how many people who ask a question later answer one? To look at this, we’ll need to link the questions with the answers table using owner_user_id and creation_date using funneljoin’s after_join function.\n\nfirst_answer_after_first_question <- questions %>%\n  after_left_join(answers,\n             by_time = \"creation_date\",\n             by_user = \"owner_user_id\",\n             type = \"first-firstafter\", \n             suffix = c(\"_question\", \"_answer\")) \n\nfirst_answer_after_first_question\n\n# A tibble: 60,335 × 12\n   id_question owner_user_id creation_date_quest…¹ score…² title body_…³ id_an…⁴\n         <dbl>         <dbl> <dttm>                  <dbl> <chr> <chr>     <dbl>\n 1       77434         14008 2008-09-16 21:40:29       171 How … \"<p>Su…      NA\n 2       95007         15842 2008-09-18 17:59:19        56 Expl… \"<p>I'… 4249121\n 3      255697       1941213 2008-11-01 15:48:30         4 Is t… \"<p>I'…      NA\n 4      359438          2173 2008-12-11 14:02:06         4 Opti… \"<p>Do…      NA\n 5      439526         37751 2009-01-13 15:58:48        23 Thin… \"<p>I …  440066\n 6      467110         11301 2009-01-21 21:33:13         5 Is R… \"<p>I …      NA\n 7      476726           277 2009-01-24 21:56:23        10 Filt… \"<p>I … 4727309\n 8      495744         12677 2009-01-30 14:48:19         2 Oper… \"<p>I … 2203628\n 9      498932           445 2009-01-31 14:50:28         3 What… \"<p>I …  511763\n10      520810         63372 2009-02-06 15:49:48        20 Does… \"<p>An…      NA\n# … with 60,325 more rows, 5 more variables: creation_date_answer <dttm>,\n#   parent_id <dbl>, score_answer <dbl>, is_accepted_answer <lgl>,\n#   body_answer <chr>, and abbreviated variable names ¹​creation_date_question,\n#   ²​score_question, ³​body_question, ⁴​id_answer\n# ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\n\nThe first two arguments are the tables we’re joining, with the first table being the events that happen first. We then specify:\n\nby_time: the time columns in each table. This would typically be a datetime or a date column. These columns are used to filter for time y being after time x.\nby_user: the user or identity columns in each table. These must always be identical for a pair of rows to match.\ntype: the type of funnel used to distinguish between event pairs, such as “first-firstafter”, first-first”, “last-first”, “any-firstafter”.\nsuffix: just like dplyr’s join functions, this specifies what should be appended to the names of columns that are in both tables.\n\nThe type argument is the most powerful one, as it allows you to switch between types of funnels easily. In this case, we wanted only the first question someone asked and then wanted to know the first answer they gave afterward.\nFor any type of after_join(), the timestamps of the second table (in this case answers) will always be after the timestamp of the first table for each user - we can see there are no rows where creation_date_answer is before creation_date_question:\n\nfirst_answer_after_first_question %>%\n  filter(creation_date_answer < creation_date_question)\n\n# A tibble: 0 × 12\n# … with 12 variables: id_question <dbl>, owner_user_id <dbl>,\n#   creation_date_question <dttm>, score_question <dbl>, title <chr>,\n#   body_question <chr>, id_answer <dbl>, creation_date_answer <dttm>,\n#   parent_id <dbl>, score_answer <dbl>, is_accepted_answer <lgl>,\n#   body_answer <chr>\n# ℹ Use `colnames()` to see all variable names\n\n\nTo answer our original question, let’s get a count by what percent of rows don’t have an id_answer, meaning they never answered a question after asking one. We’ll use the funneljoin’s summarize_conversions() function, where you specify what column indicates whether someone “converted” (in this case answered a question) and returns the total number of users (nb_users), the number of conversions (nb_conversions), and the percent converted (pct_converted).\n\nfirst_answer_after_first_question %>%\n  summarize_conversions(converted = id_answer)\n\n# A tibble: 1 × 3\n  nb_users nb_conversions pct_converted\n     <int>          <int>         <dbl>\n1    60335          13688         0.227\n\n\nWe see that of the approximately 60,000 users that asked an R question, 22.7% percent later went on to answer one.\nHow long does it take for people to answer their first question? We can add gap_col = TRUE to after_join() to add a column, .gap, which is the gap between events (in seconds).\n\ntime_between_first_question_and_answer <- questions %>%\n  after_left_join(answers,\n                  by_time = \"creation_date\",\n                  by_user = \"owner_user_id\",\n                  type = \"first-firstafter\",\n                  gap_col = TRUE) %>%\n  mutate(gap_hours = .gap  / 3600)\n\ntime_between_first_question_and_answer %>%\n  ggplot(aes(x = gap_hours)) + \n  geom_histogram() + \n  scale_x_log10(breaks = c(1, 24, 24 * 7, 24 * 7 * 30), \n                     labels = c(\"1 hour\", \"1 day\", \"1 week\", \"1 month\")) + \n  labs(x = \"Time between asking the first question and answering one\",\n       y = \"Number of users\",\n       title = \"What's the gap between asking your first question and answering one?\",\n       subtitle = \"Only for questions tagged with R on Stack Overflow\") \n\n\n\n\nWe can get an idea from this graph what percentage of people who ask a question answer one within a week, or we could filter our data to get an exact answer. To make it even easier though, we can use the max_gap argument in after_join() to specify that someone needs to have answered a question within a week from their data to be joined. max_gap takes either a difftime or an integer representing the gap in seconds and will filter so that the time between events is less than or equal to that max_gap.\n\nquestions %>%\n  after_join(answers,\n             by_time = \"creation_date\",\n             by_user = \"owner_user_id\",\n             type = \"first-firstafter\",\n             mode = \"left\",\n             suffix = c(\"_question\", \"_answer\"),\n             max_gap = as.difftime(1, units = \"weeks\")) %>%\n  summarize_conversions(converted = id_answer)\n\n# A tibble: 1 × 3\n  nb_users nb_conversions pct_converted\n     <int>          <int>         <dbl>\n1    60335           5349        0.0887\n\n\nNow we see that only 8.9% answer an R question within a week of asking their first one.\nWe might be curious if the likelihood of answering a question later varies by the score of the question they asked. Before doing summarize_conversions, we can group by the score. There are some scores that only appear once (e.g. one person got a score of -18), so we’ll filter for only scores between -4 and 10.\n\nfirst_answer_after_first_question %>%\n  group_by(score_question) %>%\n  summarize_conversions(converted = id_answer) %>%\n  filter(between(score_question, -4, 10)) %>%\n  ggplot(aes(x = score_question, y = pct_converted)) + \n  geom_line() + \n  geom_point(aes(size = nb_users)) + \n  scale_y_continuous(labels = scales::percent) + \n  labs(y = \"% who later answer a question\",\n      x = \"Score on a user's first question\",\n      title = \"If your first question is scored highly, you're more to answer a question later\",\n      subtitle = \"Only for questions tagged with R on Stack Overflow\",\n      size = \"Number of users\") + \n  expand_limits(y = 0)\n\n\n\n\nMost people’s first questions have a score between -1 and 4, but for those who manage to score higher, they’re more likely to answer a question later. As always, you have to be careful of any claims of causality: it’s likely be those who are asking higher scored questions are better at R and thus have the knowledge to later provide answers."
  },
  {
    "objectID": "posts/2019-07-25_introducing-the-funneljoin-package/index.html#other-joins-and-funnel-types",
    "href": "posts/2019-07-25_introducing-the-funneljoin-package/index.html#other-joins-and-funnel-types",
    "title": "Introducing the funneljoin package",
    "section": "Other joins and funnel types",
    "text": "Other joins and funnel types\nWe’ve been looking so far at people’s answers after they’ve asked a question. But are there people who answer a question before they ever ask one?\nWe can examine this by changing the join to:\n\nfirst-first type to filter both tables to the first instance of a user (e.g. their first answer and their first question).\nafter_right_join to keep everyone who asks a question, whether or not they ever answered any before.\n\n\nanswers %>%\n  after_right_join(questions,\n           by_time = \"creation_date\",\n           by_user = \"owner_user_id\",\n           type = \"first-first\",\n           suffix = c(\"_answer\", \"_question\")) %>%\n  summarize_conversions(converted = id_answer)\n\n# A tibble: 1 × 3\n  nb_users nb_conversions pct_converted\n     <int>          <int>         <dbl>\n1    60335           2795        0.0463\n\n\nYes, 4.63% of people have answered a question before they asked their first one.\nFor people who answer questions after asking one, let’s find out how many they answer. We’ll switch our query to an after_inner_join with a type first-any. Each user will only have one question, their first, as we used a first-Y type. But it has one row per answer they gave afterwards as we used a X-any type.\n\nquestions %>%\n  after_inner_join(answers,\n           by_time = \"creation_date\",\n           by_user = \"owner_user_id\",\n           type = \"first-any\") %>%\n  count(owner_user_id) %>%\n  ggplot(aes(n)) + \n  geom_histogram() + \n  scale_x_log10() + \n  labs(x = \"Number of answers\",\n       y = \"Number of users\",\n       title = \"How many questions do people answer after asking their first one?\",\n       subtitle = \"Only for questions tagged with R on Stack Overflow and people who answer at least one afterwards\")\n\n\n\n\nNot surprisingly, we see people mostly answer only 1 or 2 questions, with a long tail of power users answering 100+ questions."
  },
  {
    "objectID": "posts/2019-07-25_introducing-the-funneljoin-package/index.html#conclusion",
    "href": "posts/2019-07-25_introducing-the-funneljoin-package/index.html#conclusion",
    "title": "Introducing the funneljoin package",
    "section": "Conclusion",
    "text": "Conclusion\nSome of the power of funneljoin comes from making it possible to code things you didn’t know how to before. But a lot of it comes from bringing things from “possible but time-consuming and error-prone” to “easy.” When you’re doing exploratory analysis, you want to be able to iterate quickly between ideas: switching from the first thing someone added to their cart after searching for an item, to everything they added, to only items they added within a week.\nIn the next post, I’ll be sharing funneljoin’s other main functions: funnel_start() and funnel_step(). In the meantime, if you find any bugs or have a feature request or question, please create an issue on GitHub or get in touch on Twitter!"
  },
  {
    "objectID": "posts/2017-11-30_making-r-code-faster-a-case-study/index.html",
    "href": "posts/2017-11-30_making-r-code-faster-a-case-study/index.html",
    "title": "Making R Code Faster: A Case Study",
    "section": "",
    "text": "About two months ago I put a call out to Rstats twitter:\nI had a working, short script that took 3 1/2 minutes to run. While this may be fine if you only need to run it once, I needed to run it hundreds of time for simulations. My first attempt to do so ended about four hours after I started the code, with 400 simulations left to go, and I knew I needed to get some help.\nThis post documents the iterative process of improving the performance of the function, culminating in a runtime of .64 seconds for 10,000 iterations, a speed-up of more than 100,000x."
  },
  {
    "objectID": "posts/2017-11-30_making-r-code-faster-a-case-study/index.html#the-problem",
    "href": "posts/2017-11-30_making-r-code-faster-a-case-study/index.html#the-problem",
    "title": "Making R Code Faster: A Case Study",
    "section": "The problem",
    "text": "The problem\nAt Etsy I work a lot on our A/B Testing system. When assigning browsers randomly to experimental groups, we do so based on their browser id (cookie) or device id for apps. But when we analyze our data, we can use two different methods: visit level (chunks of behavior) and browser level. For more details, see my talk on A/B Testing (starting at 17:30).\nWhile we offer both, we analysts encourage everyone to favor browser metrics over visit metrics. One reason is that visits can come from the same browser or person, violating the independence assumption of the statistical tests we use. In theory, this should inflate our false positive rate, but we’d never actually tested this with our own browsers, and a theoretical concept was not always convincing to our partner teams.\nI therefore set out to simulate hundreds of null A/B Tests using our own data. I wanted to see if the percentage with p < .05, our false positive rate, was actually around 5%, or if it was inflated, as we expected."
  },
  {
    "objectID": "posts/2017-11-30_making-r-code-faster-a-case-study/index.html#asking-for-help",
    "href": "posts/2017-11-30_making-r-code-faster-a-case-study/index.html#asking-for-help",
    "title": "Making R Code Faster: A Case Study",
    "section": "Asking for help",
    "text": "Asking for help\nI am very fortunate to have a data scientist brother who will jump in to help out even when not directly asked. A lot of the code improvements following come from him.\n\n\n\ncenter\n\n\nI’ve also gotten to know many other people in the R community through RLadies, conferences, and twitter, some of whom offered help as well. But even if you’re new to R and don’t know anyone, there are people who will jump in and help you too. RStudio recently launched a new community website where you can ask quesitons ranging from a specific issue that you need to debug to why you should use RMarkdown. There’s also a slack community, organized by Jesse Maegan, that brings people wanting to learn R together with mentors to work through Garrett Grolemund and Hadley Wickham’s R for Data Science book. Jesse has been writing a great series of blog posts reflecting on some lessons from each week, and she’ll also be doing another round.\nPart of why I wrote this post is I believe those who are privileged–whether by having a data science job, getting to go to conferences, or having a formal education in programming or statistics–should try to share that through public work. I hope some of the lessons learned can help others optimize their code performance when needed. The first three are helpful for any language and the final two for R especially."
  },
  {
    "objectID": "posts/2017-11-30_making-r-code-faster-a-case-study/index.html#lessons-learned-on-performance",
    "href": "posts/2017-11-30_making-r-code-faster-a-case-study/index.html#lessons-learned-on-performance",
    "title": "Making R Code Faster: A Case Study",
    "section": "Lessons learned on performance",
    "text": "Lessons learned on performance\n\nYou may not need big data tools\nIn my tweet, I mentioned some packages I’d been trying, including foreach and doparallel, packages for parallel processing. Some folks replying also suggested sparklyr (which integrates spark and R) and Rcpp (which integrates R and C++). I thought I needed to use these because I was dealing with big data (for R) - 10+ million rows with some text columns!\nBut even if the data you start with is large, you may be able to make it smaller by eliminating extra columns or summarizing the data. In the next section, you’ll see how I compressed my data into 3 numeric columns and fewer than a thousand rows.\n\n\nTry to do everything (grouping and counting) you can in SQL and eliminate unnecessary information\nIf you want to follow along, please run this R code to simulate a dataset (this is smaller than my dataset, so if you time the later code your times will not match mine):\ninitial_bis <- sprintf(\"%s%s%s\", stringi::stri_rand_strings(800000, 5, '[A-Z]'),\n      stringi::stri_rand_strings(800000, 4, '[0-9]'), \n      stringi::stri_rand_strings(800000, 1, '[A-Z]'))\nbi <- sample(initial_bis, size = 1000000, replace = TRUE)\nc <- rbinom(1000000, 1, 0.2)\nvi <- sprintf(\"%s%s%s\", stringi::stri_rand_strings(1000000, 5, '[A-Z]'),\n      stringi::stri_rand_strings(1000000, 4, '[0-9]'), \n      stringi::stri_rand_strings(1000000, 1, '[A-Z]'))\n      \nbi_name <- \"browser_id\"\nc_name <- \"converted\"\nvi_name <- \"visit_id\"\n\nsearch_visits <- data.frame(bi, c, vi)\nnames(search_visits) <- c(bi_name, c_name, vi_name)\nHere was my original code that:\n\nPulled a table down from SQL that had all the visits that had a search in the previous X days, including whether they converted or not, and their browser id.\n\nSELECT * FROM erobinson.simulate_fp_search\n\nRandomly assigned on the browser level a label of 0 or 1.\n\nlibrary(data.table)\nlibrary(dplyr) \n\nN <- nrow(distinct(search_visits, browser_id))\n\nbrowsers <- search_visits %>%\n  distinct(browser_id) %>%\n  mutate(ab_variant = sample(c(0,1), N, replace = TRUE))\n\nbrowsers <- as.data.table(browsers)\nbrowsers <- setkey(browsers, browser_id)\ndat_w_labels <- merge(search_visits, browsers, all.x=TRUE)\ndat_w_labels <- dat_w_labels[, .(.N), by = .(ab_variant, converted)] %>% \narrange(ab_variant)\n\nCounted up the number of total visits and converting visits for each label.\n\nfailures <- dat_w_labels %>%\n  filter(converted == 0) %>% \n  pull(N)\n\nsuccesses <-  dat_w_labels %>%\n  filter(converted == 1) %>% \n  pull(N)\n\nRan a prop test comparing the two groups.\n\nres <- prop.test(successes, failures + successes)\nThe table I started with was over 10 million rows, and I was doing many operations:\n\nGetting the number of distinct browser ids\nMaking a new table that had every browser id and their label\nMerging the new table with the original search visits table\nGrouping that new table by whether the visit converted or not and its label, counting the number of each type\nExtracting the number of conversions and non-conversions\nDoing a prop test\n\nThe last three steps are fast, but the first three are very long. This isn’t even including the initial 5 minutes (!) it takes to load the sql table in the first place. We’re able to refactor and make it faster by realizing a few things:\n\nWe don’t need the big text columns visit id and browser id\nIn SQL, we can group by browser id so each row has 1) the number of visits for a browser and 2) the number of visits that converted for that browser. With that, we’ll have a smaller table of only two numeric columns.\nWe can then label each row with 0 or 1 randomly, assigning treatment on the browser level.\nNext, we sum up the total visits column and the converted column, grouping by label.\nFinally, we run a prop.test\n\nHere’s what the sql new code looks like:\nSELECT count(*) as total_visits, sum(converted) as converted \nFROM erobinson.simulate_fp_search\nGROUP BY browser_id \nAgain, to follow along, here’s that sql code in R:\nlibrary(dplyr)\n\nsearch_visits <- search_visits %>% \n  group_by(browser_id) %>%\n  mutate(total_visits = n(), converted = sum(converted)) %>%\n  select(total_visits, converted)\nI then created my simulation function and ran it 1000 times.\nlibrary(dplyr)\n\nsimulate_p_value_visits <- function() {\n  results <- search_visits %>%\n    # group_by here is really a mutate and group_by\n    group_by(label = sample(c(0,1), n(), replace = TRUE)) %>%\n    summarize(total_visits = sum(total_visits), converted = sum(converted)) \n  \n  prop.test(results$converted, results$total_visits)$p.value\n}\n\nsimulate_p_values_visit_result <- replicate(1000, simulate_p_value_visits())\n\nfalse_positive_rate <- sum(simulate_p_values_visit_result < .05)/\nlength(simulate_p_values_visit_result)*100  \nWhile switching the dplyr to data.table could probably speed it up even more, right now we’re down to about 14 minutes runtime for a 1000 iterations.\n\n\nEliminate redundancy\nBut we can then recognize that our table currently has a lot of redudancy: we have many browsers that have 1 visit and 0 conversions, 2 visits and 0 conversions, etc.\nTherefore, we can make our code faster by: - Transforming our table so each row is a unique combination of visits & conversions, with a column that is the number of browsers with that combination. We can do this in SQL and output the table as “count of counts”.\n\nwith counts as (\n  SELECT count(*) as total_visits, sum(converted) as converted \n  FROM erobinson.simulate_fp_search\n  GROUP BY browser_id\n)\n  SELECT count(*) as n, total_visits, converted\n  FROM counts\n  GROUP BY total_visits, converted\nTo follow along, run the R code to create count_of_counts.\ncount_of_counts <- search_visits %>%\n  count(total_visits, converted)\n\nUsing the binomial distribution to simulate splitting browsers into A and B groups.\n\nBecause the bernoulli distribution is just a special case of the binomial distribution where n = 1, we’re doing the same process as before, but we do many fewer computations!\n\nAs before, summarizing the number of visits and conversions in each group and apply our proportion test.\n\nlibrary(dplyr) \n\nsimulate_p_value <- function() {\n  # put about half (with binomial sampling) in each group\n  result <- count_of_counts %>%\n    mutate(A = rbinom(n(), n, .5),\n           B = n - A) %>%\n    summarize(total_A = sum(total_visits * A),\n              total_B = sum(total_visits * B),\n              converted_A = sum(converted * A),\n              converted_B = sum(converted * B))\n\n  prop.test(c(result$converted_A, result$converted_B), \n  c(result$total_A, result$total_B))$p.value\n}\n\nsim_pvals <- replicate(1000, simulate_p_value())\n\nfalse_positive_rate <- sum(sim_pvals < .05)/length(sim_pvals)*100  \n\n\nVectorize\nWhile the previous code is pretty fast, we can get it even faster by vectorizing the prop test. If you’re not familiar with vectorization in R and why it’s faster, check out Noam Ross’s excellent blog post.\nHere is the new code using a vectorized proportion test (courtsey of David Robinson’s splittestr package).\nlibrary(devtools)\ninstall_github(\"dgrtwo/splittestr\")\nlibrary(splittestr)\nlibrary(dplyr)\nlibrary(tidyr)\n\nsimulated_pvals <- count_of_counts %>%\n  crossing(trial = 1:1000) %>%\n  mutate(A = rbinom(n(), n, .5), B = n - A) %>%\n  group_by(trial) %>%\n  summarize(total_A = sum(total_visits * A),\n            total_B = sum(total_visits * B),\n            converted_A = sum(converted * A),\n            converted_B = sum(converted * B)) %>%\n  mutate(pvalue = vectorized_prop_test(converted_A, total_A - converted_A, \n  converted_B, total_B - converted_B)$p.value)\n  \nfalse_positive_rate <- sum(simulated_pvals$pvalue < .05)/\nlength(simulated_pvals$pvalue)*100  \nCrossing is the tidyr version of mutate: it creates a tibble from all the combinations of the supplied vectors. In this case, that means we’ll have a 1000x the number of rows in “count of counts.” For each trial, we’ll simulate putting half of the browsers in A and half in B. Then we can get the total number of visits and converted visits for each trial and use our vectorized prop test to create a new variable that is the p-value.\n\n\nUse matrix operations\nBut wait, there’s more! The issue with the previous version is memory: we’re creating that intermediate product that has hundreds of thousands of rows. Instead, we can use matrix operations, which are faster and less memory-taxing than R. We don’t get to use tidyverse code, but sometimes sacrifices must be made.\nHere, we first create two matrixes, A and B. Each column represents one simulation, and each row a unique combination of visits and conversion (e.g. one row is for browsers with 1 visit and 0 conversions, another for 2 visits and 1 conversion, etc). We’ve used the binomial distribution again to simulate splitting n, the number of browsers with a certain combination of visits and conversions, into A and B.\nA <- replicate(1000, rbinom(nrow(count_of_counts), count_of_counts$n, .5))\nB <- count_of_counts$n - A\nWe create four vectors of length 1000, one entry for each simulation. Two are for the total number of visits in A or B and two are for the the number of converted visits in A or B. For example, if the first entry in A and B represents the number of browsers with 5 visits and 1 conversions in A and B, respectively, we just multiply each of those by 5 to get the number of visits and by 1 to get the number of conversions. So if it was 2 in A and 3 in B, that means A had 10 visits and 2 conversion while B had 15 visits and 3 conversions. We do this for every row and then add up the column to get the total number of visits and total number of conversions for that simulation in A and in B. This operation is repeated for all 1000 columns (simulations).\ntotal_A <- colSums(A * count_of_counts$total)\ntotal_B <- colSums(B * count_of_counts$total)\nconverted_A <- colSums(A * count_of_counts$converted)\nconverted_B <- colSums(B * count_of_counts$converted)\nOur last step is to use the vectorized prop test to get a 1000 p-values and then calculate what percentage of them are less than .05.\nlibrary(devtools)\ninstall_github(\"dgrtwo/splittestr\")\nlibrary(splittestr)\n\npvals <- vectorized_prop_test(converted_A, total_A - converted_A,\n                              converted_B, total_B - converted_B)\n                              \nfalse_positive_rate <- sum(pvals < .05)/length(pvals)*100                         \n\n\nFinal Tally\nHere’s the final comparison of performance (calculated using the great tictoc package):\n\n\n\n\n\n\n\n\n\n\nAttempt\nSQL table runtime\n1 iteration runtime\n1000 iterations runtime\n10000 iterations runtime\n\n\n\n\nOriginal\n5+ minutes\n215 seconds\n\n\n\n\nSecond Version\n23 seconds\n.5 seconds\n839 seconds\n\n\n\nSummarized Version\n.7 seconds\n.03 seconds\n9.2 seconds\n\n\n\nVectorized Version\n.7 seconds\n\n.39 seconds\n7.9 seconds\n\n\nMatrix Version\n.7 seconds\n\n.11 seconds\n.64 seconds"
  },
  {
    "objectID": "posts/2017-11-30_making-r-code-faster-a-case-study/index.html#conclusion",
    "href": "posts/2017-11-30_making-r-code-faster-a-case-study/index.html#conclusion",
    "title": "Making R Code Faster: A Case Study",
    "section": "Conclusion",
    "text": "Conclusion\n\n\nOne rule of thumb for maximizing #rstats performance is that the way you’d do something once is rarely the best way to do it 1000X\n\n— David Robinson (@drob) October 5, 2017\n\n\nAt RStudio::Conf 2017, Hadley Wickham discussed how the bottleneck in writing code is usually thinking speed, not computational speed. Therefore, he advised that you shouldn’t prematurely worry about optimizing for performance but rather about making sure your code is clear. In this case, though, the point was to run the same code hundreds of times, so I needed to start worrying about performance. Through the process, I learned a lot about how R works “under the hood” and how to conceptualize problems a different way.\nNext time, I’ll be returning soon to the topic of A/B Testing, sharing what I’ve learned from reading industry papers."
  },
  {
    "objectID": "posts/2018-05-29_advice-for-applying-to-data-science-jobs/index.html",
    "href": "posts/2018-05-29_advice-for-applying-to-data-science-jobs/index.html",
    "title": "Advice for Applying to Data Science Jobs",
    "section": "",
    "text": "Following Dave Robinson’s sage tweet to write a blog post when you’ve given the same advice three times, this post is a collection of my thoughts and recommendations for people interested in applying to data science jobs in the US. Many of these principles also apply to tech jobs in general.\nA disclaimer: I have never worked as a recruiter or career coach. This knowledge comes from mainly from my study of Organizational Behavior (including negotiations and women in tech) in graduate school and my own career. This advice also will not fit every single situation. The people who will benefit the most from this post are probably those who are applying for their first data science job and/or are in or recently out of school. If there’s another tip or a caveat I should add, send me a note on twitter!"
  },
  {
    "objectID": "posts/2018-05-29_advice-for-applying-to-data-science-jobs/index.html#application",
    "href": "posts/2018-05-29_advice-for-applying-to-data-science-jobs/index.html#application",
    "title": "Advice for Applying to Data Science Jobs",
    "section": "Application",
    "text": "Application\n\nOnline Presence\nTake stock of your online presence. Check your privacy settings on Facebook or any other social media you use to make sure you’ve limited what’s available publicly. While many online job applications now have places for twitter handles, don’t include yours unless you use it mainly in a professional manner (e.g. tweeting about data science resources or pets. Pets are always appropriate).\nIf you have a GitHub, pin the repos you want people to see and add READMEs that explain what the project is. I also strongly recommend creating a blog to write about data science, whether it’s projects you’ve worked on, an explanation of a machine learning method, or a summary of a conference you attended. If you need more convincing that blogging is a great use of your time, check out this post by Dave Robinson on the subject. If you program in R, you can use blogdown to create a website. Emily Zabor has written a great tutorial for this; find more blogdown resources in Mara Averick’s extensive list. If you don’t use R, you can use Hugo (which blogdown is built on top of) directly or start out with Medium.\n\n\nFinding and Evaluating Jobs\n\nBrowse widely: jobs in data science go by many names besides data scientist. These include: product analyst, data analyst, research scientist, quantitative analyst, and machine learning engineer. Different companies use different names for similar roles and some are changing what their titles mean (see Lyft’s post on their recent shift of data analyst to data scientist and data scientist to research scientist). Even the definition of what separates a data analyst from a data scientist is not agreed upon (see this recent post by Mikhail Popov summarizing different perspectives). Try searching for all of these terms to find positions and then use the description to evaluate the fit. If you’re interested in start-ups, Angelist has thousands of positions, many of which list a salary range.\nDo some self-reflection: rather than applying to every type of data science job you find, think about where you want to specialize. A distinction I’ve found helpful when thinking of my own career and looking at jobs is the Type A vs. Type B data scientist. “A” stands for analysis: type A data scientists have strong statistics skill and the ability to work with messy data and communicate results. “B” stands for build: type B data scientists have very strong coding skills, maybe have a background in software engineering, and focus on putting machine learning models, such as recommendation systems, into production. Also consider what data stage you want your company to be in (see discussion in Robert Chang’s post). An advantage of working at a smaller company is you can try different parts of data science; big companies are more specialized, so you may not be able to try both creating a lifetime value model and tuning a recommendation system, and many problems, like data engineering, are abstracted away. On the other hand, at smaller companies you’ll usually need to deal more with data quality problems, poor documentation, and slow query processing. While data stage is correlated with company size, a big company that’s been around for 100 years may not have data science maturity.\nDon’t demand perfection: your first job in the field probably won’t be your dream one. It’s easier to transition within your field or bring data science into your current role; even if you’re looking to eventually leave your domain, you may need to start out by moving to a position where you can leverage your other skills. That doesn’t mean you shouldn’t have certain requirements and preferences (see above), but it does mean you’ll want to have some flexibility. It’s very normal to switch jobs in tech even after just a year or two, so you’re not signing yourself up for the next 15 years. To me, the most important criteria for my first job was that it was a supportive environment, with lots of other analyst, where I could learn a lot. And as Jacqueline Nolis points out, you can’t know exactly what you want before you’re even in the field and you’ll learn even from bad jobs, so don’t stress too much:\n\n\n\nEarly in my career I spent a lot of emotional effort trying to figure out which job was “right” for me and what exactly I needed, but now I realize that even my bad jobs were good experiences to learn what’s important to me. 2/\n\n— Jonathan Nolis (@skyetetra) May 29, 2018\n\n\n\nDon’t undersell yourself: job descriptions are generally wishlists with some flexibility. If you meet 80% of the requirements (e.g. you’re a year short of their required work experience or haven’t worked with one component of their tech stack), but are otherwise a good fit, you should still apply. With that said, be wary of job descriptions that describe a unicorn - a PhD in Computer Science who’s also worked for 5+ years as a data scientist and is an expert in cutting-edge statistics, deep learning, and communicating with business partners - and lists a huge range of responsibilities, running from doing production-level machine learning to creating dashboards to running A/B tests. It usually means they don’t know what they’re looking for and they expect a data scientist to come and solve all their problems without any support.\nLook for connections on LinkedIn: check if you know anyone at the company you’re interested in. If you don’t know anyone, see if there’s anyone in your alumni networks (bootcamp, college, that kickball team you joined one summer). You can also check for second connections and see if the person who bridges you can introduce you. Many jobs get hundreds if not thousands of applications for each position and having someone refer you or give you feedback on what the team is looking for is enormously helpful. If you do reach out, take the time to tailor the message: if you don’t know the person, do your research and reference their background or public work.\nCheck out meetups and conferences: sometimes hiring managers will come to meetups or conferences to recruit. You may also meet someone in the company or sub-industry you’re interested in. You can ask if they have time for an informational interview so you can learn more about their field. If you ask if their company has an opening or if they can refer you, though, you’ll probably be directed to the company’s career page. This is why it’s important to build your network before you need it - starting off with a strong ask is not a great way to build a mutually fulfilling relationship.\n\nCheck out my previous posts about finding community and reaching out if you’re interested to read more about how you can build your network.\n\n\nResume and Cover Letter\n\nKeep your resume to one page: you don’t have to list everything you’ve ever done. Try making a master resume which has all of your work history and then pull what’s most relevant for each job. Don’t put a laundry list of every technology you’ve ever used; focus on the ones that you’ll be comfortable talking about. A tip from a recruiter: “don’t use graphs showing how experienced you are in certain technologies. If you have python being a full bar, do you actually know 100% of what there is to know about python?” If you’re running out of room and have a summary or objective statement, you can leave it off as those are much less common now. If you come from academia and have work experience, consider leaving off publications if they’re not looking for a PhD and it’s not a research position.\nProofread: having typos or grammar errors in you’re reSume can be the quickest way to have your aplication elimminated. Use spel-check and have a friend or too check it over.\nDon’t include a picture: while this is common in some countries in Europe and South America, it is not appropriate in the US.\nTailor your resume to the position: bigger companies often have automated resume-screening systems that check for keywords. Look in the job description to find those words; you can use a tool like TagCrowd to see which words are most common. For example, does it say leadership instead of management? Does it say NLP or natural language processing? Change your words to match theirs.\nStart your bullets with a verb: write “analyzed 300 papers” instead of “I analyzed 300 papers.”\nQuantify your accomplishments where possible: instead of saying “ran experiments on our ranking algorithm”, write “ran more than 60 experiments that led to $2.3 million in additional revenue.”\nWrite a cover letter: if there is a place to submit a cover letter, do so. Some companies will eliminate candidates if they haven’t written one. Just like your resume, you can have a master cover letter that you pull paragraphs from. Tailor at least the first and closing paragraph to the company and make sure you get their name correct! The more specific you can be, the better; show you’ve done your research on the company and have specific reasons why you’re interested in this position. Try to find the hiring manager’s name (see here) so you can address the letter to them instead of a “Dear Hiring Manager.” If you want more guidance on writing a good letter, these articles of 31 tips and 5 common phrases to avoid are great places to start.\n\n\n\ndata science hiring PSA:some advice out there says that cover letters don't matter, so just slap a new file name on your resume and label it a cover letter, or write a single sentence that says \"this is my cover letter.\"this is bad advice.\n\n— Jesse Maegan (@kierisi) May 7, 2018"
  },
  {
    "objectID": "posts/2018-05-29_advice-for-applying-to-data-science-jobs/index.html#interviewing",
    "href": "posts/2018-05-29_advice-for-applying-to-data-science-jobs/index.html#interviewing",
    "title": "Advice for Applying to Data Science Jobs",
    "section": "Interviewing",
    "text": "Interviewing\n\nPrepare and practice: there are two main types of questions you’ll be asked- technical and behavioral. For behavioral questions such as “tell me about a time you had a conflict with a team member” or “what’s your greatest strength,” tell a story using the situation-behavior-outcome model: describe the situation you faced, how you handled it, and the result. Try to keep your answers short but comprehensive. It’s worth keeping a list of these examples and thinking about which ones most show case the experiences and traits most relevant to the specific job you’re interviewing for. For technical questions, you’ll want to find out the interview structure. The structure of interviews and types of questions range widely: you may have to do a take-home, answer Computer Science questions (e.g. invert a binary tree), explain random forests, give a presentation, write SQL code, etc. It’s very helpful to know ahead of time what to expect. Check out Glassdoor for reviews of the interview process and ask your network if anyone’s interviewed there. You can also ask HR in your initial phone screen what to expect. If you’re coming in for multiple interviews, you can ask what topics will be covered in each and what the structure will be (e.g. “Will I be doing any whiteboard coding?”). Once you know, you can focus your preparation there. If you’re stumped on a problem, consider saying, “I am not sure, but this is how I would go about solving it.” Often, you’re evaluated on how you work through the problem, not whether you get the correct answer. For more tips, check out Trey Causey’s post on his experience in the data science job market, Erin Shellman’s guide to landing a data science job, and Mikhail Popov’s post on Wikimedia Foundation’s process for fulfilling a data analyst position.\nAnything on your resume is fair game: if you have an internship from six years ago on your resume or list AWS as a skill, be prepared to be asked a question about it.\nResearch the company: you should have done some when writing your cover letter, but once you get the interview, dig a little deeper. In addition to the tips in the thread below, find out about your interviewers’ professional accomplishments. I was very impressed when a candidate I was interviewing asked some technical questions about a presentation I had given.\n\n\n\nEspecially if you are interviewing at a startup, spend an hour doing your research. Here's the formula:1) Crunchbase. Find out funding, investors, etc.2) LinkedIn. Who works there?3) Glassdoor. What do people say about the company?4) Company web site. Read all you can stand.\n\n— Jensen Harris (@jensenharris) May 16, 2018\n\n\n\nHave questions ready: each interviewer should leave time for questions at the end. If they don’t, that’s a bad sign! Interviewing is a two-way matching process: you’re evaluating them as much as they’re evaluating you. If you don’t know what to ask, check out Julia Evans’ list. She splits them into categories such as quality of life, culture, and management practices; think about what matters most to you! You can ask each interviewer different questions to maximize how many you can get answered, but you could also try to ask multiple people the same questions to see if and how their answers differ. If it’s a start-up, I highly recommend checking out this list of questions to ask.\nNever name a salary number: don’t tell them either your current salary or your expectation for this one. In some places, including New York City and California, it is actually illegal for them to ask you your current salary. If you name a number, you risk them giving you a lower offer than they would have otherwise. Their offer should not depend on your current salary or expectations; it should be your worth in the market and similar to the salary of your peers there! If there’s question on the application form, put “NA” or “flexible;” if forced to write a number, put 0 and add somewhere else “Note: I entered $0 on the salary question because I am flexible on salary if we determine there is a mutual fit.” If you’re asked in an interview, redirect the conversation by saying something like, “Before discussing salary, I’d like to learn more about the position and focus on what value I can bring to the company. I’m sure if it’s a good fit we can reach an agreement on a competitive overall package.” If it’s about your current salary and you’re in a different job type/industry with a lower salary, you can say something like, “Since this position is a large change from my current job, let’s discuss what my responsibilities at this company will be and work together at the end of the process to determine a fair salary for this position.” If they insist on knowing your expectation, give a large range based on research into what’s standard in the industry and at that company. For example, you can say, “From my research and past experience, my understanding is a base salary of $95k to $120k is standard, but I’m most interested in the fit with the position and overall compensation package.” You can look at the H1B visa data, Glassdoor, and paysa to give yourself a ballpark. And talk to friends! Knowledge is power. Some friendly folks will also do a salary information exchange:\n\n\n\nwhile I'm on it, my DMs are always open if you want to send me your salary. especially if you work in data, work in Toronto, or work in a \"technical\" role with 0-3 years of experience. if you're an URM, I'll tell you mine too. knowledge is power 💪\n\n— Sharla Gelfand (@sharlagelfand) April 15, 2018\n\n\n\nHandle rejection gracefully: you will almost inevitably get rejected from jobs, maybe dozens of them. Data science is a competitive field, and this is a very normal process that everyone goes through. If they give you the rejection (versus just never hearing back), you can express your disappointment politely (e.g. “I’m sorry to hear that”) and thank them for their consideration. You can ask for feedback, but know that many hiring managers won’t be able to give you any because they want to avoid the possibility of being sued if a comment is interpreted as discriminatory. While it’s okay to take a little time to wallow, don’t lash out in public or to the hiring manager. It won’t help the situation, but it will hurt your professional reputation.\n\n\n\none of the things that should go without saying, but here we are, is that it's important to handle rejection gracefully. data science positions can be highly competitive. having a public meltdown and/or sending the hiring manager nasty/demanding emails is… bad.\n\n— Jesse Maegan (@kierisi) May 29, 2018"
  },
  {
    "objectID": "posts/2018-05-29_advice-for-applying-to-data-science-jobs/index.html#offer-stage",
    "href": "posts/2018-05-29_advice-for-applying-to-data-science-jobs/index.html#offer-stage",
    "title": "Advice for Applying to Data Science Jobs",
    "section": "Offer Stage",
    "text": "Offer Stage\n\nInitial response\nFor the initial offer, you may get a phone call, an email with the details, or an email asking for time to go over the offer by phone. In all cases, make sure you express your excitement and gratitude for the opportunity. Don’t accept right away: get the full offer in writing, say you need to review it, and ask if you can reconnect in a few days. This sets the stage for negotiating and gives you time if you’re considering other offers. If you need more information (e.g. starting date possibilities, health insurance information), you should ask for it before your second conversation so you can have the full picture.\n\n\nJuggling multiple options\nUnfortunately, you probably won’t get offers (or rejections) from all the companies you’re interested in at once. More likely, when you get an offer, you might be in the final round for another company, just finished with the phone screen in another, and waiting to hear back from others. What if one of the ones you’re waiting on is your “dream job”?\nIf you’re in or very close to the final round with a company, tell them about your other option. Reiterate how interested you are in their company (and if they’re your first choice, tell them!), but explain that you’re received another offer and would appreciate if they would be able to give you a decision by X date. Recruiters understand that you’re likely to be interviewing with other companies and deal with this situation all the time. If you’re very early in the other process, it’s unlikely they’ll be able to speed it along enough to have an offer in time, and you may need to decide whether to accept the offer without knowing your other options.\n\n\nNegotiating\nEven if the offer is more than you expected or it would be a big increase from your current job, you should negotiate. Tech companies expect you to negotiate. You can almost always get at least another $5k or a 5% increase in base salary and very likely more depending on the company and original offer. You could also ask for a signing bonus or more stock options. Remember that it’s not selfish or greedy to negotiate:\n\n\nOther thoughts: it's not about the salary you make now. It's about the salaries of your peers at the new place. It may be a large bump for you now, but once you're hired you'll be doing the same work for less pay.\n\n— bletchley punk (@alicegoldfuss) May 2, 2018\n\n\nAlso think about the non-monetary benefits you’re interested in. For example, maybe you want to be able to work from home one day a week. Maybe you want them to cover the cost of two conferences a year. If it’s a smaller company, you can ask for a different job title.\nHow much you can negotiate for depends on your bargaining position and the company. A non-profit will probably have less room on salary. If you have another offer or you’re well compensated in your current job, you have a strong alternative. If you have a rare skill that they recruited you for or the role has been open for a long time, it’s going to be harder for them to find someone else. You should do your research on the salary websites mentioned above so you can explain why you’re asking for the numbers you are. Aim high so you have room to compromise and give a number, not a range - if you say you’re looking for between a 5k and 10k increase, they’ll probably give you the 5k. Reiterate why you’re excited about the company and position and how you bring X, Y, Z to the table. Finally, start the conversation with the set of things you’re interested in instead of raising and settling one issue after another. This way, they’ll both feel they have the picture of what you want and you can do compromises among issues (e.g. you might accept a $5k base salary increase and $5k signing bonus instead of a $10k base salary increase).\nI highly recommend reading more about how to negotiate well; this comprehensive two-part post by Haseeb Qureshi on his 10 rules for negotiating (with many specific examples from his own successful tech job search) is an excellent place to start. I also like these HBR articles on 15 rules for negotiating an offer and on advice for women (who unfortunately need to sometimes modify negotiation tactics).\nIt is very, very, very rare for a company to pull an offer because you negotiated. If they do, you do not want to work there.\nOne caveat: if you’re negotiating and you get everything you ask for, the company expects you to accept! You of course don’t have to, but you shouldn’t be stringing along a company you’re not serious about. And knowing you’ll accept also allows you to say the magic words, “If we can get to X, Y, and Z, I would be thrilled to accept the offer.”"
  },
  {
    "objectID": "posts/2018-05-29_advice-for-applying-to-data-science-jobs/index.html#conclusion",
    "href": "posts/2018-05-29_advice-for-applying-to-data-science-jobs/index.html#conclusion",
    "title": "Advice for Applying to Data Science Jobs",
    "section": "Conclusion",
    "text": "Conclusion\nJob hunting is stressful, especially if you’re working full-time and/or looking to change industries. I hope this post has given you a good starting point for understanding the hiring process in data science, what mistakes to avoid, and what strategies you can leverage in the process. If you’re still looking more, this is an incredibly comprehensive guide from Favio Vazquez, which includes his own thoughts and links to dozens of resources.\nAs I said at the start, feedback and additions to this post are very welcome! Thank you to Ilana Mauskopf, Dana Levin-Robinson, Jesse Maegan, Philipp Singer, Jacqueline Nolis, Naoya Kanai, and Michael Berkowitz for their additions to this post.\nIf you found this post useful, you might be interested in the book on data science careers I wrotewith Jacqueline Nolis, “Build Your Career in Data Science,”, available for 40% off with the code buildbook40%."
  },
  {
    "objectID": "posts/2016-09-30_topic-modeling-the-new-york-times-and-trump/index.html",
    "href": "posts/2016-09-30_topic-modeling-the-new-york-times-and-trump/index.html",
    "title": "Topic Modeling the New York Times and Trump",
    "section": "",
    "text": "When Donald Trump first entered the Republican presidential primary on June 16, 2015, no media outlet seemed to take him seriously as a contender. He is a highly unusual candidate, and some in the media have admitted that they, and the media more generally, don’t know how to cover him, both in the primary and now in the general election. Trump himself has criticized the media’s coverage of him:\nI was interested in quantitatively examining how the media has covered Trump from the start of his campaign until now. For my initial exploration, I focused on extracting the topics of the New York Times’ articles covering Trump. You can find all of my code and my presentation to Metis on my github repo."
  },
  {
    "objectID": "posts/2016-09-30_topic-modeling-the-new-york-times-and-trump/index.html#creating-the-dataset",
    "href": "posts/2016-09-30_topic-modeling-the-new-york-times-and-trump/index.html#creating-the-dataset",
    "title": "Topic Modeling the New York Times and Trump",
    "section": "Creating the dataset",
    "text": "Creating the dataset\nThe Times has a pretty user-friendly API. You simply need to request an API key and then you can get started gathering articles. I collected all the articles about Donald Trump between the start of his campaign (June 16, 2015) until now, but I then faced the main limitation of the Times API: it doesn’t return the full-text of the articles. For this, I turned to the newspaper package for Python. The newspaper package takes a url and then returns information about the article, including the authors and full text. The package also includes some natural language processing and can give you a summary and keywords list for each article.\nI also had to limit my articles to those that were primarily about Trump. Because the API returned any article that simply had “Donald Trump” in it, I had a lot of irrelevant articles that were not actually about Trump and would have biased the analysis. I used the previously mentioned keywords function in the newspaper package to find the keywords for every article, selecting only those articles that had “Donald” or “Trump” as a keyword. My final dataset for analysis was about 2,200 full-text news articles primarily on Trump."
  },
  {
    "objectID": "posts/2016-09-30_topic-modeling-the-new-york-times-and-trump/index.html#topic-modeling",
    "href": "posts/2016-09-30_topic-modeling-the-new-york-times-and-trump/index.html#topic-modeling",
    "title": "Topic Modeling the New York Times and Trump",
    "section": "Topic Modeling",
    "text": "Topic Modeling\nTo extract the topics of articles, I first had to transform each article into a word vector. I did this using tf-idf, short for “term frequency-inverse document frequency.” Tf-idf is a statistic for each word in an article that increases with the number of times a word appears in that article while being offset by how frequently that word appears across the entire set of documents. Using tfidfvectorizer from scikit-learn to transform the corpus of documents, I created a matrix where each row is an article and each column is a word that has appeared in the corpus. Tf-idf is useful for when all your documents are about the same broad subject and likely to have a lot of words in common.\nThere are a few tricks to using tfidfvectorizer. The first is to remove a list of “stop words,” or common terms such as “it” and “they” that are not informative about the content of a document. There is a built-in list of stopwords in sklearn, but I added my own words to this list after I ran my first topic model and found a lot of irrelevant words (such as “mrs” and “nytimes”). The second is to set a minimum and maximum document argument. These exclude words that appear in more documents than your maximum number or that appear in fewer documents than your minimum. This avoids words that are so rare or so common that they are unlikely to be meaningful for understanding what a specific article is about.\nOnce I had my transformed articles, I could then use Non-negative Matrix Factorization (NMF) to extract the topics of articles. LDA is the more common method for topic modeling but can’t be used with tf-idf. When using NMF you have to pick a number of topics to extract. There’s no “right” answer for how many or a statistic you can use to help you choose. In my case, I iterated though multiple different numbers and settled on 30. This was the tipping point where all the topics were interpretable but any further topics were not."
  },
  {
    "objectID": "posts/2016-09-30_topic-modeling-the-new-york-times-and-trump/index.html#interpreting-topics",
    "href": "posts/2016-09-30_topic-modeling-the-new-york-times-and-trump/index.html#interpreting-topics",
    "title": "Topic Modeling the New York Times and Trump",
    "section": "Interpreting Topics",
    "text": "Interpreting Topics\nWhat do I mean by whether topics were interpretable? Well, topics don’t come with labels; rather, you have to interpret for yourself what each topic means. I did this by looking at the top 20 words for each topic. For example, what would you call this topic?\nTopic #16:\nkhan khizr khans mccain ghazala captain family muslim son humayun iraq sacrifice sen killed soldier army parents gold star capt\nSeeing words like “khan,” “son,” “soldier,” and “muslim,” this topic appears to be about Donald Trump’s confrontations with Khizr and Ghazala Khan, the parents of a Muslim American soldier killed in Iraq who criticized Donald Trump at the DNC. Therefore, I decided to call it the “Khan Family” topic.\nOne issue I ran into in naming topics was that there were multiple topics that I would consider to be about fundamentally the same thing. For example, look at these topics:\nTopic #1:\ncruz rubio ted kasich conservative texas marco primary south debate roe iowa establishment carolina wisconsin values saturday conservatives vote indiana\nTopic #7:\ndelegates kasich delegate win vote tuesday nomination winner ohio rules 237 majority michigan indiana district california rubio dakota districts winning\nTopic #17:\ncarson ben fiorina candidates debate carly percent muslim neurosurgeon choice huckabee christians religious conservative iowa retired faith jeb second survey\nThese all looked to me to be about the Republican primary. But if I reduced the number of topics from 30 to, say, 28 when I did my NMF, these “extra” republican primary topics didn’t simply merge into one. Instead, I lost other interpretable topics that I wanted to keep. Therefore, I decided to keep all 30 topics and simply call them “Republican Primary 1,” “Republican Primary 2,” etc."
  },
  {
    "objectID": "posts/2016-09-30_topic-modeling-the-new-york-times-and-trump/index.html#working-with-nmf",
    "href": "posts/2016-09-30_topic-modeling-the-new-york-times-and-trump/index.html#working-with-nmf",
    "title": "Topic Modeling the New York Times and Trump",
    "section": "Working with NMF",
    "text": "Working with NMF\nI now had a matrix where each row was an article, each column was a topic, and the value was how prevalent that topic was in that article. While each value is between 0 and 1, unlike LDA the columns in a row do not add up to 1, and so you can’t interpret a value as the “proportion” of a topic in a given article. Each document also can be, and usually is, associated with more than one topic. For example, an article about the economic policies of the republican primary contenders would probably be related to both the “economy” and “republican primary” topics. Because of that, it’s generally not appropriate to assign each document to a single topic (e.g. by picking the topic with the highest value).\nIn my case, I decided to consider an article was about a topic if the article-topic entry wasn’t zero. I thus transformed my matrix so that anything that wasn’t zero became 1. This certainly has some disadvantages, but it was appropriate for my interest in understanding when a topic was covered at all by the Times."
  },
  {
    "objectID": "posts/2016-09-30_topic-modeling-the-new-york-times-and-trump/index.html#visualizing-coverage-over-time",
    "href": "posts/2016-09-30_topic-modeling-the-new-york-times-and-trump/index.html#visualizing-coverage-over-time",
    "title": "Topic Modeling the New York Times and Trump",
    "section": "Visualizing Coverage over Time",
    "text": "Visualizing Coverage over Time\nAfter my first attempt to graph all of the topics on the same time-series plot looked like something out of a nightmare, I realized I needed to limit my graphs to only a few topics at a time. One way to do this was to group topics into a few categories and create a plot for each one. Here’s one for all the topics I considered to be policy issues.\n\n\n\ncenter\n\n\nYou can see that foreign policy has been the one generally covered the most (possibly because of Trump’s frequent mentions of Putin), and police had a big spike in July when the Dallas shooting happened."
  },
  {
    "objectID": "posts/2016-09-30_topic-modeling-the-new-york-times-and-trump/index.html#pitfalls-of-topic-modeling",
    "href": "posts/2016-09-30_topic-modeling-the-new-york-times-and-trump/index.html#pitfalls-of-topic-modeling",
    "title": "Topic Modeling the New York Times and Trump",
    "section": "Pitfalls of Topic Modeling",
    "text": "Pitfalls of Topic Modeling\nTake a look at the following graph. Notice anything strange about it?\n\n\n\ncenter\n\n\nTake a look at the Khan family topic in May and June 2016. The points are non-zero, meaning there were articles about that topic in those months. But this was before the incident even happened!\nI took a closer look at these articles and found they were about Donald Trump and Sadiq Khan, London’s newly elected Muslim mayor. The problem was that while certainly all top words in the topic, such as “sacrifice” or “star,” weren’t in these articles, a few of the most unique ones (khan, muslim) were. In fact, if I was to assign a topic to this article, that topic would still be the best fit.\nNormally it’s not “impossible” for a topic to be present in a document and so you can’t see these types of limitations. But this is a good illustration of the limitations of interpreting a topic."
  },
  {
    "objectID": "posts/2016-09-30_topic-modeling-the-new-york-times-and-trump/index.html#conclusions",
    "href": "posts/2016-09-30_topic-modeling-the-new-york-times-and-trump/index.html#conclusions",
    "title": "Topic Modeling the New York Times and Trump",
    "section": "Conclusions",
    "text": "Conclusions\nBecause this was an exploratory analysis, I didn’t start out with a specific hypothesis I wanted to test. For example, I could have instead used sentiment analysis to test if Times’ articles about Trump used more negative words than those about Clinton. I also don’t have a final number at the end that tells me how “good” my analysis is, as I did in one of my other projects where I evaluated the accuracy of my predictions using AUC.\nInstead, I’m only able to conclude with some general impressions I gained from the topic modeling. The first is that none of the topics were particularly surprising; you can see the full list in my jupyter notebook. The most commonly covered topics were about the Republican primary and polls (“percent”, “poll”, “points”, “survey”). I did find it interesting that two controversial incidents, the Trump University lawsuit and the Khan family conflict, were prominent enough to get their own topics."
  },
  {
    "objectID": "posts/2016-09-30_topic-modeling-the-new-york-times-and-trump/index.html#next-time",
    "href": "posts/2016-09-30_topic-modeling-the-new-york-times-and-trump/index.html#next-time",
    "title": "Topic Modeling the New York Times and Trump",
    "section": "Next Time",
    "text": "Next Time\nOne of the benefits of attending Metis is that the last three weeks of Metis are devoted to working on a project entirely of your own choosing. You can check out some of the projects here; they range from an application that uses computer vision to read wine labels to predicting the length of an NFL runningback’s career. While our previous projects are presented just to the other Metis students and the instructors, we present our final projects on a “Career Day” to about 25 employers.\nFor my final project, I created an application that helps data science freelancers find the freelance jobs that best fit their preferences and skills. You can check out the application here. All of my code for the project and the application is on github. My next blog post will document how my project evolved and some of the exploratory work I did before creating the application."
  },
  {
    "objectID": "posts/2018-01-10_building-your-data-science-network-reaching-out/index.html",
    "href": "posts/2018-01-10_building-your-data-science-network-reaching-out/index.html",
    "title": "Building your Data Science Network: Reaching Out",
    "section": "",
    "text": "In the other part of this post, I covered how to start becoming involved in the data science community and meet people in general. But what if you read a really cool post by someone and want to follow up with them? This post offers some thoughts on how you can most effectively reach out to specific people.\nTwo important caveats to start, both inspired by other posts on similar topics. First, to quote Trey Causey: “I am not without sin, and I’m also still figuring all this out.” Second, for those of you who hate the very idea of networking and think your work should speak for yourself, I really like Rachel Thomas’ take on this. She’s writing about personal branding, but I think there’s a lot of overlap there (as I discuss later):"
  },
  {
    "objectID": "posts/2018-01-10_building-your-data-science-network-reaching-out/index.html#conclusion",
    "href": "posts/2018-01-10_building-your-data-science-network-reaching-out/index.html#conclusion",
    "title": "Building your Data Science Network: Reaching Out",
    "section": "Conclusion",
    "text": "Conclusion\nRemember that others want to help; just look at how many have signed up to help answer questions and mentor data science newcomers at data helpers (this is a great resource for finding someone to reach out to). And the person who’s most helpful to you is probably not the most famous data scientist. You’ll likely learn more by talking with peers facing the same issues, teaching beginners to force a deeper understanding of core topics, or asking someone who recently accomplished your goal (e.g. giving your first talk).\nThe main way I try to help others in the community is through these posts, and I’ll be back to tackling the challenges of A/B Testing soon.\nIf you found this post useful, you might be interested in the book on data science careers I’m writing with Jacqueline Nolis, “Build Your Career in Data Science,” to be published by Manning in March 2020. The entire book is available online if you pre-order, and you can get 40% off with the code buildbook40%."
  },
  {
    "objectID": "posts/2019-01-09_going-off-the-map-exploring-purrrs-other-functions/index.html",
    "href": "posts/2019-01-09_going-off-the-map-exploring-purrrs-other-functions/index.html",
    "title": "Going Off the Map: Exploring purrr’s Other Functions",
    "section": "",
    "text": "I recently completed Colin Fay’s excellent DataCamp course, Intermediate Functional Programming with purrr (full disclosure: I work at DataCamp, but part of why I joined was that I was a big fan of the short, interactive course format). Although I’ve used the purrr package before, there were a lot of functions in this course that were new to me. I wrote this post to hopefully demystify purrr a bit for those who find it overwhelming and illustrate some of its lesser known functions. Most of these functions are covered in Colin’s course, though I added a few I found on the purrr cheatsheet."
  },
  {
    "objectID": "posts/2019-01-09_going-off-the-map-exploring-purrrs-other-functions/index.html#introduction",
    "href": "posts/2019-01-09_going-off-the-map-exploring-purrrs-other-functions/index.html#introduction",
    "title": "Going Off the Map: Exploring purrr’s Other Functions",
    "section": "Introduction",
    "text": "Introduction\npurrr is a package for functional programming in R. If you’re familiar with it, it’s probably because of the map()* functions. And if you’ve been a little bit intimidated by them, I’m right there with you. You’ll often see purrr used with nested lists or dataframes, like in this (modified) example from one of the last lessons in Jenny Bryan’s purrr tutorial:\n\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(gapminder)\nlibrary(tidyr)\n\n\ngapminder %>%\n  group_by(country) %>%\n  nest() %>%  \n  mutate(fit = map(data, ~ lm(lifeExp ~ year, data = .x))) %>%\n  mutate(rsq = map_dbl(fit, ~ summary(.x)[[\"r.squared\"]])) %>%\n  arrange(rsq)\n\n# A tibble: 142 × 4\n# Groups:   country [142]\n   country          data              fit       rsq\n   <fct>            <list>            <list>  <dbl>\n 1 Rwanda           <tibble [12 × 5]> <lm>   0.0172\n 2 Botswana         <tibble [12 × 5]> <lm>   0.0340\n 3 Zimbabwe         <tibble [12 × 5]> <lm>   0.0562\n 4 Zambia           <tibble [12 × 5]> <lm>   0.0598\n 5 Swaziland        <tibble [12 × 5]> <lm>   0.0682\n 6 Lesotho          <tibble [12 × 5]> <lm>   0.0849\n 7 Cote d'Ivoire    <tibble [12 × 5]> <lm>   0.283 \n 8 South Africa     <tibble [12 × 5]> <lm>   0.312 \n 9 Uganda           <tibble [12 × 5]> <lm>   0.342 \n10 Congo, Dem. Rep. <tibble [12 × 5]> <lm>   0.348 \n# … with 132 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nIf you’ve generally worked with plain-old table data or vectors (like I have), you might have this reaction to that code:\n\nBut I am here to tell you: purrr can make your life easier even if you never write code like this. Certainly, knowing how to work with complicated nested lists and dataframes is very useful - it can simplify code you’ve written, and your data may arrive in that format (for example, JSON data is often represented as nested lists or dataframes in R). But even if all you ever work with is “simple” lists, dataframes, and vectors, you’ll be glad to know a bit of purrr."
  },
  {
    "objectID": "posts/2019-01-09_going-off-the-map-exploring-purrrs-other-functions/index.html#a-brief-introduction-to-map",
    "href": "posts/2019-01-09_going-off-the-map-exploring-purrrs-other-functions/index.html#a-brief-introduction-to-map",
    "title": "Going Off the Map: Exploring purrr’s Other Functions",
    "section": "A brief introduction to map()",
    "text": "A brief introduction to map()\nmap() lets you take a list or vector and apply a function to each element. If you’ve been using R for a while, you might be familiar with the apply functions, including sapply() and lapply(). map() does essentially the same thing, but offers several advantages, most importantly consistency of output and helpers that let you write more concise code (see the first Stack Overflow answer by Hadley Wickham here).\nLet’s look at an example, where we take a list of four numbers and round each of them.\n\nmy_vector <- c(1.0212, 2.483, 3.189, 4.5938)\n\nmap(my_vector, round)\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 2\n\n[[3]]\n[1] 3\n\n[[4]]\n[1] 5\n\n\nThe result is a list (hence the double brackets), which is always the case with map(). But you can return a dataframe or different types of vectors instead by using the appropriate map_*() function. In this case, let’s return a vector of type double1:\n\nmap_dbl(my_vector, round)\n\n[1] 1 2 3 5\n\n\nYou can also use map with an “anonymous” function, a function that doesn’t have a name. The formula is a ~ followed by what you want to do to each element, with .x representing the element (. also works). We can make an anonymous function to add ten to each element of our vector:\n\nmap_dbl(my_vector, ~ .x + 10)\n\n[1] 11.0212 12.4830 13.1890 14.5938\n\n\nmap() can get much (much) more complicated, with nested lists and multiple inputs and arguments, but even knowing this basic use case can help you! If you do want to dive in more, check out chapter 21 of R for Data Science, Jenny Bryan’s purrr tutorials, Auriel Fournier’s Foundations of Functional Programming with purrr course, and chapters 3 and 4 of Writing Functions in R by Charlotte and Hadley Wickham on DataCamp."
  },
  {
    "objectID": "posts/2019-01-09_going-off-the-map-exploring-purrrs-other-functions/index.html#beyond-map",
    "href": "posts/2019-01-09_going-off-the-map-exploring-purrrs-other-functions/index.html#beyond-map",
    "title": "Going Off the Map: Exploring purrr’s Other Functions",
    "section": "Beyond map()",
    "text": "Beyond map()\nWhile map*() is great, it can still take a while to wrap your head around. But purrr offers dozens of useful functions that you can start using right away to streamline your workflow, even if you don’t use map(). Let’s check out a few. I’ll separate them into two types: those that create new functions and those that modify a list/vector.\n\nModifying and summarizing vectors/lists\n\nkeep() and discard()\nkeep() and discard() … keep and discard elements of a list or vector based on a predicate function. A predicate function is a function that returns TRUE or FALSE. So is.factor() is a predicate function, because it always returns TRUE or FALSE, while round() is not.\nFor example, we can keep() all elements of our list that are less than 3 with the following code:\n\nkeep(my_vector, ~ .x < 3)\n\n[1] 1.0212 2.4830\n\n\nSimilarly, we could discard() all elements less than 3:\n\ndiscard(my_vector, ~ .x < 3)\n\n[1] 3.1890 4.5938\n\n\n\n\nmap_if()\nWhat if you’re not sure about the types of every element in your list, and you want to apply a function that needs the input to be of a certain type? For example, let’s say we wanted to add 10 to every element of a list.\n\nmixed_list <- list(\"happy\", 2L, 4.39)\n\nadd_ten <- function(n) {\n  n + 10\n}\n\n\nmap(mixed_list, add_ten)\n\nError in n + 10: non-numeric argument to binary operator\n\n\nWe get an error since we’re trying to add 10 to “happy”, which isn’t numeric!\nThis is where map_if() comes in handy. Just like mutate_if(), select_if(), and summarise_if(), you add a condition and the function will only apply to those columns (or list elements) where the condition is met. Here, we know that condition is that the element needs to be numeric. Let’s try again with map_if():\n\nmap_if(mixed_list, is.numeric, add_ten)\n\n[[1]]\n[1] \"happy\"\n\n[[2]]\n[1] 12\n\n[[3]]\n[1] 14.39\n\n\nAlright! We see it skipped over “happy”, preserving it as is, and added ten to the two numeric elements.\n\n\nevery() and some()\nSometimes you have a giant list and want to know whether each element meets a condition, like being numeric. You can use every(), which will check if every element of a list satisfies a predicate function:\n\nevery(mixed_list, is.numeric)\n\n[1] FALSE\n\n\nSince “happy” is the first element of mixed_list, it doesn’t past the test.\nIf we want to be less strict and just check if some of the elements satisfies a predicate function, we can use some() instead:\n\nsome(mixed_list, is.numeric)\n\n[1] TRUE\n\n\nIn this case, since some elements of mixed_list were numeric, we got TRUE.\n\n\n\nModifying functions\npurrr includes adverbs - functions that take a function and return a modified version (just as an adverb modifies a verb). Let’s check out a few!\n\nnegate()\nnegate() … negates a predicate function (aren’t the purrr function names great?). For example, let’s say you want to check which elements of a list were not null. This is how you would do it with map_lgl (which returns a logical vector rather than a list):\n\nlst <- list(\"a\", 3, 22, NULL, \"q\", NULL)\n\n\nmap_lgl(lst, ~ !is.null(.))\n\n[1]  TRUE  TRUE  TRUE FALSE  TRUE FALSE\n\n\nThis works, but it’s not super easy to read. Instead, we can make an is_not_null() function using negate():\n\nis_not_null <- negate(is.null)\n\nmap_lgl(lst, is_not_null)\n\n[1]  TRUE  TRUE  TRUE FALSE  TRUE FALSE\n\n\nVoila!\n\n\npartial()\nYou probably have a couple functions where you almost always use an extra argument, like mean() with na.rm = TRUE or round() with digits = 1. You can use partial() to create a new function where those are always specified, saving you some repetitive typing!\n\nmean(c(10, NA, 5, 7))\n\n[1] NA\n\n\n\nmean(c(10, NA, 5, 7), na.rm = TRUE)\n\n[1] 7.333333\n\n\n\nmy_mean <- partial(mean, na.rm = TRUE)\n\nmy_mean(c(10, NA, 5, 7))\n\n[1] 7.333333\n\n\n\nmy_round <- partial(round, digits = 1)\n\nmy_round(10.484)\n\n[1] 10.5\n\n\n\n\nsafely() and possibly()\nWe saw earlier that map_if() could be used where we have a condition we want to be met before applying a function. In our case, we used it to avoid an error, but you could also use it to meet a condition like the number being negative or greater than a threshold. But what if you don’t know where errors could come from but want to handle them? This is where safely() and possibly() come in.\nIf you’re not interested in what the error is, you should use possibly(). In addition to the function it’s wrapping around, you need to specify the argument otherwise, which is what you want to return if there is an error. Let’s take a look:\n\npossibly_add_ten <- possibly(add_ten, otherwise = \"I'm not numeric!\")\n\nmap(mixed_list, possibly_add_ten)\n\n[[1]]\n[1] \"I'm not numeric!\"\n\n[[2]]\n[1] 12\n\n[[3]]\n[1] 14.39\n\n\nSide-note - I find the the double-brackets confusing. We’ve got them since our list isn’t named. Let’s set the name of each element with purrr's set_names(), giving our elements the very creative names a, b, and c.\n\nmixed_list <- set_names(mixed_list, c(\"a\", \"b\", \"c\"))\n\nmixed_list\n\n$a\n[1] \"happy\"\n\n$b\n[1] 2\n\n$c\n[1] 4.39\n\n\nWell … mildly easier to read at least.\nOn the other hand, sometimes you do want to know what the error is. If that’s the case, you can use safely() instead:\n\nmap(mixed_list, safely(add_ten))\n\n$a\n$a$result\nNULL\n\n$a$error\n<simpleError in n + 10: non-numeric argument to binary operator>\n\n\n$b\n$b$result\n[1] 12\n\n$b$error\nNULL\n\n\n$c\n$c$result\n[1] 14.39\n\n$c$error\nNULL\n\n\nsafely() returns a list of lists. Each element from the original list has two entries: result and error. One is always NULL - if there was an error, result is NULL and error is the error message; if there wasn’t an error, the result is the result and error is NULL. If you want to get back all the errors or results, you can use another handy feature of map(). If you give map() a string as the second argument, for each element of the list, it will return the element inside of it with that name.\n\nsafely_add_ten <- safely(add_ten)\n\nmixed_list %>%\n  map(safely_add_ten) %>%\n  map(\"error\")\n\n$a\n<simpleError in n + 10: non-numeric argument to binary operator>\n\n$b\nNULL\n\n$c\nNULL\n\n\nAh, no more list of lists. That was scary. I promise I won’t do it again.\n\n\ncompact()\nIn the previous example, we probably wouldn’t be interested in the instances where errors were NULL. We could use discard(is.null), but purrr actually provides a function just for the purpose of getting rid of NULLs: compact().\n\nmixed_list %>%\n  map(safely_add_ten) %>%\n  map(\"error\") %>%\n  compact()\n\n$a\n<simpleError in n + 10: non-numeric argument to binary operator>\n\n\n\n\ncompose()\ncompose() lets you string together multiple functions. Let’s say you want to add_ten(), take the log, and then round a vector of numbers. You could do either of these:\n\nc(1, 20, 500) %>%\n  add_ten() %>%\n  log() %>%\n  round()\n\n[1] 2 3 6\n\n\n\nround(log(add_ten(c(1, 20, 500))))\n\n[1] 2 3 6\n\n\nBut you could also make a new function using compose(). You give compose() functions to execute in order from right to left (just like we have written above):\n\nadd_ten_log_and_round <- compose(round, log, add_ten)\n\nadd_ten_log_and_round(c(1, 20, 500))\n\n[1] 2 3 6\n\n\ncompose() is great for simplifying your code if you’re going to use a sequence of functions again and again.\nWhat if we wanted to round to the nearest tenth instead? You can combine compose() with partial()!\n\nround_tenth <- partial(round, digits = 1)\n\nadd_ten_log_and_round_tenth <- compose(round_tenth, log, add_ten)\n\nadd_ten_log_and_round_tenth(c(1, 20, 500))\n\n[1] 2.4 3.4 6.2"
  },
  {
    "objectID": "posts/2019-01-09_going-off-the-map-exploring-purrrs-other-functions/index.html#conclusion",
    "href": "posts/2019-01-09_going-off-the-map-exploring-purrrs-other-functions/index.html#conclusion",
    "title": "Going Off the Map: Exploring purrr’s Other Functions",
    "section": "Conclusion",
    "text": "Conclusion\nThere’s so much to learn in the wonderful world of purrr! I’m a big believer that just knowing something exists gets you a lot of the way there. Maybe you can’t think of a use for partial() right now, but you’ll run into a problem a few months down the line where it would be handy. You don’t need to remember the exact syntax; all you need to know is what to Google for and that you don’t need to spend hours trying to find some other solution or resign yourself to writing repetitive code.\nOne thing I didn’t show in this post was how all of these functions can fit together in an analysis. For that, I’ll recommend Colin’s course again. He also has a series of six blog posts illustrating different purrr functions, including some I didn’t cover here; check them out!"
  },
  {
    "objectID": "posts/2017-01-19_rstudio-conference-tips-and-tricks/index.html",
    "href": "posts/2017-01-19_rstudio-conference-tips-and-tricks/index.html",
    "title": "RStudio Conference: Tips and Tricks",
    "section": "",
    "text": "This is the second part of my posts on the rstudio::conf. If you’re interested in more general thoughts on the conference and some personal notes, check out my other post. This post is to gather, as succintly and organized as possible, the practical and technical things I learned at the conference. While I did a whole training day on writing R Packages, I haven’t included most of what I’ve learned here. Instead, I’ll be integrating it into a future post on writing my first R package."
  },
  {
    "objectID": "posts/2017-01-19_rstudio-conference-tips-and-tricks/index.html#useful-packages-tools-and-functions",
    "href": "posts/2017-01-19_rstudio-conference-tips-and-tricks/index.html#useful-packages-tools-and-functions",
    "title": "RStudio Conference: Tips and Tricks",
    "section": "Useful Packages, Tools, and Functions",
    "text": "Useful Packages, Tools, and Functions\n\nAssertr’s verify function: The verify function is meant for use in a data analysis pipeline. It raises an error if the logical within the function is false and just returns the data if True. This is a great way to add some assumption checks in your data pipelines. For example, mtcars %>% verify(nrow(.) == 32) %>% filter(cyl == 6) simply returns the data frame of all cars with 6 cylinders, as expected, because mtcars does indeed have 32 rows. If, however, we had put the wrong number of rows in (e.g. verify(nrow(.) == 24)), we would have gotten no data, with this error instead: Error in verify(., nrow(.) == 24) : verification failed! (1 failure).\nProfvis: This is a tool to visualize how long each part of your code is taking to run. This is a great way to figure out how to speed up your code, as often your intuition of what is taking the most time does not match reality. To use it, all you have to do is wrap your code in the profvis function, like so: profvis({ my_function }). A new pane will then pop up in RStudio that shows how long each line takes to run and even what functions each calls. Learn more on the RStudio Profvis page.\nAbbreviating arguments: You can abbreviate arguments within functions. So instead of writing mean(c(10, 5, NA), na.rm = TRUE), you can simply right mean(c(10, 5, NA), n = T). CAUTION: this can go wrong, especially in the case of using T and F as abbreviations for TRUE and FALSE. For example, if for some reason you (or an evil coworker) had put T <- F before you run this code, now your output will be NA!\nListviewer: One of the hardest parts about working with nested lists is trying to figure out what the heck is in them. The jsonedit function from listviewer allows you to see the layers of a list and even search through them. Here’s what it looks like when I run it with got_chars, a list from Jenny Bryan’s great repurrrsive package:\n\n\n\n\nImage of Listviewer"
  },
  {
    "objectID": "posts/2017-01-19_rstudio-conference-tips-and-tricks/index.html#writing-functions",
    "href": "posts/2017-01-19_rstudio-conference-tips-and-tricks/index.html#writing-functions",
    "title": "RStudio Conference: Tips and Tricks",
    "section": "Writing Functions",
    "text": "Writing Functions\n\nWhen writing a function, the last thing you should do is start writing a function. You should always start by figuring out how to solve problems with specific x and y and then generalize.\nIt’s better to do little steps and check after each one. That way you don’t go a long way, realize you did something wrong, and have to backtrack hours of work.\n\nIt’s time to write a function when you copy and paste three times. Copying and pasting too much is bad because it increases the possibility for error and clutters up your code.\nA function is “pure” if its output only depends on its input and it makes no changes to state of the world (such as resetting options).\nTry to make functions “purer” by having it do one thing. You can put the other things in a different function.\nProperties of a good function\n\nIt does one thing (avoids side effects)\nThe output is consistent, meaning it always returns the same type of object (e.g. dataframe, character vector, etc.)\nIt fails fast\nIt has a good name: not too long and makes it clear what the function does\nIt works\n\nYour function should be understandable. This is about being correct in the future. Often what you want to do is going to change over time, and if you can’t understand how a function works, your chances of making a change correctly is smaller. If you try to make your function too clever, you’ll probably end up like this\n\n\n\n.@hadleywickham on clever programming solutionsNow: \"My god I am a total coding genius” 3 months later: \"I have no idea what this means\"\n\n— Emily Robinson (@robinson_es) January 11, 2017\n\n\n\nWriting good error messages is really hard, because it relies on you having good mental model of someone else’s flawed mental model of your function.\nOnly ever use return for special cases. If a function can return early, it should use an explicit return call. Otherwise return just adds verbosity for no real reason. You want to save return as a call out to mean this is special case.\nDon’t write functions that both compute something and then do something with it. For example, the summary function for a linear model both computes and prints the p-value"
  },
  {
    "objectID": "posts/2017-01-19_rstudio-conference-tips-and-tricks/index.html#keyboard-shortcuts",
    "href": "posts/2017-01-19_rstudio-conference-tips-and-tricks/index.html#keyboard-shortcuts",
    "title": "RStudio Conference: Tips and Tricks",
    "section": "Keyboard Shortcuts",
    "text": "Keyboard Shortcuts\n\nHit tab after you start typing to get all functions that start with those letters. Cmd/Ctrl + up arrow instead gives you the commands you’ve typed\nHold alt and drag your cursor to type on multiple lines\nAlt + shift + k for all keyboard shortcuts\nAnd you can make your own keyboard shortcuts"
  },
  {
    "objectID": "posts/2017-01-19_rstudio-conference-tips-and-tricks/index.html#general-tips-and-tricks",
    "href": "posts/2017-01-19_rstudio-conference-tips-and-tricks/index.html#general-tips-and-tricks",
    "title": "RStudio Conference: Tips and Tricks",
    "section": "General Tips and Tricks",
    "text": "General Tips and Tricks\n\nReading rcode broadly is useful, as it can help expand your R vocabulary.\nHadley sets these options in R, to avoid the pitfalls of R’s “helpful” partial name matching\n\noptions(warnPartialMatchArgs = TRUE, warnPartialMatchDollar = TRUE, warnPartialMatchAttr = TRUE)\n\nMost of the time the bottleneck is thinking speed, not computational speed\nDon’t proactively worry about performance of your code, but about whether it’s clear.\nDon’t try to read your code and think whether it will be fast or slow. Your intuition is terrible. Just run it! You can also use profvis to help.\nIt’s very easy to make code faster by making it incorrect. One of the reasons to write tests!\nRestart R a few times a day and never restore or save your .RData. This will help the reproducibility of your code and also if your coworkers do something like redefine + (yes, you can do that in R).\nDon’t use comments to say what/how your code is doing, use it to describe why. Otherwise, you have to remember to change comments when you change your code. You really don’t want to end up with your code doing one thing and your comment saying you’re doing something else.\nYou can be too verbose in your code because don’t have enough r vocabulary. For example:\n\nif(x == TRUE) is the same as if(x)\ny == \"a\" | y == \"b\" | y == \"c\" is the same as y %in% c(\"a\", \"b\", “c”)\n\nBob Rudis’ five rules for using pipes. P.S. This was probably the most elaborate and fun slideshow. Definitely check it out!\n\n\n\n\nImage of pipes"
  },
  {
    "objectID": "posts/2017-01-19_rstudio-conference-tips-and-tricks/index.html#other-conference-write-ups",
    "href": "posts/2017-01-19_rstudio-conference-tips-and-tricks/index.html#other-conference-write-ups",
    "title": "RStudio Conference: Tips and Tricks",
    "section": "Other Conference Write-Ups",
    "text": "Other Conference Write-Ups\n\nSharon Machlis’s tips and takeaways, which is in a bullet-point and short paragraph format. This is a great complement to this list, since hers are from the conference talks and most of mine are from the training days with Hadley\nStephen Turner’s recap, with nice short summaries of some of the talks\nSimon Jackson’s takeaways from the conference, organized around the opinions and challenges about data science processes in the R community. This organizational schema was inspired by Hilary Parker’s great talk on Opinionated Analysis Development"
  },
  {
    "objectID": "speaking.html",
    "href": "speaking.html",
    "title": "Speaking",
    "section": "",
    "text": "I regularly give talks on A/B Testing, programming in R, data science career advice, and more. I’ve given talks across the country and internationally, to audiences ranging from 25 people to 800. If you’re interested in having me speak at your event, please contact me at robinson.es[at]gmail.com\nMy co-author Jacqueline Nolis and I also have our own podcast based on our book of the same title, Build a Career in Data Science. Each episode covers one chapter of the book, sharing the main takeaways as well as our personal experiences. The podcast is available on all major podcast platforms, and you can visit the website here."
  },
  {
    "objectID": "speaking.html#ab-testing",
    "href": "speaking.html#ab-testing",
    "title": "Speaking",
    "section": "A/B Testing",
    "text": "A/B Testing\n\n6 Guidelines for A/B Testing\nMarch 2019, CXL Live. Slides, Recording - you will need to sign-up here for free.\n\n\nBuilding an A/B Testing Analytics System\nJanuary 2019, RStudio Conference. Slides, Recording.\nNovember 2018, DC R Conference. Recording.\nOctober 2018, Nor’eastR Conference.\n\n\n10 Guidelines for A/B Testing\nJune 2019, Keynote at Microsoft internal One Analyst conference in Seattle.\nNovember 2018, DataCamp Webinar. Recording, Slides.\nOctober 2018, PyData NYC.\nOctober 2018, Taiwanese Data Professionals.\nFebruary 2018, Data Science and Online Experiments at Etsy on the DataFramed podcast. Recording.\n\n\nA/B Testing in the Wild\nOctober 2017, West Point Academy. Slides.\nSeptember 2017, Demystifying Data Science Conference by Metis. Recording, Slides.\nJuly 2017, New York Open Statistical Programming Meetup. Recording, Slides."
  },
  {
    "objectID": "speaking.html#programming-in-r",
    "href": "speaking.html#programming-in-r",
    "title": "Speaking",
    "section": "Programming in R",
    "text": "Programming in R\n\nA UseR’s Introduction to Machine Learning in AWS\nOctober 2021, Salt Lake City R Users Group. Slides, Recording.\n\n\nfunneljoin: Defining a Tidy Grammar of Funnels in R\nAugust 2020, NY R conference.Recording.\nNovember 2019, Booking.com Headquarters.\nNovember 2019, DC R conference. Slides, Recording.\n\n\nEverything You Wanted to Know About Making R Packages but Were Afraid to Ask\nNovember 2019, Columbia University.\nMay 2019, New York R Conference. Recording, Slides.\n\n\nThe Lesser Known Stars of the Tidyverse\nDecember 2018, DataCamp Webinar. Recording.\nApril 2018, New York R Conference. Recording, Slides.\nApril 2018, Women in Analytics Conference, Facebook. Slides, Code.\nFebruary 2018, RStudio Conference. Recording, Slides.\nJanuary 2018, Data Day Texas Conference. Slides, Code."
  },
  {
    "objectID": "speaking.html#data-science-career-advice",
    "href": "speaking.html#data-science-career-advice",
    "title": "Speaking",
    "section": "Data Science Career Advice",
    "text": "Data Science Career Advice\n\nBuild a Career in Data Science\nAugust 2020, Grace Hoppers Celebration Scholars meetup.\nJune 2020, Data Umbrella, Recording (including 45 minutes of Q&A).\nJune 2020, DataScienceGO Virtual, Recording, Slides.\nNovember 2019, R-Ladies DC and DC Emerging Technologies, Slides.\n\n\nData Science Demstifyed\nNovember 2019, Conversion Hotel in the Netherlands. Slides, 5-minute video summary.\n\n\nJoining the Data Science Community\nJuly 2019, Demystifying Data Science, Slides, Recording.\nMay 2018, July 2018, and April 2020, Metis Data Science Bootcamp. Slides.\n\n\nCreating a Strong Data Science Portfolio\nJune 2019, Data Matters in London. Recording, Slides."
  },
  {
    "objectID": "speaking.html#podcasts",
    "href": "speaking.html#podcasts",
    "title": "Speaking",
    "section": "Podcasts",
    "text": "Podcasts\nNovember 2020, Building a Career in Data Science on Chai Data Science. Recording.\nOctober 2020, Build a Career in Data Science on The Artists of Data Science . Recording.\nJuly 2020, How to Jump into Data Science from Any Background on Develomentor. Recording.\nJune 2020, Build a Career in Data Science Book Session with Women in Analytics and Women in Tech International. Recording.\nApril 2020, Build a Career in Data Science on Talk Python to Me. Recording.\nApril 2020, Tackling Data Science Job Hunting, Interviews & Negotiations on Super Data Science Podcast. Recording and transcript.\nApril 2020, Data Science and Sponsorship on Greater Than Code. Recording and episode outline.\nApril 2020, Building a Career in Data Science on Towards Data Science. Recording and summary.\nMarch 2020, Start a Career in Data Science on Women in TECH with Ariana. Recording.\nMarch 2020, Building a Career in Data Science on Practical AI. Recording and transcript.\nFebruary 2018, Data Science and Online Experiments at Etsy on DataFramed. Recording."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hooked on Data",
    "section": "",
    "text": "Answers to your Career Questions\n\n\n\nCareer\n\n\n\nMy favorite resources for a variety of career dilemmas.\n\n\n\nEmily Robinson\n\n\nJun 22, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPublishing a Technical Book (Part 4): After the Book is Published\n\n\n\nCareer\n\n\nPublishing\n\n\n\nWhat it’s like after your book is finished and available in the wild.\n\n\n\nEmily Robinson\n\n\nApr 6, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPublishing a Technical Book (Part 3): The Writing Process\n\n\n\nCareer\n\n\nPublishing\n\n\n\nAn inside look into how to find the time to write a book and what it’s like to work with a co-author and publisher.\n\n\n\nEmily Robinson\n\n\nApr 5, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPublishing a Technical Book (Part 2): Finding a Publisher\n\n\n\nCareer\n\n\nPublishing\n\n\n\nDemystifying how to find a technical book publisher, what they help with, and if you should think about self-publishing.\n\n\n\nEmily Robinson, Emily Robinson\n\n\nApr 4, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPublishing a Technical Book (Part 1): Why Do It?\n\n\n\nCareer\n\n\nPublishing\n\n\n\nWhat are (and aren’t) the benefits of writing a technical book?\n\n\n\nEmily Robinson\n\n\nApr 3, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat Types Should You Have on Your Pokémon Team? Efficient Simulation with Matrices in R\n\n\n\nR\n\n\nCode\n\n\n\nFinding the best Pokémon type combinations for a maximally effective team.\n\n\n\nEmily Robinson\n\n\nAug 20, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Bob Ross paintings\n\n\n\nR\n\n\nCode\n\n\nVisualization\n\n\n\nUsing the tidyverse and PCA to explore the features of Bob Ross’ paintings.\n\n\n\nEmily Robinson\n\n\nAug 14, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing the funneljoin package\n\n\n\nR\n\n\nCode\n\n\n\nHow our new R package funneljoin makes it easy to analyze sequence of events.\n\n\n\nEmily Robinson\n\n\nAug 1, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGoing Off the Map: Exploring purrr’s Other Functions\n\n\n\nR\n\n\nCode\n\n\n\nA walkthrough of some lesser-known purrr functions.\n\n\n\nEmily Robinson\n\n\nJan 9, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Lesser Known Stars of the Tidyverse\n\n\n\nR\n\n\nCode\n\n\n\nUsing exploratory data analysis to shine a light on other tidyverse functions and packages you may not know.\n\n\n\nEmily Robinson\n\n\nNov 16, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGuidelines for A/B Testing\n\n\n\nA/B Testing\n\n\n\n12 Guidelines to help you run more effective, trustworthy A/B tests.\n\n\n\nEmily Robinson\n\n\nAug 7, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRed Flags in Data Science Interviews\n\n\n\nCareer\n\n\n\nWhat to look out for when interviewing for a new data science job.\n\n\n\nEmily Robinson, Emily Robinson, Jacqueline Nolis\n\n\nJul 3, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdvice for Applying to Data Science Jobs\n\n\n\nCareer\n\n\n\nSome tips for applying, interviewing, and negotiating for data science jobs.\n\n\n\nEmily Robinson\n\n\nMay 29, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Importance of Sponsorship\n\n\n\nCareer\n\n\n\nWhile many people know about mentorship, many fewer are familiar with sponsorship, even though it can actually be more important.\n\n\n\nEmily Robinson\n\n\nFeb 14, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding Your Data Science Network: Finding Community\n\n\n\nCareer\n\n\n\nWhere to find other people in your field.\n\n\n\nEmily Robinson\n\n\nJan 10, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding your Data Science Network: Reaching Out\n\n\n\nCareer\n\n\n\nHow to effectively reach out to people in your field directly.\n\n\n\nEmily Robinson\n\n\nJan 10, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMaking R Code Faster: A Case Study\n\n\n\nCode\n\n\nR\n\n\n\nHow I sped up my R code and went from it taking 3 minutes to run one iteration to taking less than a second to run 10,000.\n\n\n\nEmily Robinson\n\n\nNov 30, 2017\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nManaging Business Challenges in Data Science\n\n\n\nCareer\n\n\n\nA few tips to help you work more effectively with your non-analyst stakeholders.\n\n\n\nEmily Robinson\n\n\nSep 28, 2017\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGiving Your First Data Science Talk\n\n\n\nCareer\n\n\n\nThe why and how of giving a good data science talk.\n\n\n\nEmily Robinson, Emily Robinson\n\n\nAug 14, 2017\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRStudio Conference Recap\n\n\n\nConference\n\n\nR\n\n\n\nSome of my favorite parts about attending my first rstudio::conf.\n\n\n\nEmily Robinson\n\n\nJan 19, 2017\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRStudio Conference: Tips and Tricks\n\n\n\nConference\n\n\nR\n\n\nCode\n\n\n\nA summary of my top technical takeaways from rstudio::conf(2017).\n\n\n\nEmily Robinson\n\n\nJan 19, 2017\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTopic Modeling the New York Times and Trump\n\n\n\nPython\n\n\nVisualization\n\n\nText Analysis\n\n\nCode\n\n\n\nHow the New York Times covered Trump leading up to the 2016 election.\n\n\n\nEmily Robinson\n\n\nSep 30, 2016\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBetter Plotting in Python with Seaborn\n\n\n\nPython\n\n\nVisualization\n\n\nCode\n\n\n\nAn introduction to Seaborn so you too don’t almost quit your bootcamp over matplotlib.\n\n\n\nEmily Robinson\n\n\nAug 8, 2016\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreating My First R Package Part 1\n\n\n\nR\n\n\nCode\n\n\n\nMy struggles with STATA and how it motivated me to want to create my first R package.\n\n\n\nEmily Robinson\n\n\nJul 26, 2016\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom Social Scientist to Data Scientist\n\n\n\n\n\nReflections on starting my transitino from social science academia into the data science field.\n\n\n\nEmily Robinson\n\n\nJul 5, 2016\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m currently working as a data science consultant, primarily as an expert witness on a trade secrets case. Previously, I worked at Warby Parker as a senior data scientist on a centralized team tackling some of the company’s biggest projects, including redesigning their experimentation platform and training and deploying machine learning models; at DataCamp, where I built and ran their experimentation analytics system; and at Etsy, where I worked with their search team to design, implement, and analyze experiments.\nI’m an author of the book Build Your Career in Data Science with Jacqueline Nolis, and the accompanying podcast. You can get 40% off the book with the code buildbook40%. I regularly give talks on A/B testing, R programming, and data science career advice at conferences and meetups (see all talks here).\nI hold a Master’s degree in Management (specialization in Organizational Behavior) from INSEAD. I also earned my bachelor’s degree from Rice University in Decision Sciences, an interdisciplinary major I designed that focused on understanding how people behave and make decisions.\nI love all animals and consider myself part of the #rcatladies despite not owning a cat. I do have a dog, Yeti, a 30-pound ball of fluff who loves cuddles."
  }
]